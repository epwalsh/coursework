\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{Solution}{\stepcounter{ProofCounter}\textbf{Solution:}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter \\}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter \\}}

\allowdisplaybreaks{}

% chktex-file 3

\lhead{Evan P. Walsh}
\chead{MATH 521}
\rhead{\thepage}
\cfoot{}

% Custom commands.
\newcommand\toinfty{\rightarrow\infty}
\newcommand\toinf{\rightarrow\infty}
\newcommand{\sinf}[1]{\sum_{#1=0}^{\infty}}
\newcommand{\linf}[1]{\lim_{#1\rightarrow\infty}}

\begin{document}\thispagestyle{empty}
\begin{center}
  \Large \textsc{math 521 -- Final Exam -- fall 2018} \\ 
  \vspace{5mm}
  \large Evan Pete Walsh
\end{center}

%------------------------------------------------------------------------------------------------------------------%
% Question 1
%------------------------------------------------------------------------------------------------------------------%

\subsection*{Problem 1(i)}
\begin{Solution}
  Fix $\theta \in \mathbb{R}$ and let $X_t := e^{i\theta}B_t$, for $t \geq 0$. Since continuity in $t$ is trivial, it suffices to show that components of each $X_t$ are independent normal with expectation 0 and that for any $t_1 < t_2$, the covariance component-wise of $(X_i(t_1), X_i(t_2))$ matches that of $(B_i(t_1), B_i(t_2))$, for $i = 1,2$. First note that we can rewrite $X_t$ as follows.
  \begin{align*}
    X_t = e^{i\theta}B_t & = e^{i\theta}(B_1(t) + i B_2(t)) \\
    & = (\cos \theta + i \sin \theta) (B_1(t) + B_2(t)) \\
    & = \cos \theta B_1(t) - \sin \theta B_2(t) + i (\sin \theta B_1(t) + \cos \theta B_2(t)).
  \end{align*}
  Therefore the components of $X_t$ are 
  \[
    X_1(t) = \cos \theta B_1(t) - \sin \theta B_2(t) \text{ and } X_2(t) = \sin \theta B_1(t) + \cos \theta B_2(t).
  \]
  So clearly the components are normal with expectation 0. Further,
  \begin{align*}
    \text{Cov}(X_1(t), X_2(t)) & = (\cos \theta)^2 \text{Cov}(B_1(t), B_2(t)) + \cos\theta \sin \theta \text{Cov}(B_1(t), B_1(t)) \\
    & \ \ \ \ \ \ - (\sin\theta)^2\text{Cov}(B_1(t), B_2(t)) - \sin\theta\cos\theta \text{Cov}(B_2(t), B_2(t)) \\
    & = 0 + \cos\theta \sin \theta t - \sin\theta\cos\theta t \\
    & = 0.
  \end{align*}
  Hence the components are independent. Now let $0 \leq t_1 < t_2$. Then
  \begin{align*}
    \text{Cov}(X_1(t_1), X_1(t_2)) & = (\cos \theta)^2 \text{Cov}(B_1(t_1), B_1(t_2)) - \cos\theta\sin\theta \text{Cov}(B_1(t_1), B_2(t_2)) \\
    & \ \ \ \ \ - \cos\theta\sin\theta \text{Cov}(B_2(t_1), B_1(t_2)) + (\sin\theta)^2 \text{Cov}(B_2(t_1), B_2(t_2)) \\
    & = (\cos\theta)^2 t_1 + (\sin\theta)^2 t_1 \\
    & = t_1 \\
    & = \text{Cov}(B_1(t_1), B_1(t_2)).
  \end{align*}
  Similarly,
  \begin{align*}
    \text{Cov}(X_2(t_1), X_2(t_2)) & = (\sin\theta)^2\text{Cov}(B_1(t_1), B_1(t_2)) + (\cos\theta)^2\text{Cov}(B_2(t_1), B_2(t_2)) \\
    & = t_1 \\
    & = \text{Cov}(B_2(t_1), B_2(t_2)).
  \end{align*}
\end{Solution}

\subsection*{Problem 1(ii)}
\begin{Solution}
  Let $\psi(t) := \sqrt{2t\log\log t}$. Note that
  \[
    \limsup_{t\rightarrow\infty} \frac{|B(t)|}{\psi(t)} \geq \limsup_{t\rightarrow\infty} \frac{|B_1(t)|}{\psi(t)} = 1, \ \ \text{a.s.}
  \]
  Therefore it remains to show that opposite inequality. Let $q > 1$ and $\epsilon > 0$, and define
  \[
    A_n := \left\{ \max_{0\leq t \leq q^n}|B(t)| \geq (1 + \epsilon) \psi(q^n) \right\}.
  \]
  So,
  \begin{align*}
    P(A_n) & = P \left( \max_{0\leq t \leq q^n} B_1(t)^2 + B_2(t)^2 \geq (1 + \epsilon)^2\psi(q^n)^2 \right) \\
    & \leq \sum_{i=1}^{2} P \left( \max_{0\leq t \leq q^n} |B_i(t)| \geq \frac{1}{\sqrt{2}}(1 + \epsilon)\psi(q^n) \right) \\
    & \leq \sum_{i=1}^{2} P \left( \max_{0\leq t \leq q^n} B_i(t) \geq \frac{1}{\sqrt{2}}(1 + \epsilon)\psi(q^n) \right) + P \left( \min_{0\leq t \leq q^n} B_i(t) \leq - \frac{1}{\sqrt{2}}(1 + \epsilon)\psi(q^n) \right) \\
    & \leq \sum_{i=1}^{2} 2 P \left( \max_{0\leq t \leq q^n} B_i(t) \geq \frac{1}{\sqrt{2}}(1 + \epsilon)\psi(q^n) \right) \\
  & = \sum_{i=1}^{2} 2 P \left( |B_i(q^n)| \geq \frac{1}{\sqrt{2}}(1 + \epsilon)\psi(q^n) \right) \ \ \ \text{(since $\max_{0\leq t \leq q^n}B_i(t) \stackrel{d}{=} |B_i(q^n)|$)} \\
    & = \sum_{i=1}^{2} 2 P \left( \frac{|B_i(q^n)|}{\sqrt{q^n}} \geq \frac{(1 + \epsilon)\psi(q^n)}{\sqrt{2q^n}} \right) \\
    & \leq 4 \exp\{ -(1 + \epsilon)^2 \log \log q^n \} \ \ \ \text{(Lemma 12.9 in appendix of Morters \& Peres)} \\
    & = \frac{4}{(n\log q)^{(1+\epsilon)^2}}.
  \end{align*}
  Thus $\sum_{n=1}^{\infty} P(A_n) < \infty$. So by Borel-Cantelli, $A$ occurs only a finite number of times with probability 1. Now, for large $t$ let $q^{n-1} \leq t < q^{n}$. Then
  \[
    \frac{|B(t)|}{\psi(t)} = \underbrace{\frac{|B(t)|}{\psi(q^n)}}_{\leq (1 + \epsilon)} \underbrace{\frac{\psi(q^n)}{q^n} \frac{t}{\psi(t)}}_{\leq 1} \underbrace{\frac{q^n}{t}}_{\leq q} \leq (1 + \epsilon) q.
  \]
  Hence
  \[
    \limsup_{t \rightarrow \infty} \frac{|B(t)|}{\psi(t)} \leq (1 + \epsilon)q \ \ \text{a.s.}
  \]
  But since $\epsilon > 0$ and $q > 1$ we arbitrary, we have
  \[
    \limsup_{t\toinf} \frac{|B(t)|}{\psi(t)} \leq 1 \ \ \text{a.s.}
  \]
  Hence we are done.
\end{Solution}

\subsection*{Problem 2(i)}
\begin{Solution}
  The components of $B_t$ are iid normal random variables with expection 0 and variance $t$. Therefore the joint density with respect to three-dimensional Lebesgue measure is given by
  \[
    \frac{1}{(2\pi t)^{3/2}} \exp \left\{ - \frac{x_1^2 + x_2^2 + x_3^2}{2t} \right\}.
  \]
\end{Solution}

\subsection*{Problem 2(ii)}
\begin{Solution}
  Let $c := \left( \frac{2}{\sqrt{2\pi}} \right)^3$, $\alpha := \frac{1}{8}$, and $\beta := - \frac{9}{8}$. Then we have
  \begin{align*}
    P(|B_n| \leq n^{\alpha}) & \leq P(|B_1(n)| \leq n^{\alpha}, |B_2(n)| \leq n^{\alpha}, |B_3(n)| \leq n^{\alpha}) \\
    & = P(|B_1(n)| \leq n^{\alpha}) P(|B_2(n)| \leq n^{\alpha}) P(|B_3(n)| \leq n^{\alpha}) \\
    & = P(|B_1(n)| \leq n^{\alpha})^3 \\
    & = P \left( \frac{|B_1(n)|}{\sqrt{n}} \leq n^{\alpha - 1/2} \right)^3 \\
    & \leq \left( \frac{1}{\sqrt{2\pi}} 2n^{\alpha - 1/2} \right)^3 \\
    & = \left( \frac{2}{\sqrt{2\pi}} \right)^3 n^{3\alpha - 3/2} \\
    & = cn^{\beta}.
  \end{align*}
\end{Solution}

\subsection*{Problem 2(iii)}
\begin{Solution}
  Let $\alpha > 0$. By invariance (compenent-wise), $|B_n - B_{n+t}| \stackrel{d}{=} |B_t|$. So
  \begin{align*}
    P(A_n) = P\left( \max_{0\leq t \leq 1} |B_t| \geq n^{\alpha} / 2\right) & = P \left( \max_{0\leq t \leq 1} \sqrt{B_1(t)^2 + B_2(t)^2 + B_3(t)^2} \geq n^{\alpha}/2 \right) \\
    & \leq \sum_{i=1}^{3} P \left( \max_{0\leq t \leq 1} |B_i(t)| \geq n^{\alpha}/ 6 \right) \\
    & \leq \sum_{i=1}^{3} P \left( \max_{0\leq t \leq 1} B_i(t) \geq n^{\alpha}/ 6 \right) \\
    & \ \ \ \ \ + P \left( \min_{0\leq t \leq 1} B_i(t) \leq - n^{\alpha}/ 6 \right) \\
    & = \sum_{i=1}^{3} 2P \left( \max_{0\leq t \leq 1} B_i(t) \geq n^{\alpha}/ 6 \right) \\
    \text{(since $\max_{0\leq t \leq 1}B_i(t) \stackrel{d}{=} |B_i(1)|$)} \ \ & = \sum_{i=1}^{3} 2P(|B_i(1)| \geq n^{\alpha}/6) \\
    \text{(Lemma 12.9 in appendex of Morters \& Peres)} \ \ & \leq 6 \cdot 2 \cdot \exp \left\{ - \frac{n^{2\alpha}}{6^2\cdot 2} \right\}.
  \end{align*}
  Thus $\sinf{n} P(A_n) < \infty$. So by the Borel-Cantelli lemma, $P(A_n \text{ i.o.}) = 0$, i.e. 
  \[
    P(A_n^c \text{ eventually}) = 1.
  \]
\end{Solution}

\subsection*{Problem 2(iv)}
\begin{Solution}
  By Part (ii) and the Borel-Cantelli Lemma, there exists $\alpha > 0$ such that with probability 1,
  \begin{equation}
    |B_n| > n^{\alpha} \ \text{ eventually.}
    \label{2(iv)(i)}
  \end{equation}
  By Part (iii), with probability 1
  \begin{equation}
    \max_{0\leq t \leq 1} |B_n - B_{n+t}| < n^{\alpha}/2 \ \text{ eventually.}
    \label{2(iv)(ii)}
  \end{equation}
  So with probability 1 both \eqref{2(iv)(i)} and \eqref{2(iv)(ii)} hold, and for all such $\omega$ and large enough $t$,
  \[
    |B(t)| > (\lfloor t \rfloor)^\alpha - (\lfloor t \rfloor)^\alpha / 2 = (\lfloor t \rfloor)^{\alpha} / 2.
  \]
  Therefore with probability 1, $\liminf_{t\toinf} |B_t| = \infty$.
\end{Solution}

\subsection*{Problem 3(i)}
\begin{Solution}
  Since $(X(t_1), X(t_2), \dots, X(t_n))$ is just a linear combination of normal random variables, it is also has a Guassian distribution. Further,
  \[
    EX_t = Ee^{-t}B(e^{2t}) = e^{-t}EB(e^{2t}) = 0,
  \]
  and for all $s, t \in \mathbb{R}$,
  \[
    \text{Cov}(X_s, X_t) = e^{-(s + t)}\text{Cov}(B(e^{2s}), B(e^{2t})) = e^{-(s+t)}\min\{e^{2s}, e^{2t}\}.
  \]
\end{Solution}

\subsection*{Problem 3(ii)}
\begin{Solution}
  Fix $s \in \mathbb{R}$. Then clearly $Y_t$ is also normal with $EY_t = 0$. Therefore it remains to show that for any $t_1, t_2 \in \mathbb{R}$, $\text{Cov}(Y_{t_1}, Y_{t_2}) = \text{Cov}(X_{t_1}, X_{t_2})$. Well, by Part (i),
  \begin{align*}
    \text{Cov}(Y_{t_1}, Y_{t_2}) = \text{Cov}(X_{s+t_1}, X_{s+t_2}) & = e^{-(s+t_1) - (s+t_2)}\min\{ e^{2(s+t_1)}, e^{2(s+t_2)}\} \\
    & = e^{-(s+t_1) - (s+t_2)} e^{2s} \min\{ e^{2t_1}, e^{2t_2}\} \\
    & = e^{-t_1 - t_2} \min\{ e^{2t_1}, e^{2t_2}\} \\
    & = \text{Cov}(X_{t_1}, X_{t_2}).
  \end{align*}
\end{Solution}

\subsection*{Problem 3(iii)}

\end{document}
