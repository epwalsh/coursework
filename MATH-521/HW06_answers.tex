\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{Solution}{\stepcounter{ProofCounter}\textbf{Solution:}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter \\}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter \\}}

\allowdisplaybreaks{}

% chktex-file 3

\lhead{Evan P. Walsh}
\chead{MATH 521}
\rhead{\thepage}
\cfoot{}

% Custom commands.
\newcommand\toinfty{\rightarrow\infty}
\newcommand\toinf{\rightarrow\infty}
\newcommand{\sinf}[1]{\sum_{#1=0}^{\infty}}
\newcommand{\linf}[1]{\lim_{#1\rightarrow\infty}}

\begin{document}\thispagestyle{empty}
\begin{center}
  \Large \textsc{math 521 -- ASSIGNMENT VI -- fall 2018} \\ 
  \vspace{5mm}
  \large Evan Pete Walsh
\end{center}

%------------------------------------------------------------------------------------------------------------------%
% Question 1
%------------------------------------------------------------------------------------------------------------------%

\subsection*{1}
\begin{tcolorbox}
  Suppose that $\{B_t, t \geq 0\}$ is Brownian motion (BM). Let $Y(t) := tB(1/t)$ for $t > 0$ and $Y_0 := 0$. Prove $\{ Y_t , t \geq 0 \}$ is BM.
\end{tcolorbox}
\begin{Solution}
  It suffices to show that the paths $t \mapsto B_t$ are continuous almost surely and that for any $0 \leq t_1 < t_2 < \dots < t_n$, the joint distribution of $(Y_{t_1}, \dots, Y_{t_n})$ matches that of BM. We'll do so through the following claims.

  \begin{claim}
    For any $0 \leq t_1, t_2 < \infty$, $\text{Cov}(B_{t_1}, B_{t_2}) = \min\{t_1, t_2\}$.
  \end{claim}
  \begin{claimproof}
    Let $0 \leq t_1, t_1 < \infty$. If $t_1 = t_2$, it is trivial. So without loss of generality assume $t_1 < t_2$. Let $t = t_1$ and $h = t_2 - t_1 = t_2 - t$. Then by definition of BM,
    \[
      h = \text{Var}(B_{t+h} - B_t) = (t + h) + (t) - 2\text{Cov}(B_t, B_{t+h}).
    \]
    Thus, $\text{Cov}(B_{t_1}, B_{t_2}) = \text{Cov}(B_t, B_{t+h}) = t = \min\{t_1, t_2\}$.
  \end{claimproof}

  \begin{claim}
    $(Y_{t_1}, \dots, Y_{t_n})$ has the same distribution as $(B_{t_1}, \dots, B_{t_n})$ for any $0 \leq t_1 < t_2 < \dots < t_n$.
  \end{claim}
  \begin{claimproof}
    Let $0 \leq t_1 < t_2 < \dots < t_n$. Clearly $(Y_{t_1}, \dots, Y_{t_n})$ is multivariate normal with expectation $\bm{0}$. Since normal distributions are uniquely determined by their mean and covariance matrices, it suffices to show that the covariance of $Y_{t_i}, Y_{t_j}$ matches that of $B_{t_i}, B_{t_j}$ for any $i, j \in \{1, \dots, n\}$. Without loss of generality assume $i \leq j$. Let $t = t_i$ and $h = t_j - t_i = t_j - t$. Then
    \begin{align*}
      \text{Cov}(Y_{t_i}, Y_{t_j}) = \text{Cov}(Y_t, Y_{t+h}) & = \text{Cov}\bigg(tB(1/t), (t+h)B(1/(t+h))\bigg) \\
      & = E\bigg[tB(1/t)(t+h)B(1/(t+h))\bigg] \\
      & = t(t+h) E\bigg[ B(1/t)B(1/(t+h)) \bigg] \\
      & = t(t+h) \text{Cov}\bigg[ B(1/t),B(1/(t+h)) \bigg] \\
      \text{(by Claim 1)} \ \ \ \ & = t(t+h)\frac{1}{t+h} \\
      & = t = t_1 = \min\{t_1, t_2\}.
    \end{align*}
    So $\text{Cov}(Y_{t_i}, Y_{t_j}) = \min\{t_1, t_2\} \stackrel{\text{Claim 1}}{=} \text{Cov}(B_{t_i}, B_{t_j})$.
  \end{claimproof}

  \begin{claim}
    For almost all $\omega$, the path $t \mapsto Y_t(\omega)$ is continuous.
  \end{claim}
  \begin{claimproof}
    Clearly this holds for $t > 0$, so it remains to show that $\limsup_{t\rightarrow 0} |Y_t| = 0$ almost surely, i.e. that
    \begin{equation}
      P \left( \limsup_{t\rightarrow 0} |Y_t| = 0 \right) = 1.
      \label{1.1}
    \end{equation}
    Since we already have that $t \mapsto Y_t$ is continuous almost surely for all $t > 0$, \eqref{1.1} is equivalent to
    \begin{equation}
      P \left( \limsup_{\mathbb{Q} \ni t\rightarrow 0} |Y_t| = 0 \right) = 1,
      \label{1.2}
    \end{equation}
    where $\mathbb{Q}$ is the set of rational numbers, since $\mathbb{Q}$ is dense in $[0, \infty)$. By Claim 2, the distribution of any finite collection $(Y_{t_1}, \dots, Y_{t_n})$ has the same distribution as $(B_{t_1}, \dots, B_{t_n})$. Therefore the distribution of $\limsup_{\mathbb{Q}\ni t \rightarrow 0}|Y_t|$ has the same distribution as $\limsup_{\mathbb{Q}\ni t \rightarrow 0}|B_t|$, since the $\limsup$ is taken over a countable set. So by the almost sure continuity of $t \mapsto B_t$ at 0,
    \[
      1 = P \left( \limsup_{\mathbb{Q} \ni t\rightarrow 0} |B_t| = 0 \right) = P \left( \limsup_{\mathbb{Q} \ni t\rightarrow 0} |Y_t| = 0 \right).
    \]
    Hence \eqref{1.2} holds, and we are done.
  \end{claimproof}

\end{Solution}

%------------------------------------------------------------------------------------------------------------------%
% Question 2
%------------------------------------------------------------------------------------------------------------------%

\newpage
\subsection*{2}
\begin{tcolorbox}
  Suppose $\{B_t, t \geq 0\}$ is BM. Let 
  \begin{align*}
    T & := \inf\{t \geq 0 : B_t = 1\}, \\
    Z_t & := B(t \wedge T), t \geq 0, \\
    Y_t & := B(T + t) - B(T), t \geq 0.
  \end{align*}
  Prove that $\{ Y_t, t \geq 0\}$ is BM independent of the $\sigma$-field $\sigma\{ Z_t, t \geq 0\}$.
\end{tcolorbox}
\begin{Solution}
  By the lemma we proved in class, $T < \infty$ almost surely, and therefore $Z_t$ and $Y_t$, for $t \geq 0$, are well-defined almost surely. For $n \geq 1$, define
  \begin{align*}
    T_n & := \min_{k} \left\{ \frac{k}{n} : B\left( \frac{k}{n} \right) > 1 \right\}, \ \ \text{(well-defined provided $T$ is finite)} \\
    Y_{t}^n & := B(T_n + t) - B(T_n), t \geq 0.
  \end{align*}
  Note that for fixed $k$, the process $\{B(k/n + t) - B(k/n), t \geq 0 \}$ is clearly BM and is independent of $\{ B_t, t  \leq k/n \}$ (by the independence of increments). In particular,
  \[
    \sigma\{B(k/n + t) - B(k/n), t \geq 0\}
  \]
  is independent of any event of the form $\{ T_n = k/n \} \cap E$, where $E \in \sigma\{Z_t, t \geq 0\}$.
  Thus, for any Borel set $A$ and any event $E \in \sigma\{Z_t, t \geq 0\}$,
  \begin{align*}
    P(\{Y_t^n \in A\} \cap E) & = \sum_{k=1}^{\infty} P\left(\{Y_t^n \in A\} \cap E \cap \left\{T_n = \frac{k}{n}\right\}\right) \\
    & = \sum_{k=1}^{\infty} P\bigg( \{B(k/n + t) - B(k/n) \in A\}\bigg)P\left(E \cap \left\{T_n = \frac{k}{n}\right\} \right) \\
    \text{(by invariance)} \ \ \ & = P\bigg( \{B(t) \in A\}\bigg)\sum_{k=1}^{\infty} P\left(E \cap \left\{T_n = \frac{k}{n}\right\} \right) \\
    & = P( \{B(t) \in A\})P(E).
  \end{align*}
  So with $E = \Omega$, we get
  \begin{equation}
    P(\{Y_t^n \in A\}) = P(\{B_t \in A\}),
    \label{2.1}
  \end{equation}
  and then
  \begin{equation}
    P(\{Y_t^n \in A\} \cap E) = P(\{B_t \in A\})P(E) = P(\{Y_t^n \in A\})P(E).
    \label{2.2}
  \end{equation}
  It follows by \eqref{2.1} that $\{ Y_t^n, t \geq 0\}$ has the same distribution as $\{ B_t, t \geq 0\}$ and by \eqref{2.2} that $\{ Y_t^n, t \geq 0\}$ is independent of $\{ Z_t, t \geq 0\}$.
  Thus each $\{ Y_t^n, t \geq 0\}$, $n \geq 1$, is Brownian motion independent of $\{Z_t, t \geq 0\}$. The fact that this holds for $\{ Y_t, t \geq 0\}$ is now apparent since
  $\lim_{n\rightarrow \infty} Y_t^n = Y_t$ almost surely for all $t \geq 0$.
\end{Solution}


%------------------------------------------------------------------------------------------------------------------%
% Question 3
%------------------------------------------------------------------------------------------------------------------%

\newpage
\subsection*{3}
\begin{tcolorbox}
  Suppose that random variables $X_n$, $n \geq 1$, converge to $X$ in distribution as $n \rightarrow \infty$. Prove that there exists a probability space $(\Omega, \mathcal{F}, P)$ and random variables $Y_n, n \geq 1$, and $Y$ defined on this space such that for each $n$, $X_n$ and $Y_n$ have the same distribution and $\lim_{n\rightarrow\infty} Y_n = Y$ almost surely.
\end{tcolorbox}

We will make use of the following lemma, which is a more general form of a result we have shown in a past homework. \\

\begin{Lemma}[Probability Integral Transformation]
  If $F$ is a CDF and $F^{-1}(u) := \sup \{ t : F(t) < u \}$, $u \in [0, 1]$, then the random variable $Z := F^{-1}(u)$ defined on the Borel probability space of $[0, 1]$ has distribution function $F$.
\end{Lemma}
\begin{Proof}
  It suffices to show that
  \begin{equation}
    F^{-1}(u) \leq t \text{ if and only if } u \leq F(t), \text{ for all  } u \in [0,1], t \in \mathbb{R}.
    \label{3.1}
  \end{equation}
  For if \eqref{3.1} holds, then $P(Z \leq t) = P(F^{-1}(u) \leq t) = P(u \leq F(t)) = F(t)$.

  \begin{claim}
    $(\Rightarrow)$ If $F^{-1}(u) \leq t$, then $u \leq F(t)$.
  \end{claim}
  \begin{claimproof}
    Suppose $F^{-1}(u) \leq t$ for some $u \in [0, 1], t \in \mathbb{R}$. By the monotonicity of $F$,
    \[
      F(F^{-1}(u)) \leq F(t).
    \]
    Thus it remains to show that $u \leq F(F^{-1}(u))$. By the right-continuity of $F$,
    \begin{equation}
      F(F^{-1}(u)) = F\left(\lim_{x\downarrow F^{-1}(u)}x\right).
      \label{3.2}
    \end{equation}
    But by definition of $F^{-1}(u)$, if $x \geq F^{-1}(u)$ then $F(x) \geq u$. So by right-continuity again,
    \begin{equation}
      F\left(\lim_{x\downarrow F^{-1}(u)}x\right) \geq u.
      \label{3.3}
    \end{equation}
    So by \eqref{3.2} and \eqref{3.3}, we are done.
  \end{claimproof}

  \begin{claim}
    $(\Leftarrow)$ If $u \leq F(t)$, then $F^{-1}(u) \leq t$.
  \end{claim}
  \begin{claimproof}
    Suppose $u \leq F(t)$ for some $u \in [0, 1], t \in \mathbb{R}$, and by way of contradiction assume $F^{-1}(u) > t$. Then by definition $F^{-1}$, there exists $t_0 > t$ such that $F(t_0) < u$. So we have
    \[
      F(t_0) < F(t).
    \]
    But this contradicts the monotonicity of $F$.
  \end{claimproof}

  By Claims 1 and 2, \eqref{3.1} holds. Hence we are done.
\end{Proof}

\vspace{5mm}

We will now prove the main result. \\

\begin{Solution}
  Let $F_n$ be the CDF of $X_n$, $n \geq 1$. Let $(\Omega, \mathcal{F}, P)$ be the Borel probability space on $[0, 1]$, and define
  \begin{align*}
    Y_n(\omega) & := \sup\{ t : F_n(t) < \omega \},  \ \ n \geq 1 \\
    Y(\omega) & := \sup\{ t : F(t) < \omega \}.
  \end{align*}

  By Lemma 1 (the PIT), $Y$ has the same distribution as $X$, and $Y_n$ has the same distribution as $X_n$ for each $n \geq 1$. It remains to show that $Y_n \rightarrow Y$ almost surely. Note that since $Y$ is monotone, the number of discontinuities of $Y$ is countable, and therefore has measure 0. So it suffices to show that $Y_n(\omega) \rightarrow Y(\omega)$ for all $\omega$ at which $Y$ is continuous. Fix such an $\omega \in (0,1)$.

  Let $y < Y(\omega)$ such that $F$ is continuous at $y$. Then $F(y) < \omega$ by the definition of $Y$, and $F_n(y) \rightarrow F(y)$ since $F$ is continuous at $y$. Hence $F_n(y) < \omega$ for large enough $n$, and so $y \leq Y_n(\omega)$ for large enough $n$. Thus
  \[
    y \leq \liminf_{n\rightarrow\infty} Y_n(\omega).
  \]
  However, since the set of continuity points of $F$ is dense in $\mathbb{R}$, we can make such a $y < Y(\omega)$ arbitrarily close to $Y(\omega)$. Thus
  \begin{equation}
    Y(\omega) \leq \liminf_{n\rightarrow\infty} Y_n(\omega).
    \label{3.4}
  \end{equation}

  Now let $\epsilon > 0$ and let $y > Y(\omega + \epsilon)$ such that $F$ is continuous at $y$. Then $\omega + \epsilon \leq F(y)$, and so $\omega < F_n(y)$ for large enough $n$. Thus
  \[
    \limsup_{n\rightarrow\infty} Y_n(\omega) \leq y.
  \]
  But by the density of the continuity points of $F$ again, we can make $y > Y(\omega + \epsilon)$ arbitrarily close to $Y(\omega + \epsilon)$. Hence
  \[
    \limsup_{n\rightarrow\infty} Y_n(\omega) \leq Y(\omega + \epsilon).
  \]
  Finally, since $\omega$ was chosen so that $Y$ is continuous at $\omega$,
  \begin{equation}
    \limsup_{n\rightarrow\infty} Y_n(\omega) \leq Y(\omega).
    \label{3.5}
  \end{equation}
  So by \eqref{3.4} and \eqref{3.5},
  \[
    Y(\omega) \leq \liminf_{n\toinf} Y_n(\omega) \leq \limsup_{n\toinf} Y_n(\omega) \leq Y(\omega).
  \]
  Hence we are done.
\end{Solution}

\end{document}
