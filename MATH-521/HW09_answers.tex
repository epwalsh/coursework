\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{Solution}{\stepcounter{ProofCounter}\textbf{Solution:}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter \\}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter \\}}

\allowdisplaybreaks{}

% chktex-file 3

\lhead{Evan P. Walsh}
\chead{MATH 521}
\rhead{\thepage}
\cfoot{}

% Custom commands.
\newcommand\toinfty{\rightarrow\infty}
\newcommand\toinf{\rightarrow\infty}
\newcommand{\sinf}[1]{\sum_{#1=0}^{\infty}}
\newcommand{\linf}[1]{\lim_{#1\rightarrow\infty}}

\begin{document}\thispagestyle{empty}
\begin{center}
  \Large \textsc{math 521 -- ASSIGNMENT IX -- fall 2018} \\ 
  \vspace{5mm}
  \large Evan Pete Walsh
\end{center}

%------------------------------------------------------------------------------------------------------------------%
% Question 1
%------------------------------------------------------------------------------------------------------------------%

\subsection*{1}
\begin{tcolorbox}
  Prove that the invariance principle holds when the assumption that the $X_k$'s are bounded is relaxed to the assumption that $EX_k^2 < \infty$ (finite variance).
\end{tcolorbox}
\begin{Solution}
  Let $\sigma^2 := \text{Var}X_k$. Let $S_n := \sum_{k=1}^n X_k$, and let $S_n(t)$ be the linear interpolation of $S_n$ for $t \geq 0$. Let 
  \[
    S_n^*(t) = \frac{S_n(nt)}{\sigma \sqrt{n}}, \ \ t \geq 0,  \ \ n \geq 1.
  \]
  We need to show that $\{ S_n^*(t) , t \geq 0 \} \rightarrow \{ B(t), t \geq 0 \}$ in distribution, where $\{ B(t), t \geq 0\}$ is standard Brownian motion.

  Fix $\delta > 0$. Then for any $\epsilon \in (0, \sigma^2)$, we can find $M > 0$ such that, for $Y_k := X_k \bm{1}_{\{|X_k| \leq M\}}$ and $V_k := X_k - Y_k$, we have
  \[
    \sigma_M^2 := \text{Var} Y_k \in (\sigma^2 - \epsilon, \sigma^2] \ \ \text{ and } \ \ \text{Var}V_k < \epsilon.
  \]
  Let $S_n^{Y}(t)$ and $S_n^{V}(t)$ be defined analogous to $S_n$, i.e. as linear interpolations of the sums of the partial sums of the $Y_n$'s and $V_n$'s, respectively. Let 
  \[
    S_n^{Y*}(t) := \frac{S_n^Y(nt)}{\sigma_M \sqrt{n}},
  \]
  and
  \[
    S_n^{V*}(t) := \frac{S_n^V(nt)}{\sigma\sqrt{n}}.
  \]
  So $\{ S_n^{Y*}(t), t \geq 0\} \rightarrow \{ B(t), t \geq 0\}$ by the version of the invariance principle proved in class, and
  \begin{equation}
    S_n^*(t) = \frac{S_n^Y(nt)}{\sigma \sqrt{n}} + \frac{S_n^V(nt)}{\sigma\sqrt{n}} = \frac{\sigma_M}{\sigma} S_n^{Y*}(t) + S_n^{V*}(t).
    \label{1.1}
  \end{equation}
  Further, by Markov's inequality,
  \begin{align}
    P \left( |S_n^{V*}(t)| \geq \delta \right) & = P \left( \frac{1}{\sigma\sqrt{n}}|S_n^V| \geq \delta \right) \nonumber \\
    & \leq \frac{1}{\delta^2} E \left[ \left( \frac{1}{\sigma \sqrt{n}}|S_n^V| \right)^2 \right] \nonumber \\
    & = \frac{1}{\sigma^2 n\delta^2} \text{Var} S_n^V \nonumber \\
    & \leq \frac{\epsilon}{\sigma^2 \delta^2}.
    \label{1.2}
  \end{align}
  Now let $x \in \mathbb{R}$. Then
  \begin{align}
    \limsup_{n\rightarrow\infty} P(S_n^*(t) \leq x) & = \limsup_{n\rightarrow\infty} \left[ P \left( S_n^*(t) \leq x, |S_n^{V*}(t)| \geq \delta \right) + P \left( S_n^*(t) \leq x, |S_n^{V*}(t)| < \delta \right) \right] \nonumber \\
    & \leq \frac{\epsilon}{\sigma^2 \delta^2} + \limsup_{n\rightarrow \infty} P \left( \frac{\sigma_M}{\sigma} S_n^{Y*}(t) - \delta \leq x \right) \nonumber \\
    & = \frac{\epsilon}{\sigma^2 \delta^2} + P \left( B(t) \leq \frac{\sigma(x + \delta)}{\sigma_M} \right).
    \label{1.3}
  \end{align}
  Similarly,
  \begin{align}
    \liminf_{n\rightarrow\infty} P(S_n^*(t) \leq x) \geq \liminf_{n\rightarrow \infty} P \left( \frac{\sigma_M}{\sigma} S_n^{Y*}(t) + \delta \leq x \right) = P \left( B(t) \leq \frac{\sigma(x - \delta)}{\sigma_M} \right).
    \label{1.4}
  \end{align}
  But since $\epsilon > 0$ was arbitrary and the distribution function of $B(t)$ is continuous, we have
  \begin{align}
    \limsup_{n\rightarrow\infty} P(S_n^*(t) \leq x) & \leq P \left( B(t) \leq x + \delta \right), \ \ \text{ and } \label{1.5} \\
    \liminf_{n\rightarrow\infty} P(S_n^*(t) \leq x) & \geq P \left( B(t) \leq x - \delta \right). \label{1.6}
  \end{align}
  Now let $\delta \rightarrow 0$ as well. Then
  \begin{equation}
    P( B(t) \leq x ) \leq \liminf_{n\rightarrow\infty} P(S_n^*(t) \leq x) \leq\limsup_{n\rightarrow\infty} P(S_n^*(t) \leq x) \leq P(B(t) \leq x).
    \label{1.7}
  \end{equation}
  The results above can be extended to any finite collection $\{ S_n^*(t_1), \dots, S_n^*(t_k) \}$, and therefore $\{ S_n^*(t), t \geq 0 \} \rightarrow \{ B(t), t \geq 0\}$ in distribution.
\end{Solution}

%------------------------------------------------------------------------------------------------------------------%
% Question 2
%------------------------------------------------------------------------------------------------------------------%

\newpage
\subsection*{2}
\begin{tcolorbox}
  Let $\lambda > 0$ and suppose $p_n \in (0, 1)$ for $n \geq 1$ such that $np_n \rightarrow \lambda$ as $n\toinf$. Suppose $X_n \sim \text{Binomial}(n, p_n)$ and $Y \sim \text{Poisson}(\lambda)$. Prove that $X_n \rightarrow Y$ in distribution.
\end{tcolorbox}
\begin{Solution}
  Let $k \geq 0$. Then for $n \geq k$,
  \begin{align}
    P(X_n = k) = \binom{n}{k} p_n^k(1 - p_n)^{n-k} & = \frac{n!}{k!(n-k)!} p_n^k(1 - p_n)^{n-k} \nonumber \\
    & = \underbrace{\frac{(np_n)^{k}}{k!}}_{A} \times \underbrace{\frac{(1-p_n)^n}{1}}_{B} \times \underbrace{\frac{1}{(1-p_n)^k}}_{C} \times \underbrace{\frac{n!}{n^k(n-k)!}}_{D}.
    \label{2.1}
  \end{align}
  Now for term $A$, we have
  \begin{equation}
    \lim_{n\rightarrow\infty} \frac{(np_n)^{k}}{k!} = \frac{\lambda^k}{k!}.
    \label{2.2}
  \end{equation}
  For term $B$,
  \begin{equation}
    \linf{n} (1 - p_n)^n = \linf{n} \left( 1 + \frac{-np_n}{n} \right)^n = e^{-\lambda}.
    \label{2.3}
  \end{equation}
  For term $C$,
  \begin{equation}
    \linf{n} \frac{1}{(1 - p_n)^k} = 1.
    \label{2.4}
  \end{equation}
  And for term $D$,
  \begin{equation}
    \linf{n} \frac{n!}{n^k(n-k)!} = 1.
    \label{2.5}
  \end{equation}
  Therefore
  \begin{equation}
    \linf{n} P(X_n = k) = \frac{\lambda^k e^{-\lambda}}{k!} = P(Y = k).
    \label{2.6}
  \end{equation}
\end{Solution}

%------------------------------------------------------------------------------------------------------------------%
% Question 3
%------------------------------------------------------------------------------------------------------------------%

\newpage
\subsection*{3}
\begin{tcolorbox}
  Suppose $(D, \mathcal{G}, \mu)$ is a $\sigma$-finite measure space with disjoint $F_1, F_2, \dots \in \mathcal{G}$ such that
  \[
    \cup_{k\geq 1}F_k = D
  \]
  and $\mu(F_k) < \infty$ for all $k \geq 1$. Further, suppose that $Z_1, Z_2, \dots$ are independent and $Z_k$ is a Poisson Point Processes (PPP) on $F_k$ with intensity $\mu_k$ defined by
  \[
    \mu_k(A) = \mu(A \cap F_k), \ \ A \in \mathcal{G}, A \cup F_k.
  \]
  Show that $Z := \cup_{k\geq 1} Z_k$ is a PPP.
\end{tcolorbox}
\begin{Solution}
  The key here is that a countable sum of Poisson random variables is also a Poisson random variable, with the convention that $\lambda = \infty$ implies that the random variable is degenerately infinite. We will prove this formally with the following claim.

  \begin{claim}
    Let $\lambda_1, \lambda_2, \dots \in \mathbb{R}$ and $X_1, X_2, \dots$ be independent random variables such that $X_k$ is Poisson with paramater $k$. Then $\sum_{k=1}^{\infty}X_k$ is Poisson with parameter $\sum_{k=1}^{\infty} \lambda_k$.
  \end{claim}
  \begin{claimproof}
    Let $S_n = \sum_{k=1}^{n}X_k$ and $\Lambda_n = \sum_{k=1}^{n} \lambda_k$. Then for all $k < \infty$, we have
    \[
      P(S_n = k) = \frac{\Lambda_n^k e^{-\Lambda_n}}{k!}
    \]
    since $S_n$ is Poisson with parameter $\Lambda_n$. So if $\sum_{n=1}^{\infty}\lambda_n$ converges, then
    \[
      P(S_n = k) \rightarrow \frac{\lambda^k e^{-\lambda}}{k!} \ \ \text{ as } n\rightarrow \infty,
    \]
    where $\lambda := \sum_{n=1}^{\infty}\lambda_n$. On the other hand, if $\sum_{n=1}^{\infty}\lambda_n$ diverges, then
    \[
      P(S_n = k) \rightarrow 0 \ \ \text{ as } n \toinf
    \]
    for all $k < \infty$, in which case $\sum_{n=1}^{\infty}X_n$ must be degenerately infinite.
  \end{claimproof}

  Now, we need to show that for all $A \in \mathcal{G}$, $\#(Z \cap A)$ is Poisson$(\mu(A))$, and for disjoint $A_1, A_2 \in \mathcal{G}$, $\#(Z \cap A_1)$ and $\#(Z\cap A_2)$ are independent.
  
  Well, independence follows immediately from the assumption that the $Z_k$'s are independent and defined on disjoint subsets of $D$. And for any $A \in \mathcal{G}$, we have
  \begin{align*}
    \#(Z \cap A) = \sum_{k=1}^{\infty} \#(Z_k \cap A) = \sum_{k=1}^{\infty} \#(Z_k \cap A \cap F_k),
  \end{align*}
  which, since each $Z_k$ is a PPP, is the countable sum of independent Poisson random variables, each with parameter $\mu_k(A)$. So by Claim 1, $\#(Z \cap A)$ is Poisson with parameter
  \[
    \sum_{k=1}^{\infty} \mu_k(A) = \sinf{k} \mu(A \cap F_k) = \mu(A).
  \]
  
\end{Solution}

\end{document}
