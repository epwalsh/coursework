\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\linespread{1.5}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 520: Assignment 5}
\author{Evan P. Walsh}
\makeatletter
\makeatother
\lhead{Evan P. Walsh}
\chead{STAT 520: Assignment 5}
\rhead{\thepage}
\cfoot{}

\begin{document}
% \maketitle

\subsection*{Nonlinear Regression}

\begin{enumerate}

  \item Consider a model of the form
    \begin{equation}
      Y_{i} = g_1(\bm{x}_i, \bm{\beta}) + \sigma g_2(\bm{x}_i, \bm{\beta}, \theta) \epsilon_i, \ \ i = 1,\dots, n,
      \label{1}
    \end{equation}
    where $\theta$ is known, $\sigma, g_2 > 0$, and $\epsilon_i \sim$ iid N(0,1). According to equation (5.34) in our notes,
    the generalized least squares (GLS) approach to estimating $\bm{\beta}$ in \eqref{1} attempts to find a solution to
    \begin{equation}
      \min_{\bm{\beta}}\sum_{i=1}^{n}\frac{\{y_i - g_1(\bm{x}_i, \bm{\beta})\}^{2}}{g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)}.
      \label{2}
    \end{equation}
    Now, for each $Y_{i}$, the density function is given by 
    \[
      f_{Y_i}(\bm{x}_i|\bm{\beta},\theta) = \{2\pi\sigma^{2}g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)\}^{-1/2}\exp\left[ 
      -\frac{1}{2g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)}\{y_i - g_1(\bm{x}_i,\bm{\beta})\}^{2}\right],
    \]
    and so
    \[
      \ell(\beta|\bm{y}_i) \propto -\log\left[\sigma g_{2}(\bm{x}_i,\bm{\beta},\theta)\right] - \frac{1}{2g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)}\{y_i -
      g_{1}(\bm{x}_i,\bm{\beta})\}^{2}.
    \]
    So, by independence, the MLE estimate of $\bm{\beta}$ is given by the solution to 
    \[
      \max_{\bm{\beta}}\sum_{i=1}^{n}\left(-\log\left[\sigma g_{2}(\bm{x}_i,\bm{\beta},\theta)\right] - \frac{1}{2g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)}\{y_i -
      g_{1}(\bm{x}_i,\bm{\beta})\}^{2}\right),
    \]
    which is equivalent to
    \begin{equation}
      \min_{\bm{\beta}}\sum_{i=1}^{n}\left(\log\left[\sigma g_{2}(\bm{x}_i,\bm{\beta},\theta)\right] + \frac{1}{2g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)}\{y_i -
      g_{1}(\bm{x}_i,\bm{\beta})\}^{2}\right).
      \label{3}
    \end{equation}
    Since \eqref{2} and \eqref{3} are clearly different objectives, the GLS and MLE estimates may very well differ.

    \vspace{2cm}

    \textbf{For the remainder of the assignment we will assume the model in \eqref{1} is expressed with }
    \begin{align}
      g_1(\bm{x}_i,\bm{\beta}) & = \beta_1\exp\left[ -\exp(\beta_2 - \beta_3x_i) \right] \nonumber \\
      g_2(\bm{x}_i, \bm{\beta}) & = [g_1(\bm{x}_i,\bm{\beta})]^{\theta},
      \label{4}
    \end{align}
    \textbf{i.e. }
    \begin{equation}
      Y_i = \mu_i(\bm{\beta}) + \sigma\{\mu_i(\bm{\beta})\}^{\theta}\epsilon_i,
      \label{5}
    \end{equation}
    \textbf{where }$\mu_i(\bm{\beta}) = \beta_1\exp\left[ -\exp(\beta_2 - \beta_3x_i) \right]$.

    \newpage

  \item Assume $\theta = 0.5$. We used the R function \texttt{nonlin} from the course webpage to estimate $\bm{\beta}$ using generalized least squares.
    Figure \ref{scatterplot} shows the relationship between the response and the covariate. Based on this, we can guestimate that the asymptote
    $\beta_1$ is around 20 and the point of inflection $\beta_2 / \beta_3$ is around 6. After some trial and error, we found that the starting values 
    $\beta_{1}^{(0)} = 20, \beta_{2}^{(0)} = 3$, and $\beta_{3}^{(0)} = 0.5$ give us convergence. Estimates of parameters along with 95\% 
    confidence intervals based on the Fundemental Theorem of Generalized Least Squares are displayed in Table \ref{tab2.1}.

    \begin{figure}[h]
      \caption{\emph{Scatterplot of response variable against the covariate.}}
      \centering
      \includegraphics[width=.9\textwidth]{./figures/hw05_scatterplot.pdf}
      \label{scatterplot}
    \end{figure}


    \begin{table}[h]
      \caption{\emph{Results of generalized least squares to estimate the parameters in model \eqref{5}.}}
      \vspace{.2cm}
      \centering
      \begin{tabular}{|c|c|c|}
        \hline
        & Estimate & 95\% CI based on Fund. Thm of GLS \\
        \hline
        $\beta_1$ & 20.062 & (19.365, 20.760) \\
        \hline
        $\beta_2$ & 3.224 & (2.921, 3.427) \\
        \hline
        $\beta_3$ & 0.539 & (0.480, 0.598) \\
        \hline
        $\beta_2 / \beta_3$ & 5.985 & (5.797, 6.172) \\
        \hline
        $\sigma^{2}$ & 0.2096 & \\
        \hline
      \end{tabular}
      \label{tab2.1}
    \end{table}

    \newpage
  \item Assume again that $\theta = 0.5$. Based on the estimates from Part 2, we used the starting values $\beta_1 = 20$, $\beta_2 = 3$, $\beta_3 =
    0.5$, and $\sigma^{2} = 0.2$ as input for the R function \texttt{optim} to minimize the negative log likelihood function.  The estimated
    parameters along with 95\% Wald theory confidence intervals are shown in Table \ref{tab3.1}.

    \begin{table}[h]
      \caption{\emph{Results of a maximum likelihood approach to estimate the parameters in model \eqref{5}.}}
      \vspace{.2cm}
      \centering
      \begin{tabular}{|c|c|c|}
        \hline
        & Estimate & 95\% Wald theory CI \\
        \hline
        $\beta_1$ & 20.064 & (19.393, 20.736) \\
        \hline
        $\beta_2$ & 3.259 & (3.009, 3.509) \\
        \hline
        $\beta_3$ & 0.544 & (0.493, 0.594) \\
        \hline
        $\beta_2 / \beta_3$ & 5.996 & (5.815, 6.176) \\
        \hline
        $\sigma^{2}$ & 0.202 & \\
        \hline
      \end{tabular}
      \label{tab3.1}
    \end{table}


  \item With $T_1 \equiv T_1(\bm{\beta}, x_i) := \exp[-\exp(\beta_2 - \beta_3x_i)]$ and $T_2 \equiv T_2(\bm{\beta}, x_i) := \exp(\beta_2 -
    \beta_3x_i)$, we found the first order partial derivatives of the log likelihood to be
    \[
      \begin{array}{ll}
        \frac{\partial \ell}{\partial \beta_1} = \sum_{i=1}^{n}\left[\frac{y_i^2}{2\sigma^2T_1\beta_1^2} - \frac{T_1}{2\sigma^2} -
        \frac{1}{2\beta_1}\right] &
        \frac{\partial \ell}{\partial \beta_2} = \sum_{i=1}^{n}\left[\frac{T_1T_2\beta_1}{2\sigma^2} + \frac{T_2}{2} -
        \frac{y_i^2T_2}{2\sigma^2T_1\beta_1}\right] \\
        \frac{\partial \ell}{\partial \beta_3} = \sum_{i=1}^{n}\left[ \frac{y_i^2x_iT_2}{2\sigma^2T_1\beta_1} - \frac{\beta_1x_iT_1T_2}{2\sigma^2} -
        \frac{x_iT_2}{2} \right] & \frac{\partial \ell}{\partial \sigma^2} = \sum_{i=1}^{n}\left[ \frac{y_i^2}{2[\sigma^2]^2\beta_1T_1} -
        \frac{y_i}{[\sigma^2]^2} + \frac{\beta_1T_1}{2[\sigma^2]^2} - \frac{1}{2\sigma^2} \right], \\ 
      \end{array}
    \]
    and the second order partial derivatives to be
    \[
      \begin{array}{ll}
        \frac{\partial^2 \ell}{\partial \beta_1 \partial \beta_1} = \sum_{i=1}^{n}\left[ \frac{1}{2\beta_1^2} - \frac{y_i^2}{\sigma^2T_1\beta_1^3} \right] & \\ 
        \frac{\partial^2 \ell}{\partial \beta_2 \partial \beta_2} = \sum_{i=1}^{n}\left[ \left\{ \frac{T_2\beta_1}{2\sigma^2} +
          \frac{y_i^2T_2}{2\sigma^2T_1^2\beta_1} \right\}(-T_1T_2) + \left\{ \frac{T_1\beta_1}{2\sigma^2} + \frac{1}{2} -
        \frac{y_i^2}{2\sigma^2T_1\beta_1} \right\}(T_2) \right] & \\ 
        \frac{\partial^2 \ell}{\partial \beta_1 \partial \beta_2} = \sum_{i=1}^{n}\left[ \left\{ -\frac{y_i^2x_iT_2}{2\sigma^2T_1^2\beta_1} -
          \frac{\beta_1x_iT_2}{2\sigma^2} \right\}(x_iT_1T_2) + \left\{ \frac{y_i^2x_i}{2\sigma^2T_1\beta_1} - \frac{\beta_1x_iT_1}{2\sigma^2} -
        \frac{x_i}{2} \right\}(-x_iT_2) \right] & \\ 
        \frac{\partial^2 \ell}{\partial \sigma^2 \partial \sigma^2} = \sum_{i=1}^{n}\left[ \frac{1}{2[\sigma^2]^2} - \frac{y_i^2}{[\sigma^2]^3\beta_1T_1}
        + \frac{2y_i - \beta_1T_1}{[\sigma^2]^3}\right] & \\ 
        \frac{\partial^2 \ell}{\partial \beta_1 \partial \beta_2} = \sum_{i=1}^{n}\left[ \frac{T_1T_2}{2\sigma^2} + \frac{y_i^2T_2}{2\sigma^2T_1\beta_1} \right] & \\ 
        \frac{\partial^2 \ell}{\partial \beta_1 \partial \beta_3} = \sum_{i=1}^{n}\left[ \frac{x_iT_2}{2\beta_1^2} - \frac{x_iT_1T_2}{2\sigma^2} -
        \frac{y_i^2x_iT_2}{2\sigma^2T_1\beta_1^2} \right] & \\ 
        \frac{\partial^2 \ell}{\partial \beta_1 \partial \sigma^2} = \sum_{i=1}^{n}\left[ \frac{T_1}{2[\sigma^2]^2} -
        \frac{y_i^2}{2[\sigma^2]^2\beta_1^2T_1} \right] & \\ 
        \frac{\partial^2 \ell}{\partial \beta_2 \partial \beta_3} = \sum_{i=1}^{n}\left[ \left\{ \frac{T_2\beta_1}{2\sigma^2} +
          \frac{y_i^2T_2}{2\sigma^2T_1^2\beta_1} \right\}(x_iT_1T_2) + \left\{ \frac{T_1\beta_1}{2\sigma^2} + \frac{1}{2} -
        \frac{y_i^2}{2\sigma^2T_1\beta_1} \right\}(-x_iT_2) \right] & \\ 
        \frac{\partial^2 \ell}{\partial \beta_2 \partial \sigma^2} = \sum_{i=1}^{n}\left[ \frac{y_i^2T_2}{2[\sigma^2]^2T_1\beta_1} -
        \frac{T_1T_2\beta_1}{2[\sigma^2]^2} \right] & \\ 
        \frac{\partial^2 \ell}{\partial \beta_3 \partial \sigma^2} = \sum_{i=1}^{n}\left[ \frac{\beta_1x_iT_1T_2}{2[\sigma^2]^2} -
        \frac{y_i^2x_iT_2}{2[\sigma^2]^2T_1\beta_1} \right]. & \\
      \end{array}
    \]
    See work attached.

    \newpage

  \item Table \ref{tab5.1} reports the value of the gradient of the log likelihood function evaluated at the GLS and MLE estimates.
    Notice that the norm of gradient is larger when evaluated at the GLS estimates than at the MLE estimates. This makes sense since the objective 
    of the maximum likelihood approach is to find a maximum to the log likelihood, which is a point at which the gradient is 0. But as we saw in Part 1, the objective
    of the GLS approach is not quite the same.

    \begin{table}[h]
      \caption{\emph{Gradient of the log likelihood evaluated at the GLS and MLE estimates.}}
      \vspace{.2cm}
      \centering
      \begin{tabular}{|c|c|c|}
        \hline
        & GLS Estimates & MLE Estimates \\
        \hline
        $\frac{\partial \ell}{\partial \beta_1}$ & -0.03582643 & 0.001490834 \\
        \hline
        $\frac{\partial \ell}{\partial \beta_2}$ & 0.7379254 & -0.0229311 \\
        \hline
        $\frac{\partial \ell}{\partial \beta_3}$ & -3.079696 & 0.1291069 \\
        \hline
        $\frac{\partial \ell}{\partial \sigma^{2}}$ & -6.076791 & -0.01993837 \\
        \hline
      \end{tabular}
      \label{tab5.1}
    \end{table}


  % Question 6
  \item Assuming that $\theta$ is now unknown, we used the R function \texttt{optim} once again to estimate all parameters by a standard maximum
    likelihood approach. For starting values we used the estimates from Part 3 for $\bm{\beta}$ and $\sigma^{2}$, and $\theta = 0.5$. Convergence was
    eventually acheived by upping the maximum number of rounds to 1000. The estimates of all parameters along with 95\% Wald theory confidence
    intervals are displayed in Table \ref{tab6.1}.

    \begin{table}[h]
      \caption{\emph{Results of a maximum likelihood approach to estimate the parameters in model \eqref{5}.}}
      \vspace{.2cm}
      \centering
      \begin{tabular}{|c|c|c|}
        \hline
        & Estimate & 95\% Wald theory CI \\
        \hline
        $\beta_1$ & 20.148 & (19.431, 20.864) \\
        \hline
        $\beta_2$ & 3.141 & (2.911, 3.371) \\
        \hline
        $\beta_3$ & 0.523 & (0.475, 0.571) \\
        \hline
        $\sigma^{2}$ & 0.107 & \\
        \hline
        $\theta$ & 0.624 & \\
        \hline
      \end{tabular}
      \label{tab6.1}
    \end{table}

\end{enumerate}


\end{document}

