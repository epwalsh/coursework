\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 520: Assignment 5}
\author{Evan P. Walsh}
\makeatletter
\makeatother
\lhead{Evan P. Walsh}
\chead{STAT 520: Assignment 5}
\rhead{\thepage}
\cfoot{}

\begin{document}
% \maketitle

\subsection*{Nonlinear Regression}

\begin{enumerate}

  \item Consider a model of the form
    \begin{equation}
      Y_{i} = g_1(\bm{x}_i, \bm{\beta}) + \sigma g_2(\bm{x}_i, \bm{\beta}, \theta) \epsilon_i, \ \ i = 1,\dots, n,
      \label{1}
    \end{equation}
    where $\theta$ is known, $\sigma, g_2 > 0$, and $\epsilon_i \sim$ iid N(0,1). According to equation (5.34) in our notes,
    the generalized least squares (GLS) approach to estimating $\bm{\beta}$ in \eqref{1} attempts to find a solution to
    \begin{equation}
      \min_{\bm{\beta}}\sum_{i=1}^{n}\frac{\{y_i - g_1(\bm{x}_i, \bm{\beta})\}^{2}}{g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)}.
      \label{2}
    \end{equation}
    Now, for each $Y_{i}$, the density function is given by 
    \[
      f_{Y_i}(\bm{x}_i|\bm{\beta},\theta) = \{2\pi\sigma^{2}g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)\}^{-1/2}\exp\left[ 
      -\frac{1}{2g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)}\{y_i - g_1(\bm{x}_i,\bm{\beta})\}^{2}\right],
    \]
    and so
    \[
      \ell(\beta|\bm{y}_i) \propto -\log\left[\sigma g_{2}(\bm{x}_i,\bm{\beta},\theta)\right] - \frac{1}{2g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)}\{y_i -
      g_{1}(\bm{x}_i,\bm{\beta})\}^{2}.
    \]
    So, by independence, the MLE estimate of $\bm{\beta}$ is given by the solution to 
    \[
      \max_{\bm{\beta}}\sum_{i=1}^{n}\left(-\log\left[\sigma g_{2}(\bm{x}_i,\bm{\beta},\theta)\right] - \frac{1}{2g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)}\{y_i -
      g_{1}(\bm{x}_i,\bm{\beta})\}^{2}\right),
    \]
    which is equivalent to
    \begin{equation}
      \min_{\bm{\beta}}\sum_{i=1}^{n}\left(\log\left[\sigma g_{2}(\bm{x}_i,\bm{\beta},\theta)\right] + \frac{1}{2g_{2}^{2}(\bm{x}_i,\bm{\beta},\theta)}\{y_i -
      g_{1}(\bm{x}_i,\bm{\beta})\}^{2}\right).
      \label{3}
    \end{equation}
    Since \eqref{2} and \eqref{3} are clearly different objectives, the GLS and MLE estimates may very well differ.

    \vspace{2cm}

    \textbf{For the remainder of the assignment we will assume the model in \eqref{1} is expressed with }
    \begin{align}
      g_1(\bm{x}_i,\bm{\beta}) & = \beta_1\exp\left[ -\exp(\beta_2 - \beta_3x_i) \right] \nonumber \\
      g_2(\bm{x}_i, \bm{\beta}) & = [g_1(\bm{x}_i,\bm{\beta})]^{\theta},
      \label{4}
    \end{align}
    \textbf{i.e. }
    \begin{equation}
      Y_i = \mu_i(\bm{\beta}) + \sigma\{\mu_i(\bm{\beta})\}^{\theta}\epsilon_i,
      \label{5}
    \end{equation}
    \textbf{where }$\mu_i(\bm{\beta}) = \beta_1\exp\left[ -\exp(\beta_2 - \beta_3x_i) \right]$.

    \newpage

  \item Assume $\theta = 0.5$. We use the R function \texttt{nonlin} from the course webpage to estimate $\bm{\beta}$ using generalized least squares.
    Figure \ref{scatterplot} shows the relationship between the response and the covariate. Based on this, we can guestimate that the asymptote
    $\beta_1$ is around 20 and the point of inflection $\beta_2 / \beta_3$ is around 6. After some trial and error, we find that the starting values 
    $\beta_{1}^{(0)} = 20, \beta_{2}^{(0)} = 3$, and $\beta_{3}^{(0)} = 0.5$ give us convergence. Estimates of parameters along with 95\% Wald theory
    confidence intervals are displayed in Table \ref{tab2.1}.

    \begin{figure}[h]
      \caption{\emph{Scatterplot of response variable against the covariate.}}
      \centering
      \includegraphics[width=.7\textwidth]{./figures/hw05_scatterplot.pdf}
      \label{scatterplot}
    \end{figure}


    \begin{table}[h]
      \caption{\emph{Results of generalized least squares to estimate the parameters in model \eqref{5}.}}
      \vspace{.2cm}
      \centering
      \begin{tabular}{|c|c|c|}
        \hline
         & Estimate & 95\% Wald theory CI \\
        \hline
        $\beta_1$ & 20.062 & (19.365, 20.760) \\
        \hline
        $\beta_2$ & 3.224 & (2.921, 3.427) \\
        \hline
        $\beta_3$ & 0.539 & (0.480, 0.598) \\
        \hline
        $\beta_2 / \beta_3$ & 5.985 & (5.797, 6.172) \\
        \hline
        $\sigma^{2}$ & 0.2096 & \\
        \hline
      \end{tabular}
      \label{tab2.1}
    \end{table}

  \item

\end{enumerate}


\end{document}

