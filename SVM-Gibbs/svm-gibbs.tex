\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}
\usepackage{eufrak}

% \declaretheoremstyle[headfont=\normalfont]{normal}
% \declaretheorem[style=normal]{Theorem}
% \declaretheorem[style=normal]{Proposition}
% \declaretheorem[style=normal]{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{Solution}{\stepcounter{ProofCounter}\textbf{Solution:}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\lhead{Notes on Geometric Ergodicity of a Gibbs Sampler for Bayesian SVM}
\chead{}
\rhead{\thepage}
\cfoot{}

\begin{document}\thispagestyle{empty}
\begin{center}
  \Large \textsc{notes -- GIBBS FOR BAYESIAN SVM -- spring 2017} \\ 
  \vspace{5mm}
  \large Evan Pete Walsh
\end{center}
\vspace{1cm}

We attempt to prove geometric ergodic via a drift condition for a Gibbs sampler from a Bayesian SVM. 

\section{Full Conditional Distributions}

Let $p(x|\cdot)$ generically denote the conditional density of $x$ given all other random variables involved. Then, for $\beta \in \mathbb{R}^{k}$,
$\lambda \in \mathbb{R}_{+}^{n}$, and $\omega \in \mathbb{R}_{+}^{k}$, we have by \cite{svm}
\[
  p(\beta|\cdot) \sim N(\mu_{\beta}, \Sigma_{\beta}),
\]
where 
\begin{align*}
  \Sigma_{\beta} & := [\nu^{-2}\Sigma^{-1}\Omega^{-1} + X_{y}^{T}\Lambda^{-1} X_{y}]^{-1} \in \mathbb{R}^{k\times k}, \\
  \mu_{\beta} & := \Sigma_{\beta}X_{y}^{T}(\bm{1} + \lambda^{-1}) \in \mathbb{R}^{k}, \\
  \Sigma & := \text{diag}(\sigma_1^2,\dots, \sigma_k^2) \in \mathbb{R}^{k\times k}, \\
  \Omega & := \text{diag}(\omega_1, \dots, \omega_k) \in \mathbb{R}^{k\times k}, \\
  \Lambda & := \text{diag}(\lambda_1, \dots, \lambda_n) \in \mathbb{R}^{n\times n}, \\
  \bm{1} & := (1, \dots, 1)^T \in \mathbb{R}^{n}, \\
  X_{y} & := \begin{bmatrix}
    y_1 \bm{x}_1^T \\
    \vdots \\
    y_n \bm{x}_n^T 
  \end{bmatrix} \in \mathbb{R}^{n\times k}, \\
  X & := \begin{bmatrix}
    \bm{x}_1^T \\
    \vdots \\
    \bm{x}_n^T 
  \end{bmatrix} \in \mathbb{R}^{n\times k}.
\end{align*}
Also note that 
\begin{equation}
  X_{y}^T \Lambda^{-1} X_{y} = \begin{bmatrix}
    \sum_{i=1}^{n}\frac{y_i^2 x_{i,1}x_{i,1}}{\lambda_i} & \dots & \sum_{i=1}^{n}\frac{ y_i^2 x_{i,1}x_{i,k}}{\lambda_i} \\
    \vdots & \ddots & \vdots \\
    \sum_{i=1}^{n} \frac{y_i^2 x_{i,1}x_{i,k}}{\lambda_i} & \dots & \sum_{i=1}^{n}\frac{y_i^2 x_{i,k}x_{i,k}}{\lambda_i}
  \end{bmatrix} = \begin{bmatrix}
    \sum_{i=1}^{n}\frac{x_{i,1}^2}{\lambda_i} & \dots & \sum_{i=1}^{n}\frac{ x_{i,1}x_{i,k}}{\lambda_i} \\
    \vdots & \ddots & \vdots \\
    \sum_{i=1}^{n} \frac{x_{i,1}x_{i,k}}{\lambda_i} & \dots & \sum_{i=1}^{n}\frac{ x_{i,k}^2}{\lambda_i}
  \end{bmatrix},
  \label{xtlx}
\end{equation}
since $y_i \in \{-1,1\}$ for all $i = 1,\dots, n$. Here $\Sigma_{\beta}$ is well-defined since $\nu^{-2}\Sigma^{-1}\Omega^{-1} \succ 0$ and $X_{y}^T
\Lambda^{-1} X_{y} \succeq 0$, and so $\nu^{-2}\Sigma^{-1}\Omega^{-1} + X_{y}^T \Lambda^{-1} X_{y} \succ 0$, which implies that $\nu^{-2}\Sigma^{-1}\Omega^{-1} +
X_{y}^T \Lambda^{-1} X_{y}$ is invertible. Further,
\[
  p(\lambda_i|\cdot) \sim \left\{ \begin{array}{cl}
      \mathcal{GIG}\left( \frac{1}{2}, 1, [1 - y_i \bm{x}_i^T\beta]^2 \right) & \text{ if } 1-y_i \bm{x}_i^T \beta \neq 0 \\ \\
      \text{Gamma}\left( \frac{1}{2}, 2 \right) & \text{ if } 1 - y_i\bm{x}_i^T\beta = 0
  \end{array} \right., \text{ for } i=1,\dots, n,
\]
and 
\[
  p(\omega_j|\cdot) \sim \left\{ \begin{array}{cl}
      \mathcal{GIG}\left( \frac{1}{2}, 1, \frac{B_j^2}{\nu^2\sigma_j^2} \right) & \text{ if } \beta_j \neq 0 \\ \\
      \text{Gamma}\left( \frac{1}{2}, 2 \right) & \text{ if } \beta_j = 0
  \end{array} \right.,
  \text{ for } j=1,\dots,k,
\]
where $\mathcal{GIG}(p, a, b)$ denotes a generalized inverse Gaussian distribution with parameters $p \in \mathbb{R}, a > 0, b > 0$ and density
\[
  p(x|p, a, b) = \frac{a^{p/2}}{2b^{p/2}K_{p}(\sqrt{ab})}x^{p - 1}\exp\left( -\frac{1}{2}\left[ ax + \frac{b}{x} \right] \right) \ \text{ for } x > 0.
\]
The first two moments are
\[
  E[X] = \frac{\sqrt{b}K_{p+1}(\sqrt{ab})}{\sqrt{a}K_{p}(\sqrt{ab})}, \qquad E[X^2] = \frac{bK_{p+2}(\sqrt{ab})}{aK_{p}(\sqrt{ab})}
\]
where $K_{p}(y)$ is a modified Bessel function of the second kind. In particular, 
\begin{equation}
  K_p(y) = \frac{1}{2} \int_{0}^{\infty} t^{p-1}\exp\left( -\frac{1}{2}\left[yt + \frac{y}{t}\right] \right) dt.
  \label{eq1}
\end{equation}
Gamma$(a,b)$ denotes a Gamma distribution with expectation $ab$.


\section{Preliminary Results}

The following results may be helpful in our construction of the drift condition. \\

\begin{lemma}
  If $X_{y}$ has full column rank, then $v^2 \Omega \Sigma - \Sigma_{\beta} \succeq 0$ and $[X_{y}^T \Lambda^{-1}X_{y}]^{-1} - \Sigma_{\beta} \succeq 0$.
  \label{l1}
\end{lemma}
\begin{Proof}
  Follows directly from the definition of $\Sigma_{\beta}$ and the Woodbury matrix identity.
\end{Proof} \\

\begin{lemma}
  If $X_{y}$ has full column rank, then $\Lambda - X_{y}[X_{y}^T \Lambda^{-1} X_{y}]^{-1} X_{y}^T \succeq 0$.
  \label{l2}
\end{lemma}
\begin{Proof}
  First note that 
  \[
    \Lambda^{-1/2}X_{y}[X_{y}^T \Lambda^{-1} X_{y}]^{-1} X_{y}^T \Lambda^{-1/2} = \Lambda^{-1/2}X_{y}[X_{y}^T \Lambda^{-1/2}\Lambda^{-1/2} X_{y}]^{-1} X_{y}^T \Lambda^{-1/2}
  \]
  is the perpendicular projection operator onto the column space of $\Lambda^{-1/2}X_{y}$. Thus, 
  \[
    I_n - \Lambda^{-1/2}X_{y}[X_{y}^T \Lambda^{-1} X_{y}]^{-1} X_{y}^T \Lambda^{-1/2}  \succeq 0,
  \]
  i.e. for all $b \in \mathbb{R}^{n}$, $b^T(I_n - \Lambda^{-1/2}X_{y}[X_{y}^T \Lambda^{-1} X_{y}]^{-1} X_{y}^T \Lambda^{-1/2})b \geq 0$,
  where $I_n$ is the $n\times n$ identity matrix. But this implies that 
  \[
    b^{T}\Lambda^{1/2} (I_n - \Lambda^{-1/2}X_{y}[X_{y}^T \Lambda^{-1} X_{y}]^{-1} X_{y}^T \Lambda^{-1/2}) \Lambda^{1/2}b \geq 0
  \]
  for all $b \in \mathbb{R}^{n}$. So
  $\Lambda^{1/2}(I_n - \Lambda^{-1/2}X_{y}[X_{y}^T \Lambda^{-1} X_{y}]^{-1} X_{y}^T \Lambda^{-1/2})\Lambda^{1/2} = \Lambda - X_{y}[X_{y}^T \Lambda^{-1} X_{y}]^{-1}X_{y}^T \succeq 0$.
\end{Proof} \\

\begin{corollary}
  If $X_{y}$ has full column rank, then for each $i = 1,\dots, n$, we have $\lambda_i \geq \bm{x_i}^T \Sigma_{\beta} \bm{x}_i$.
  \label{c0}
\end{corollary}
\begin{Proof}
  First note that $\left( \bm{x}_i^T \Sigma_{\beta} \bm{x}_i \right)_{i=1}^{n}  = \left( (y_i \bm{x}_i)^T \Sigma_{\beta} (y_i \bm{x}_i) \right)_{i=1}^{n}$ are precisely
  the diagonal entries of $X_{y}\Sigma_{\beta} X_{y}^T$. Thus, by Lemma \ref{l1}, Lemma \ref{l2}, and the fact that the diagonal entries of a positive
  semi-definite matrix are non-negative, $\lambda_i \geq y_i^2 x_i \Sigma_{\beta} x_i = x_i \Sigma_{\beta} x_i$ for each $i = 1, \dots, n$.
\end{Proof} \\

\begin{lemma}
  (Pal \& Khare 2014, Proposition A7)
  For $p, x > 0$, $2K_p(x) \leq \left( \frac{2}{x} \right)^{p}\Gamma(p)$, where $\Gamma$ is the Gamma function.
  \label{l3}
\end{lemma}
\begin{Proof}
  By \eqref{eq1}, 
  \[
    2K_p(x) = \int_{0}^{\infty} t^{p-1} \exp\left( -\frac{x}{2}\left[ t + \frac{1}{t} \right] \right)dt 
    \leq \int_{0}^{\infty} t^{p-1}\exp\left( -\frac{xt}{2} \right) = \left( \frac{2}{x} \right)^{p}\Gamma(p).
  \]
\end{Proof} \\

\begin{lemma}
  (Segura 2011, Theorem 2)
  For $p, x > 0$, 
  \[
    \frac{K_{p+1/2}(x)}{K_{p-1/2}(x)} \leq \frac{p + \sqrt{p^2 + x^2}}{x} \leq \frac{2p + x}{x}.
  \]
  \label{l4}
\end{lemma} 

\begin{corollary}
  For $x > 0, p > 1/2$,
  \begin{align*}
    \frac{K_{p+1}(x)}{K_{p-1}(x)} & \leq \frac{\left[(p+1/2) + \sqrt{(p+1/2)^2 + x^2}\right]\times\left[(p-1/2) + \sqrt{(p-1/2)^2 + x^2}\right]}{x^2}
    \\
    & \leq \frac{(2(p + 1/2) + x)^2}{x^2}.
  \end{align*}
  \label{c1}
\end{corollary}

\begin{corollary}
  For $x > 0, p > 3/2$,
  \[
    \frac{K_{p+2}(x)}{K_{p-2}(x)} \leq \frac{(2(p + 3/2) + x)^4}{x^4}.
  \]
  \label{c3}
\end{corollary}

\begin{corollary}
  For $i = 1,\dots, n$ and $j = 1,\dots,k$, we have
  \begin{align*}
    E[\lambda_i^2|\beta_0, \cdot] & = [1 - y_i\bm{x}_i^T\beta_0]^4 \frac{K_{9/2}\left( |1 - y_i\bm{x}_i^T\beta_0| \right)}{K_{1/2}\left( |1 - y_i\bm{x}_i^T\beta_0|
    \right)} \leq \left( 8 + |1 - y_i \bm{x}_i^T \beta_0| \right)^4, \\
    E[\lambda_i^2|\beta_0, \cdot] & = [1 - y_i\bm{x}_i^T\beta_0]^2 \frac{K_{5/2}\left( |1 - y_i\bm{x}_i^T\beta_0| \right)}{K_{1/2}\left( |1 - y_i\bm{x}_i^T\beta_0|
    \right)} \leq \left( 4 + |1 - y_i \bm{x}_i^T \beta_0| \right)^2, \\
    E[\lambda_i|\beta_0, \cdot] & = |1 - y_i\bm{x}_i^T\beta_0| \frac{K_{3/2}\left( |1 - y_i\bm{x}_i^T\beta_0| \right)}{K_{1/2}\left( |1 - y_i\bm{x}_i^T\beta_0|
    \right)} \leq 2 + |1-y_i\bm{x}_i^T\beta_0|,  \\
    E[\omega_j^2|\beta_0, \cdot] & = \frac{\beta_{0,j}^2}{\nu^2 \sigma_j^2} \frac{K_{5/2}\left( |\beta_{0,j}| / (\nu\sigma_j) \right)}{ 
    K_{1/2}\left( |\beta_{0,j}| / (\nu \sigma_{j}) \right)} \leq \left( 4 + \frac{|\beta_{0,j}|}{\nu \sigma_j} \right)^2, \\
    E[\omega_j|\beta_0, \cdot] & = \frac{|\beta_{0,j}|}{\nu\sigma_j} \frac{K_{3/2}\left( |\beta_{0,j}| / (\nu\sigma_j) \right)}{ 
    K_{1/2}\left( |\beta_{0,j}| / (\nu \sigma_{j}) \right)} \leq 2 + \frac{|\beta_{0,j}|}{\nu \sigma_j},
  \end{align*}
  where $\beta_0 = (\beta_{0,1}, \dots, \beta_{0,k})^T$.
  \label{c2}
\end{corollary}


\begin{lemma}
  If $X \sim \mathcal{GIG}(p, a, b)$, then for each $q \in \mathbb{R}$,
  \[
    E[X^q] =\frac{b^{q/2}K_{p+q}(\sqrt{ab})}{a^{q/2}K_p(\sqrt{ab})}.
  \]
  \label{l5}
\end{lemma}
\begin{Proof}
  Let $q \in \mathbb{R}$. Then 
  \begin{align*}
    E[X^q] & = \int_{0}^{\infty} \frac{a^{p/2}}{2b^{p/2}K_p(\sqrt{ab})} x^{p+q-1}\exp\left( -\frac{1}{2}\left[ ax + \frac{b}{x} \right] \right)dx \\
    & = \frac{b^{q/2}K_{p+q}(\sqrt{ab})}{a^{q/2}K_p(\sqrt{ab})}
    \int_{0}^{\infty}\frac{a^{(p+q)/2}}{2b^{(p+q)/2}K_{p+q}(\sqrt{ab})} x^{p+q-1} \exp\left( -\frac{1}{2}\left[ ax + \frac{b}{x} \right] \right)dx \\
    & = \frac{b^{q/2}K_{p+q}(\sqrt{ab})}{a^{q/2}K_p(\sqrt{ab})}.
  \end{align*}
\end{Proof}

\begin{lemma}
  (Pal \& Khare 2014, Proposition A.1)
  Suppose $X_{y} \sim N(\mu, \gamma^2)$. Then for any $\delta \in (0,1)$, 
  \[
    E\left[ |X_{y}|^{-\delta} \right] \leq \frac{\Gamma\left( \frac{1-\delta}{2} \right) 2^{\frac{1-\delta}{2}}}{\gamma^\delta \sqrt{2\pi}}.
  \]
  \label{l6}
\end{lemma}

\begin{lemma}
  For all $x,y \in \mathbb{R}^n \setminus \bm{0}$ and $c > 0$, 
  \[
    \|x + y\|_2^2 \leq (1 + c) \|x\|_2^2 + (1 + c^{-1})\|y\|_2^2.
  \]
  \label{l7}
\end{lemma}
\begin{Proof}
  Let $x, y \in \mathbb{R}^n$, $x, y \neq \bm{0}$, and $c > 0$. Note that for each $k > 0$, $2 \leq k + \frac{1}{k}$. Hence, with $k := c \frac{\|x\|_2}{\|y\|_2}$, we have
  \[
    \|x + y\|_2^2 \leq \|x\|_2^2 + 2\|x\|_2 \|y\|_2 + \|y\|_2^2 \leq \|x\|_2^2 + \left( c\frac{\|x\|_2}{\|y\|_2} + \frac{1}{c}\frac{\|y\|_2}{\|x\|_2}
    \right)\|x\|_2 \|y\|_2 + \|y\|_2^2,
  \]
  which gives us the result.
\end{Proof} \\

\begin{lemma}
  (Chakraborty \& Khare 2016, Proposition A.1)
  For any matrix $B \in \mathbb{R}^{n\times k}$ with $B \neq 0_{n\times k}$, all eigenvalues of $B(B^T B + I_{k})^{-1}B^T$ lie within $[0,1)$, with at
  least one eigenvalue strictly positive. Further, as a porism to the proof in \cite{probit}, the non-zero eigenvalues are of the form 
  \[
    \frac{d_i^2}{1 + d_i^2}, \ \ i \in\{ 1, \dots, k\},
  \]
  where each $d_i$ corresponds the square root of an eigenvalue of $B^T B$.
  \label{l8}
\end{lemma}



\newpage

\section{The Drift Condition}

For a given drift function $V$, we will attempt to prove the following. \\

\begin{theorem}
  There exists some $0 < r < 1$ and $M < \infty$ such that 
  \[
    \int_{\mathcal{X}} V(x)k(x|x_0)\ dx \leq r V(x_0) + M,
  \]
  for all $x_0 \in \mathcal{X}$. Here $x$ may represent $\beta, \lambda, \omega$, or any combination, and $\mathcal{X}$ the corresponding state space.
  \label{thm1}
\end{theorem}

Remark: We have such freedom in how we define $x$ and $\mathcal{X}$ since the convergence rate of any marginal part of the chain is equal to the convergence rate
of the joint chain by \cite{jointvmarg}.

\newpage

\section{First Attempt}

Let $V(\beta) := \sum_{i=1}^{n}(1 - y_i\bm{x}_i^T\beta)^2$. \\

This function of course may not be bounded off compact sets.

\begin{Proof}
  Note that 
  \begin{align}
    \int_{\mathbb{R}^{k}} V(\beta) k(\beta|\beta_0)d\beta & = \int_{\mathbb{R}^{k}}V(\beta) \int_{\mathbb{R}^{n}_+\times \mathbb{R}^{k}_+}
    p(\beta|\lambda,\omega, \cdot)p(\lambda|\beta_0, \cdot)p(\omega|\beta_0, \cdot)\ d(\lambda, \omega)\ d\beta \nonumber \\
    \text{(by Tonelli's Theorem)} \ & = \int_{\mathbb{R}^{n}_+\times\mathbb{R}^{k}_+} \left( \int_{\mathbb{R}^{k}} V(\beta)p(\beta|\lambda, \omega,
    \cdot)\ d\beta \right) p(\lambda|\beta_0,\cdot)p(\omega|\beta_0, \cdot)\ d(\lambda, \omega).
    \label{eq0}
  \end{align}
  We first consider the inner integral, which is simply a version of the conditional expectation $E[V(\beta)|\lambda,\omega,\cdot]$.
  But note that 
  \begin{equation*}
    1 - y_i\bm{x}_i^T\beta | \lambda, \omega, \cdot \sim N\left( 1 - y_i\bm{x}_i^T\mu_{\beta}, y_i^2 \bm{x}_i^T \Sigma_{\beta}\bm{x}_i\right).
    \label{eq2}
  \end{equation*}
  Thus,
  \begin{align}
    E[V(\beta)|\lambda,\omega,\cdot] = \sum_{i=1}^{n}\left[(1 - y_i\bm{x}_i^T\mu_{\beta})^2 + y_i^2 \bm{x}_i^T \Sigma_{\beta} \bm{x}_i\right] & = \sum_{i=1}^{n}
    (1 - y_i\bm{x}_i^T\mu_{\beta})^2 + \sum_{i=1}^{n}(y_i\bm{x}_i^T)\Sigma_{\beta}(y_i \bm{x}_i). \nonumber \\
    & = \sum_{i=1}^{n} (1 - y_i\bm{x}_i^T\mu_{\beta})^2 + \text{trace}(X_{y} \Sigma_{\beta} X_{y}^T).
    \label{eq3}
  \end{align}
  Focusing on the first term in \eqref{eq3}, we have 
  \begin{align}
    \sum_{i=1}^{n}(1 - y_i \bm{x}_i^T \mu_{\beta})^2 = \|\mathbf{1} + X_{y}\mu_{\beta}\|_{2}^{2} 
    & \leq \left(1 + \frac{1}{c}\right) \|\mathbf{1}\|_{2}^{2} + \left( 1 + c \right)\|X_{y}\mu_{\beta}\|_{2}^{2} \nonumber \\
    & = \left( 1 + \frac{1}{c} \right) n + (1 + c)\|X_y \mu_\beta \|_2^2,
    \label{eq4}
  \end{align}
  for any $c > 0$ by Lemma \ref{l7}. Further, with $\tilde{X} := \Lambda^{-1/2}X_{y}(\nu^{-2}\Omega^{-1}\Sigma^{-1})^{-1/2}$ and 
  $\ell_{\max} :=$ the maximum eigenvalue of $\tilde{X}[\mathbb{I}_k + \tilde{X}^T \tilde{X}]^{-1} \tilde{X}^T$, we have
  \begin{align}
    \| X_{y} \mu_{\beta}\|_{2}^{2} = \|X_{y}\Sigma_{\beta} X_{y}^T (\bm{1} + \lambda^{-1})\|_{2}^{2} 
    & = \| \Lambda^{1/2}\Lambda^{-1/2} X_{y}\Sigma_{\beta}X_{y}^T \Lambda^{-1/2} \Lambda^{1/2} (\bm{1} + \lambda^{-1})\|_{2}^{2} \nonumber \\
    & = \| \Lambda^{1/2} \tilde{X}[ \mathbb{I}_{k} + \tilde{X}^T \tilde{X} ]^{-1} \tilde{X}^T \Lambda^{1/2} (\mathbf{1} + \lambda^{-1})\|_{2}^{2} \nonumber \\
    & \leq \| \Lambda^{1/2} (\ell_{\max}I_{n}) \Lambda^{1/2}  (\mathbf{1} + \lambda^{-1})\|_{2}^{2} \nonumber \\
    & = \ell_{\max}^2 \| \Lambda (\mathbf{1} + \lambda^{-1})\|_{2}^{2} \nonumber \\
    & = \ell_{\max}^2 \| \lambda + 1\|_{2}^{2} \nonumber \\
    & \leq \ell_{\max}^2 \left[ \left( 1 + \frac{1}{c} \right) \|1\|_{2}^{2} + (1 + c) \|\lambda \|_{2}^{2}\right] \nonumber \\
    & = \ell_{\max}^2 \left[ \left( 1 + \frac{1}{c} \right) n + (1 + c) \|\lambda \|_{2}^{2}\right].
    \label{eq5}
  \end{align}
  So by \eqref{eq4} and \eqref{eq5}, we have 
  \[
    \sum_{i=1}^{n} ( 1 - y_i \bm{x}_i^T \mu_\beta )^2 \leq \left( 1 + \frac{1}{c} \right)n + \ell_{\max}^2(1 + c)\left( 1 + \frac{1}{c}\right) n 
    + \ell_{\max}^2 (1 + c)^2 \|\lambda\|_2^2.
  \]
  Now, by Lemma \ref{l8}, $0 < \ell_{\max} < 1$, so
  \begin{align}
    \sum_{i=1}^{n} ( 1 - y_i \bm{x}_i^T \mu_\beta )^2 & \leq (2 + c)\left( 1 + \frac{1}{c} \right)n + \ell_{\max}^2 (1 + c)^2 \|\lambda\|_2^2
    \nonumber \\
    & = A(c) + \ell_{\max}^2(1 + c)^2 \sum_{i=1}^{n} \lambda_i^2,
    \label{eq6}
  \end{align}
  where $A(c) := (2 + c)\left( 1 + \frac{1}{c} \right) n \in \mathbb{R}_{+}$.
  For the second term in \eqref{eq3}, using the same tricks as in \eqref{eq5}, we get
  \begin{equation}
    \text{trace}(X_y \Sigma_\beta X_{y}^T) \leq \text{trace}(\ell_{\max} \Lambda) = \ell_{\max} \sum_{i=1}^{n} \lambda_i.
    \label{eq7}
  \end{equation}
  Hence by \eqref{eq6} and \eqref{eq7} we now have a bound on the inner expectation given by 
  \begin{align}
    E[V(\beta) | \lambda, \omega, \cdot] & \leq A(c) + \ell_{\max}^2(1 + c)^2 \sum_{i=1}^{n} \lambda_i^2 + \ell_{\max} \sum_{i=1}^{n} \lambda_i
    \nonumber \\
    & \leq A(c) + \ell_{\max}(1 + c)^2 \sum_{i=1}^{n}\lambda_i^2 + \ell_{\max}(1 + c)^2 \sum_{i=1}^{n}\lambda_i \nonumber \\
    & \leq A(c) + \ell_{\max}(1 + c)^2 \sum_{i=1}^{n}\lambda_i^2 + \ell_{\max}(1 + c)^2 \sum_{i=1}^{n}2\lambda_i  + \ell_{\max}(1 + c)^2 n \nonumber \\
    & = A(c) + \ell_{\max}(1 + c)^2 \sum_{i=1}^{n}(\lambda_i + 1)^2 \nonumber \\
    \text{(by Lemma \ref{l7}) } \ & \leq A(c) + \ell_{\max}(1 + c)^2 \left[ \left( 1 + \frac{1}{c} \right)n + (1 + c)\sum_{i=1}^{n}\lambda_i^2 \right] \nonumber \\
    & \leq A(c) + (1 + c)^2\left( 1 + \frac{1}{c} \right)n + \ell_{\max}(1 + c)^3 \sum_{i=1}^{n} \lambda_i^2 \nonumber \\
    & = A'(c) + \ell_{\max}(1 + c)^3 \sum_{i=1}^{n} \lambda_i^2,
    \label{eq9}
  \end{align}
  where $A'(c) := A(c) + (1 + c)^2\left( 1 + \frac{1}{c} \right)n$. By Lemma \ref{l8} again, $\ell_{\max}$ is of the form 
  \begin{equation}
    \ell_{\max} = \frac{d}{1 + d}, 
    \label{eq10}
  \end{equation}
  where $d$ is the square root of the maximum eigenvalue of $\tilde{X}^T
  \tilde{X}$. Further, note that 
  \begin{align*}
    \tilde{X}^T \tilde{X} & = \nu^{-2} \Omega^{-1/2}\Sigma^{-1/2} X_y^T \Lambda^{-1} X_y \Sigma^{-1/2}\Omega^{-1/2} \\
    & = \nu^{-2} \Omega^{-1/2}\Sigma^{-1/2} \begin{bmatrix}
      \sum_{i=1}^{n}\frac{x_{i,1}^2}{\lambda_i} & \dots & \sum_{i=1}^{n}\frac{ x_{i,1}x_{i,k}}{\lambda_i} \\
      \vdots & \ddots & \vdots \\
      \sum_{i=1}^{n} \frac{x_{i,1}x_{i,k}}{\lambda_i} & \dots & \sum_{i=1}^{n}\frac{ x_{i,k}^2}{\lambda_i}
    \end{bmatrix} \Sigma^{-1/2} \Omega^{-1/2} \\
    & = \nu^{-2} \Omega^{-1/2}\Sigma^{-1/2} \begin{bmatrix}
      \frac{1}{\sigma_1 \sqrt{\omega_1}}\sum_{i=1}^{n}\frac{x_{i,1}^2}{\lambda_i} & \dots & \frac{1}{\sigma_k\sqrt{\omega_k}}\sum_{i=1}^{n}\frac{ x_{i,1}x_{i,k}}{\lambda_i} \\
      \vdots & \ddots & \vdots \\
      \frac{1}{\sigma_1 \sqrt{\omega_1}}\sum_{i=1}^{n} \frac{x_{i,1}x_{i,k}}{\lambda_i} & \dots & \frac{1}{\sigma_k\sqrt{\omega_k}}\sum_{i=1}^{n}\frac{ x_{i,k}^2}{\lambda_i}
    \end{bmatrix}  \\
    & = \nu^{-2} \begin{bmatrix}
      \frac{1}{\sigma_1^2 \omega_1}\sum_{i=1}^{n}\frac{x_{i,1}^2}{\lambda_i} & \dots & \frac{1}{\sigma_1\sigma_k\sqrt{\omega_1\omega_k}}\sum_{i=1}^{n}\frac{ x_{i,1}x_{i,k}}{\lambda_i} \\
      \vdots & \ddots & \vdots \\
      \frac{1}{\sigma_1\sigma_k \sqrt{\omega_1\omega_k}}\sum_{i=1}^{n} \frac{x_{i,1}x_{i,k}}{\lambda_i} & \dots & \frac{1}{\sigma_k^2\omega_k}\sum_{i=1}^{n}\frac{ x_{i,k}^2}{\lambda_i}
    \end{bmatrix}.  \\
  \end{align*}
  Thus, since the trace of a symmetric matrix is equal to the sum of the eigenvalues, 
  \begin{equation*}
    d^2 \leq \frac{1}{\nu^{2}} \sum_{j=1}^{k} \frac{1}{\sigma_j^2 \omega_j} \sum_{i=1}^{n} \frac{x_{i,j}^2}{\lambda_i},
  \end{equation*}
  and so 
  \begin{equation}
    d \leq \frac{1}{\nu} \sum_{j=1}^{k} \frac{1}{\sigma_j \sqrt{\omega_j}} \sum_{i=1}^{n} \frac{|x_{i,j}|}{\sqrt{\lambda_i}},
    \label{eq11}
  \end{equation}
  since $\sqrt{a + b} \leq \sqrt{a} + \sqrt{b}$ for all $a, b > 0$. So by \eqref{eq10} and \eqref{eq11}, we have
  \begin{equation}
    \ell_{\max} \leq \frac{ \frac{1}{\nu} \sum_{j=1}^{k} \frac{1}{\sigma_j \sqrt{\omega_j}} \sum_{i=1}^{n} \frac{|x_{i,j}|}{\sqrt{\lambda_i}} }{ 1 + 
    \frac{1}{\nu} \sum_{j=1}^{k} \frac{1}{\sigma_j \sqrt{\omega_j}} \sum_{i=1}^{n} \frac{|x_{i,j}|}{\sqrt{\lambda_i}} }.
    \label{eq12}
  \end{equation}
\end{Proof}



\newpage

\section{Second Attempt}

Let $V(\lambda, \omega) := \sum_{i=1}^{n}[\lambda_i^2 + \lambda^{-\delta}] + \sum_{i=j}^{k}[\omega_j^2 + \omega^{-\delta}]$, $\delta \in (0,1)$.

\section{Third Attempt}

Let $V(\beta, \lambda, \omega) := (\bm{1} + \lambda - X_{y}\beta)^T (\bm{1} + \lambda - X_{y}\beta) + v^{-2} \beta^T \Omega^{-1}\Sigma^{-1}\beta +
\omega^T \omega$.


\newpage

\begin{thebibliography}{9}
  \bibitem{probit}
    Chakraborty, S. and Khare, K. (2016) Convergence properties of Gibbs samplers for Bayesian probit regression with proper priors.
    \emph{Submitted somewhere}.

  \bibitem{lasso}
    Khare, K. and Hobert, J.P. (2013). Geometric ergodicity for the Bayesian lasso. \emph{Electronic Journal of Statistics}, \textbf{7}, 2150-5163.

  \bibitem{shrinkage}
    Pal, S. and Khare, K. (2014). Geometric ergodicity for Bayesian shrinkage models. \emph{Electronic Journal of Statistics}, \textbf{8}, 604-645.

  \bibitem{svm}
    Polson, N.G. and Scott, S.L. (2011). Data augmentation for support vector machines. \emph{Bayesian Analysis}, \textbf{6}, 1-24.

  \bibitem{jointvmarg}
    Roberts, G.O. and Rosenthal, J.S. (2001). Markov chains and de-initialising processes. \emph{Scandinavian Journal of Statistics}, \textbf{28},
    489-504.

  \bibitem{bessel}
    Segura, J. (2011). Bounds for ratios of modified Bessel functions and associated Turan-type inequalities. \emph{Journal of Mathematical Analysis
    and Applications}, \textbf{374}, 516-528.
\end{thebibliography}



\end{document}
