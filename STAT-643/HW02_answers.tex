\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 643: HW 2}
\author{Evan P. Walsh}
\makeatletter
\makeatother
\lhead{Evan P. Walsh}
\chead{STAT 643: HW 2}
\rhead{\thepage}
\cfoot{}

\begin{document}
\maketitle


\subsection*{1}
\begin{tcolorbox}
Suppose that $(X_1,Y_1),\ldots, (X_n,Y_n)$ are iid random vectors and that $X_i$ and $Y_i$ are independently distributed as 
$N(\mu,\sigma^2_1)$ and $N(\mu,\sigma^2_2)$, respectively, with $\bm{\theta}=(\mu,\sigma_1^2,\sigma_2^2)\in \mathbb{R}\times (0,\infty)\times (0,\infty)$.  Let $\bar{X}$ and $S_X^2$ denote the sample mean  and sample variance of the $X_i$'s and let
$\bar{Y}$ and $S_Y^2$ denote the sample mean  and sample variance of the $Y_i$'s.  Show that $(\bar{X},\bar{Y},S_X^2,S_Y^2)$ is minimal sufficient
but not boundedly complete.  What is a first-order ancillary statistic here?
\end{tcolorbox}
\textbf{Solution:} Let $T := (\bar{X}, \bar{Y}, S_{X}^{2}, S_{Y}^{2})$. First we will show that $T$ is minimal sufficient but not boundedly complete.

\begin{Proof}
  Without loss of generality assume $n \geq 2$.
  Let $\mu$ be the Lebesgue measure on $(\mathbb{R}^{2n}, \mathcal{B}(\mathbb{R}^{2n}))$.
  Note that 
  \begin{align*}
    \sum_{i=1}^{n}(x_i - \mu)^{2} & = \sum_{i=1}^{n}(x_{i} - \bar{x} + \bar{x} - \mu)^{2} \\
    & = \sum_{i=1}^{n}(x_i - \bar{x})^{2} + 2\bar{x} \sum_{i=1}^{n} - 2n\bar{x}^{2} - 2\mu\sum_{i=1}^{n}x_i + 2n\mu\bar{x} + n(\bar{x} - \mu)^{2} \\
    & = (n-1)s_{X}^{2} + n(\bar{x} - \mu)^{2}.
  \end{align*}
  By the symmetry of the above argument, the same holds for $y_{1}, \dots, y_{n}$. Hence, by independence,
  \begin{align*}
    \frac{dP_{\theta}}{d\mu}(x_1, \dots, x_n, y_1, \dots, y_n) & = \left(\frac{1}{2\pi\sigma_1 \sigma_2}\right)^{n}\exp\left\{
    -\frac{1}{2\sigma_{1}^{2}}\sum_{i=1}^{n}(x_i - \mu)^{2} - \frac{1}{2\sigma_{2}^{2}}\sum_{i=1}^{n}(y_i - \mu)^{2} \right\} \\
    & = \left(\frac{1}{2\pi\sigma_1 \sigma_2}\right)^{n}\exp\bigg\{ -\frac{1}{2\sigma_{1}^{2}}\left[ 
      (n-1)s_{X}^{2} + n(\bar{x} - \mu)^{2}\right] \\
    & \qquad \qquad\qquad\qquad\ \ \ \  -\frac{1}{2\sigma_{2}^{2}}\left[ (n-1)s_{Y}^{2} + n(\bar{y} - \mu)^{2}\right] \bigg\} \ \text{ a.e. } \mu.
  \end{align*}
  Thus, by the Factorization Theorem, $T$ is sufficient. 

  \begin{claim}
    $T$ is minimal sufficient.
  \end{claim}
  \begin{claimproof}
    To show the minimality of $T$ we will make use of Theorem 06. Let $f_{\theta}$ denote the
    version of the pdf shown above, and suppose
    $(w_1, z_1), \dots, (w_n, z_n)$ is another dataset. Then,
    \begin{align*}
      \frac{f_{\theta}(x_1, \dots, x_n, y_1, \dots, y_n)}{ f_{\theta}(w_1, \dots, w_n, z_1, \dots, z_n)} & =
      \exp\bigg\{ -\frac{1}{2\sigma_{1}^{2}}\left[ (n-1)(s_{X}^{2} - s_{W}^{2}) + n(\bar{x}^{2} - \bar{w}^{2} - 2\mu(\bar{x} - \bar{w})) \right] \\
        & \qquad \qquad - \frac{1}{2\sigma_{2}^{2}}\left[ (n-1)(s_{Y}^{2} - s_{Z}^{2}) + n(\bar{y}^{2} - \bar{z}^{2} - 2\mu(\bar{y} - \bar{z}))
      \right]\bigg\} \\
      & = \exp\bigg\{ -\frac{1}{2\sigma_{1}^{2}}\left[ \sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}w_{i}^{2} - 2n\mu(\bar{x} - \bar{w}) \right] \\
        & \qquad \qquad - \frac{1}{2\sigma_{2}^{2}}\left[ \sum_{i=1}^{n}y_{i}^{2}-\sum_{i=1}^{n}z_{i}^{2} - 2n\mu(\bar{y} - \bar{z})
      \right]\bigg\}, \\
    \end{align*}
    which is free of $\theta$ if and only if $\bar{x} = \bar{w}$, $\bar{y} = \bar{z}$, $\sum x_{i}^{2} = \sum w_{i}^{2}$, and $\sum y_{i}^{2} = \sum
    z_{i}^{2}$ if and only if $\bar{x} = \bar{w}$, $\bar{y} = \bar{z}$, $s_{X}^{2} = s_{W}^{2}$, and $s_{Y}^{2} = s_{Z}^{2}$ by the sample variance
    shortcut formula. Hence $T$ is minimal sufficient by Theorem 06.
  \end{claimproof}

  \begin{claim}
    $T$ is not complete.
  \end{claim}
  \begin{claimproof}
    Since $X_{1},\dots, X_{n}$ are iid $N(\mu, \sigma_{1}^{2})$, it is a well-known fact that $\frac{\bar{X}}{\sqrt{S_{X}^{2} / n}} \sim
    t_{n-1}(\mu)$ (as long as $n > 2$, otherwise is Cauchy if $n = 2$), where $\mu$ is the non-centrality parameter. By symmetry, $\frac{\bar{Y}}{\sqrt{S_{Y}^{2} / n}} \sim t_{n-1}(\mu)$ as well.
    Now, let 
    \[
      \phi(t) := \left\{ \begin{array}{cl}
          -1 & \text{ if } x \leq -1 \\
          t & \text{ if } -1 < x < 1 \\
          1 & \text{ if } x \geq 1
      \end{array} \right., \ \ t \in \mathbb{R}.
    \]
    Then let 
    \[
      h(T) := \phi\left( \frac{\bar{X}}{\sqrt{S_{X}^{2} / n}} \right) - \phi\left( \frac{\bar{Y}}{\sqrt{S_{Y}^{2}/n}} \right).
    \]
    So clearly $h$ is bounded and $E_{\theta}h(T) = 0$ for all $\theta \in \mathbb{R} \times (0,\infty) \times (0,\infty)$, but $h \not\equiv 0$. Thus $T$ is not boundedly complete.

  \end{claimproof}

\end{Proof}


There are many choices for a statistic that is ancillary, the simplest of which is any constant function $S \equiv c$, where $c \in \mathbb{R}$.
A non-trivial example would be $h(T)$ as defined in Claim 2 above since $E_{\theta}h(T) \equiv 0$ for all $\theta \in
\mathbb{R}\times(0,\infty)\times(0,\infty)$.


\newpage
\subsection*{2}
\begin{tcolorbox}
Suppose that $X_1,\ldots,X_n$ are iid $P_\theta$ for $\theta \in \mathbb{R}$, where if $\theta \neq 0$ the distribution $P_\theta$ is $N(\theta,1)$, while $P_0$ is $N(0,2)$.  Show that $\bar{X}=\sum_{i=1}^n X_i/n$ is complete but not sufficient for $\theta$.
\end{tcolorbox}
\textbf{Solution:} Let $X := (X_1, \dots, X_n)$ and $\mu$ be the Lebesgue measure on $\mathbb{R}^{n}$. Then for $x = (x_1, \dots, x_n) \in
\mathbb{R}^{n}$,
\begin{align*}
  \frac{dP_{\theta}^{X}}{d\mu}(x) = f_{\theta}(x) & := \left(2\pi \sigma^{2}(\theta)\right)^{-n/2} \exp\left\{ -\frac{1}{2\sigma^{2}(\theta)}
  \left[ \sum_{i=1}^{n}x_{i}^{2} - 2\theta \sum_{i=1}^{n}x_{i} + n\theta^{2} \right]\right\} \\
  & = \left(2\pi \sigma^{2}(\theta)\right)^{-n/2} \exp\left\{ -\frac{1}{2\sigma^{2}(\theta)}
  \left[ \sum_{i=1}^{n}x_{i}^{2} - 2n\theta \bar{x} + n\theta^{2} \right]\right\} \ \ \text{ a.e. } \mu,
\end{align*}
where $\sigma^{2}(\theta) := I(\theta \neq 0) + 2I(\theta = 0)$. Since $\sum_{i=1}^{n}x_{i}^{2}$ cannot be expressed as a function of $\bar{x}$,
$\bar{x}$ is not sufficient by the Factorization Theorem. Now let $\Theta' := \left\{ \theta \in \mathbb{R} : \theta \neq 0 \right\}$ and
$\mathcal{P}' := \{P_{\theta}^{X} : \theta\neq 0\}$.
So for $\theta \in \Theta'$ and $x = (x_1, \dots, x_n) \in \mathbb{R}^{n}$,
\begin{align*}
  f_{\theta}(x) & = (2\pi)^{-n/2}\exp\left\{ n\theta\bar{x} - \frac{n\theta^{2}}{2} - \frac{1}{2}\sum_{i=1}^{n}x_{i}^{2}  \right\} \\
  & = \exp\left\{ \eta(\theta)\bar{x} \right\}K(\theta)h(x),
\end{align*}
where $\eta(\theta) := n\theta$, $K(\theta) := \exp\left( -\frac{n\theta^2}{2} \right)$, and $h(x) := (2\pi)^{-n/2}\exp\left(
-\frac{1}{2}\sum_{i=1}^{n}x_{i}^{2} \right)$. Note that $\Gamma_{\Theta'} := \{ \eta(\theta) : \theta \in \Theta'\} = \mathbb{R} - \{0\}$, which is
open as a subset of the natural parameter space $\Gamma \equiv \mathbb{R}$ for this family.
Hence, by Claim 22, $\bar{X}$ is complete for $\mathcal{P}'$. But since $\Theta' \subset \Theta$ and $\Theta'$ dominates $\Theta$, $\bar{X}$ is
complete for $\mathcal{P}$ by Theorem 13.

\newpage


\subsection*{3}
\begin{tcolorbox}
Let $\tilde{X}$ be an exponential random variable with mean $\lambda^{-1}>0$.    Suppose that, independent of $\tilde{X}$, $Y$ is exponential with mean 1
and that with $V = \min(\tilde{X},Y)$, one observes $X=(I[V=\tilde{X}],V)$ (so that one either sees $\tilde{X}$ or a random censoring time $Y$
less than $\tilde{X}$ as well as an indicator variable of whether it is $\tilde{X}$ or $Y$ that is seen).
The family of distributions of $X$, $\mathcal{P}\equiv \{P^X_\lambda\}_{\lambda \in (0,\infty)}$, is absolutely continuous with respect to the
product of the counting measure and the Lebesgue measure on $\mathcal{X}=\{0,1\}\times \mathbb{R}$ (call this dominating measure $\mu$).
\begin{enumerate}
  \item Find an R-N derivative of $P_\lambda^X$ with respect to $\mu$ on $\mathcal{X}$.
  \item Suppose that $X_1,\ldots,X_n$ are iid with the distribution of $P_\lambda^X$.  Find a minimal sufficient statistic. \\
    Hint: Claim 23 applies here under the much weaker assumption that there exist $k+1$ points $\bm{\eta}_0,\ldots,\bm{\eta}_k\in \Gamma_\Theta$ such
    that their convex hull contains an open set in $\mathbb{R}^k$, as given on page 44 of {\it Theory of Point Estimation}, 1983, by E.L.~Lehmann.  Here $\Theta=\Lambda \equiv (0,\infty)$ and $k=2$, so the convex hull is the triangle whose vertices consist of $\bm{\eta}_0,\bm{\eta}_1,\bm{\eta}_2$.
\end{enumerate}
\end{tcolorbox}

\textbf{Solution:} Let $X = (X^{(0)}, X^{(1)})$. Then $X^{(0)} = 0$ if and only if $X^{(1)} = Y$ if and only if $\tilde{X} > Y$. Similarly, $X^{(0)} =
1$ if and only if $X^{(1)} = \tilde{X}$ if and only if $\tilde{X} < Y$. Thus, for $A\times B \in 2^{\{0,1\}} \times \mathcal{B}(0,\infty)$, we can
write
\begin{align*}
  P_{\lambda}(X \in A\times B) & = I(0 \in A)\int_{B}e^{-x^{(1)}}\int_{x^{(1)}}^{\infty} \lambda e^{-\lambda t}dm(t)\ dm(x^{(1)})  \\
  & \qquad + I(1 \in A) \int_{B}\lambda e^{-\lambda x^{(1)}} \int_{x^{(1)}}^{\infty} e^{-t} dm(t)\ dm(x^{(1)}) \\
  & = I(0 \in A)\int_{B}e^{-x^{(1)}(1 + \lambda)} dm(x^{(1)}) + I(1 \in A) \int_{B} \lambda e^{-x^{(1)}(1 + \lambda)} dm(x^{(1)}),
\end{align*}
where $x = (x^{(0)}, x^{(1)})$, which implies 
\[
  \frac{dP_{\lambda}^{X}}{d\mu}(x) = I(x^{(0)} = 0) e^{-x^{(1)}(1 + \lambda)} + I(x^{(0)} = 1) \lambda e^{-x^{(1)}(1 + \lambda)} \ \ \text{a.e. } \mu.
\]
Now let $\bm{X} = (X_{1}, X_{2}, \dots, X_{n})$, where $X_1, \dots, X_n$ are iid with distribution $P_{\lambda}^{X}$. Then for 
$\bm{x} = (x_{1}, \dots, x_{n}) \in (\{0,1\}\times (0,\infty))^{n}$,
\begin{align*}
  \frac{dP_{\lambda}^{\bm{X}}}{d\mu^{n}}(\bm{x}) & = \prod_{j=1}^{n} \frac{dP_{\lambda}^{X}}{d\mu}(x_{j}) \\
  & = \exp\left\{ \sum_{j=1}^{n}\log\left[ I(x_{j}^{(0)} = 0) e^{-x_{j}^{(1)}(1 + \lambda)} + I(x_{j}^{(0)} = 1) \lambda e^{-x_{j}^{(1)}(1 + \lambda)}
  \right] \right\} \\
  & = \exp\left\{ \sum_{j=1}^{n}I(x_{j}^{(0)} = 0)\left[ -x_{j}^{(1)}(1 + \lambda) \right] + \sum_{j=1}^{n}\left[ \log(\lambda) - x_{j}^{(1)}(1 +
  \lambda) \right] \right\} \\
  & = \exp\left\{ \log(\lambda)\sum_{j=1}^{n}I(x_{j}^{(0)} = 1) - \lambda\sum_{j=1}^{n}x_{j}^{(1)} - \sum_{j=1}^{n}x_{j}^{(1)} \right\} \ \ \text{a.e.
  } \mu^{n},
\end{align*}
which is the form of an exponential family.
Further, $\Gamma_{\Theta} = \left\{\left(-\lambda, \log(\lambda)\right) : \lambda > 0 \right\} = \left\{ \left(\lambda, \log(-\lambda)\right) : \lambda <
0\right\}$, which describes a curve through $\mathbb{R}^{2}$ (as opposed to a straight line). Hence, by the hint given, the statistic 
\[
  \left( \sum_{j=1}^{n}I(X_{j}^{(0)} = 1), \sum_{j=1}^{n}X_{j}^{(1)} \right)
\]
is minimal sufficient for $\lambda$.


\subsection*{4}
\begin{tcolorbox}
(Truncated Exponential Families) Suppose that the distributions $P_{\bm{\eta}}$ have R-N derivatives with respect to a
$\sigma$-measure $\mu$ of the form $f_{\bm{\eta}}(x) = K(\bm{\eta}) \exp \left(  \sum_{i=1}^k \eta_i T_i(x)\right) h(x)$.
For a measurable set $A$ with $P_{\bm{\eta}}(A)>0$, consider the family of distributions $Q_{\bm{\eta}}^A$ on $A$ having R-N derivatives
with respect to $\mu$ as
\[
  g_{\bm{\eta}}(x ) \propto f_{\bm{\eta}}(x) I[x\in A].
\]
Argue that this is an exponential family and say what you can about the natural parameter space for this family in comparison to that of the $P_{\bm{\eta}}$ family.
\end{tcolorbox}

\textbf{Solution:} Let $\Gamma$ denote the natural parameter space for $\{P_{\bm{\eta}}\}$, and $\Gamma^{A}$ denote the natural parameter space for
$\{Q_{\bm{\eta}}^{A}\}$. By definition $\Gamma$ consists of $\bm{\eta} \in \mathbb{R}^{k}$ such that 
\begin{equation}
  \int_{\mathcal{X}} h(x)\exp\left( \sum_{i=1}^{n}\eta_{i}T_i(x) \right)d\mu(x) < \infty,
  \label{4.0}
\end{equation}
while $\Gamma^{A}$ consists of $\bm{\eta} \in \mathbb{R}^{k}$ such that
\begin{equation}
  \int_{X} h(x)I_A(x)\exp\left( \sum_{i=1}^{n}\eta_{i}T_i(x) \right) d\mu(x) < \infty.
  \label{4.1}
\end{equation}
Since \eqref{4.0} is a stricter condition than \eqref{4.1}, $\Gamma^{A} \supseteq \Gamma$.



\newpage

\subsection*{5}
\begin{tcolorbox}
  Suppose that for $\bm{\theta}=(p_1,p_2)$, $Y\sim$Binomial$(n,p_1)$ and, conditional on $Y=y$, $Z\sim$Binomial$(y,p_2)$. Let $X=(Y,Z)$.  Find $\bm{I}_{X}(\bm{\theta}_0)$.
\end{tcolorbox}


\newpage

\subsection*{6}
\begin{tcolorbox}
  Let $P_0$ and $P_1$ be two distributions on $\mathcal{X}$ and $f_0$ and $f_1$ be their densities with respect to a dominating $\sigma$-finite
  measure $\mu$.  Consider the parametric family of distributions with parameter $\theta \in \Theta \equiv [0,1]$ and densities with respect to $\mu$
  of the form
  \[
    f_\theta(x) = (1-\theta) f_0(x) +\theta f_1(x).
  \]
  Suppose that $X$ has the density $f_\theta$ and $\theta$ has a distribution $G$ on $\Theta$.
  \begin{enumerate}
    \item If $G(\{0\})=G(\{1\})=1/2$, find the posterior distribution of $\theta|X$.
    \item Suppose now that $G$ is the uniform distribution on $[0,1]$.  Find the posterior distribution of $\theta|X$ and answer  the following:

      \begin{enumerate}
        \item What is the mean of this posterior distribution (which provides a Bayesian way of creating a point estimator for $\theta$)?
        \item Consider the partition of $\Theta$ into $\Theta_0=[0,0.5]$ and $\Theta_1=(0.5,1]$.  One Bayesian way of inventing a test for $H_0: \theta \in \Theta_0$ is to decide in favor of $H_0$ if the posterior probability assigned to $\Theta_0$ is at least $0.5$. Describe as explicitly as you can the subset of $\mathcal{X}$ that favors $H_0$.
      \end{enumerate}
  \end{enumerate}
\end{tcolorbox}

\textbf{Solution:}

\begin{enumerate}
  \item Let $x \in \mathcal{X}$. Then
    \begin{equation}
      \int_{\Theta}f_{\theta}(x)dG(\theta) = \frac{1}{2}f_{\theta}(x)\bigg|_{\theta = 0} + \frac{1}{2}f_{\theta}(x)\bigg|_{\theta = 1} =
      \frac{1}{2}[f_0(x) + f_1(x)].
      \label{6.1}
    \end{equation}
    Now let $C \in \mathcal{C}$, the $\sigma$-algebra associated with $\Theta$. Then the posterior distribution of $\theta | X$ is given by
    \begin{align*}
      \pi^{\theta|X}(C|X=x) = \int_{C} \frac{f_{\theta}(x)}{\int_{\Theta}f_{\tilde{\theta}}(x)dG(\tilde{\theta})}dG(\theta) 
      & = \frac{ 2\int_{C}f_{\theta}(x)dG(\theta) }{ f_0(x) + f_1(x) } \ \ \text{ by \eqref{6.1}} \\
      & = \frac{ I(0 \in C) f_0(x) + I(1 \in C)f_1(x) }{ f_0(x) + f_1(x) }.
    \end{align*}

  \item Now suppose $G$ is the uniform distribution on $[0,1]$. Then $G$ is actually the Lebesgue measure $\mu$ on $[0,1]$. So,
    \begin{equation}
      \int_{\Theta} f_{\theta}(x) dG(\theta) = \int_{[0,1]}(1-\theta)f_0(x) + \theta f_1(x)\ d\mu(\theta) = \frac{1}{2}[ f_0(x) - f_1(x) ].
      \label{6.2}
    \end{equation}
    Thus, for $C \in \mathcal{C} = \mathcal{B}[0,1]$, 
    \[
      \pi^{\theta|X}(C|X=x) = \frac{ 2\int_{C}f_{\theta}(x)d\mu(\theta) }{ f_0(x) + f_1(x) } = \frac{ 2\int_{C} (1-\theta) f_0(x) + \theta f_1(x) \
      d\mu(\theta) }{ f_0(x) + f_1(x) }.
    \]
    This implies that the Radon-Nikodym derivative of $\pi^{\theta | X}$ with respect to $\mu$ is given by
    \begin{equation}
      \frac{ 2(1 - \theta) f_0(x) + 2\theta f_1(x) }{ f_0(x) + f_1(x) } \ \ \text{ a.e. } \mu.
      \label{6.3}
    \end{equation}
    \begin{enumerate}
      \item By \eqref{6.3}, the posterior mean is given by 
        \begin{align*}
          E(\theta) := \frac{ 2 \int_{0}^{1} \theta(1 - \theta) f_0(x) + \theta^{2}f_1(x) \ d\mu(\theta) }{ f_0(x) + f_1(x) } 
          & = \frac{2 \left[ \frac{1}{2}f_0(x) - \frac{1}{3}f_0(x) + \frac{1}{3}f_1(x) \right] }{ f_0(x) + f_1(x) } \\
          & = \frac{ \frac{1}{3}f_0(x) + \frac{2}{3}f_1(x) }{ f_0(x) + f_1(x) }.
        \end{align*}
      \item We are interested in describing the set $A := \{ x \in \mathcal{X} : \pi^{\theta|X}(\Theta_0|X=x) \geq 0.5 \}$. But for any $x \in
        \mathcal{X}$, 
        \begin{align*}
          \pi^{\theta | X}(\Theta_0 | X=x) = \frac{ 2\int_{0}^{1/2}(1-\theta)f_0(x) + \theta f_1(x) \ d\mu(\theta) }{ f_0(x) + f_1(x) } 
          & = \frac{ 2\left[ \frac{1}{2}f_0(x) - \frac{1}{8} f_0(x) + \frac{1}{8}f_1(x) \right] }{ f_0(x) + f_1(x) } \\
          & = \frac{ \frac{3}{4}f_0(x) + \frac{1}{4}f_1(x) }{ f_0(x) + f_1(x) },
        \end{align*}
        which, by inspection, is greater than or equal to 0.5 if and only if $f_0(x) \geq f_1(x)$.
        Thus $A = \{ x \in \mathcal{X} : f_0(x) \geq f_1(x) \}$.
    \end{enumerate}
\end{enumerate}



\end{document}
