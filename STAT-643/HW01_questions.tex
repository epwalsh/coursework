\documentclass[11pt]{article}

\usepackage{epsfig}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{bm}
\usepackage{color}


\setlength{\textwidth}{7.0in} \setlength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{-.5in} \setlength{\textheight}{10in}
\setlength{\topmargin}{-.7in}

\renewcommand{\baselinestretch}{1}

\begin{document}

\vspace{6cm}
\begin{center}\Large \textbf{Homework 1 -- STAT 643} \\
\normalsize \textbf{Due Friday, September 9 (in class)}
\end{center}

\begin{enumerate} \itemsep .5cm
\item Consider a probability space $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2),P)$ where $P$ is defined in terms of a measurable density $f(x,y) \geq 0$ defined by a product measure $\mu_1\times \mu_2$ on $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2))$  such that
    \[
      P(C) = \int_C  f(x,y) d(\mu_{1}\times \mu_2)(x,y),\quad C \in \mathcal{B}(\mathbb{R}^2).
    \]
Define the random vectors $X$ and $Y$ on $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2),P)$  such that $X(x,y)=x$
 and $Y(x,y)=y$ for $(x,y)\in \mathbb{R}^2$ (i.e., $X$ and $Y$ are just first and second coordinate mappings, each taking $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2))$ to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$).  Let $\sigma \langle  Y\rangle =\{ Y^{-1}(A): A \in \mathcal{B}(\mathbb{R}) \} = \{ \mathbb{R}\times A: A\in  \mathcal{B}(\mathbb{R})\} \subset \mathcal{B}(\mathbb{R}^2) $ denote the  $\sigma$-algebra generated by $Y$
 and let $f_Y(y) = \int_{\mathbb{R}} f(x,y) d \mu_1(x)$ for $y\in\mathbb{R}$.  Fix $B\in\mathcal{B}(\mathbb{R})$ and define
 \[
     h(x,y) = \left\{ \begin{array}{cl}
     \int_B f(t,y) d \mu_1(t) /f_Y(y) &\mbox{if $f_Y(y)>0$},\\
     \Phi(B) &\mbox{otherwise},
   \end{array}  \right.
 \]
 where $\Phi(\cdot)$ denotes the standard normal distribution. \\[.1cm]
 Show that $h$ is a version of the conditional probability $P(X\in B|Y)\equiv P( X\in B| \sigma \langle  Y\rangle)$.\\[.2cm]
 Note: By Fubini's theorem, one has that $\int_B f(x,y) d \mu_1(x)$ is  a $ \langle(\mathbb{R},\mathcal{B}(\mathbb{R})),(\mathbb{R},\mathcal{B}(\mathbb{R}))\rangle $-measurable function of $y$ (for any given $B\in\mathcal{B}(\mathbb{R})$).



















\item Suppose that the distributions $\mathcal{P}=\{P_\theta\}_{\theta \in \Theta}$ are dominated by a $\sigma$-finite measure $\mu$ and that
$\theta_0\in\Theta$ is such that $f_{\theta_0} = \frac{d P_{\theta_0}}{d \mu}>0$ a.e.~$\mu$.  Consider the function of $x$ and $\theta$ given by
\[
\Delta(\theta,x) = \frac{f_\theta(x)}{f_{\theta_0}(x)}.
\]
The random function of $\theta$, $\Delta(\theta,X)$, can be thought of as a ``statistic."  Argue that it is sufficient.

\item Suppose that $X=(X_1,X_2,\ldots,X_n)$ has independent components, where each $X_i$ is generated as follows.  For independent random variables $W_i\sim N(\mu,1)$  and $Z_i \sim $Poisson$(\mu)$, $X_i=W_i$ with probability $p$ and $X_i=Z_i$ with probability $1-p$.  Suppose that $\mu\in[0,\infty)$.  Use the factorization theorem and find low-dimensional sufficient statistics in the cases that
    \begin{enumerate}
      \item $p$ is known to be $1/2$;
      \item $p\in[0,1]$ is unknown.
    \end{enumerate}
    Note: In the first case, the parameter space is $\Theta =\{1/2\}\times [0,\infty)$, while in the second case it is  $\Theta =[0,1]\times [0,\infty)$.
\item Let $X$ be a sample from $P\in\mathcal{P}$, where $\mathcal{P}$ is a family of distributions on $(\mathbb{R}^k,\mathcal{B}(\mathbb{R}^k))$ (where $\mathcal{P}$ may not necessarily be dominated by a $\sigma$-finite measure $\mu$).  Show that if $T(X)$ is sufficient for $\mathcal{P}$ and $T=\psi(S)$, where $\psi$ is measurable and $S(X)$ is another statistic, then $S(X)$ is sufficient for $\mathcal{P}$.\\[.2cm]
Hint: Fix $B\in \mathcal{B}(\mathbb{R}^k)$.  Pick/fix $P_{\theta_0} \in \mathcal{P}=\{P_\theta:\theta\in \Theta\}$.  To prove $S(X)$ is sufficient, it suffices to show that $P_{\theta_0}(B|\mathcal{B}_S)=P_{\theta}(B|\mathcal{B}_S)$ a.s.~$P_\theta$ holds for any $\theta \in \Theta$, where $\mathcal{B}_S = \sigma\langle S\rangle$.
Pick $\theta \neq \theta_0\in \Theta$.
Form $\tilde{\mathcal{P}}=\{P_{\theta_0},P_\theta\}$, which is dominated by finite measure $\mu = \frac{1}{2}P_{\theta_0} + \frac{1}{2}P_{\theta}$. As  $T$ must be sufficient for $\tilde{\mathcal{P}} \subset \mathcal{P}$, one can use the factorization theorem to the density $f_{\theta_0}$ or $f_{\theta}$ to find that $S$ is sufficient for $\tilde{\mathcal{P}}$ with $\{x: P_{\theta_0}(B|\mathcal{B}_S)(x)\neq P_{\theta}(B|\mathcal{B}_S)(x)\}$ has $\mu$-measure zero.  


\newpage

\item Suppose that $Z$ is exponential with mean $1/\lambda>0$, but that one only observes $X = Z I(Z \geq 1)$ (where $I(\cdot)$ is the indicator function).
    \begin{enumerate}
    \item Consider the measure $\mu$ on $\mathcal{X} = \{0\}\cup [1,\infty)$ consisting of a point mass of 1 at 0 plus the Lebesgue measure on $[1,\infty)$.  Give a formula for the R-N derivative of $P_\lambda^X$ with respect to $\mu$ on $\mathcal{X}$.
        \item Suppose that $X_1,X_2,\ldots,X_n$ are iid with the distribution $P_\lambda^X$.  Find a two-dimensional sufficient statistic for this problem and argue that it is indeed sufficient.
            \item Argue carefully that your statistic from (b) is minimal sufficient.
 \end{enumerate}

\item Consider a family of distributions $\mathcal{P}=\{P_\theta\}_{\theta\in \Theta}$ on $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ which is absolutely continuous with respect to the Lebesgue measure $\mu$, where $f_\theta=\frac{d P_\theta}{d \mu}>0$ for any $\theta\in \Theta$ a.e.~$\mu$.  Let $F_\theta$ be the cdf of $P_\theta$.  For $d\in \mathbb{R}$, let $P_{\theta,d}$ have the density
    \[
      f_{\theta,d}(x) = I(x>d) \frac{f_\theta(x)}{1-F_\theta(d)}
    \]
with respect to the Lebesgue measure $\mu$.  Let $X=(X_1,\ldots,X_n)$ and suppose that $T(X)$ is sufficient for $\theta$ in a model where $X_1,X_2,\ldots,X_n$ are iid $P_\theta$

\begin{enumerate}
\item Prove or give a counter-example to that $[T(X),\min_{1\leq i \leq n} X_i]$ is sufficient for $(\theta,d)$.
\item If $T(X)$ is minimal sufficient for $\theta$, is $[T(X),\min_{1\leq i \leq n} X_i]$ guaranteed to be minimal sufficient for $(\theta,d)$?
\end{enumerate}
\item  Suppose that $X_1,X_2,\ldots,X_n$ are iid $P_{\bm{\theta}}$ for $\bm{\theta}=(\theta_1,\theta_2)\in(0,1)\times\{1,2\}$, where $P_{(\theta_1,1)}$ is the Poisson$(\theta_1)$ distribution and $P_{(\theta_2,2)}$ is the Bernoulli$(\theta_2)$ distribution.  Find a two-dimensional minimal sufficient statistic for $\bm{\theta}$ (argue carefully for minimal sufficiency).
\item Suppose that $X_1,X_2,\ldots,X_n$ are iid $N(\gamma,\gamma^2)$ for $\gamma\in\mathbb{R}$.  Find a minimal sufficient statistic and show that it is not complete.
\end{enumerate}
\end{document}
