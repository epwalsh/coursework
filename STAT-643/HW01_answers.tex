\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 643: HW 1}
\author{Evan Pete Walsh}
\makeatletter
\makeatother
\lhead{Evan Pete Walsh}
\chead{STAT 643: HW 1}
\rhead{\thepage}
\cfoot{}

\begin{document}
% \maketitle

\subsection*{1}
\begin{tcolorbox}
Consider a probability space $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2),P)$ where $P$ is defined in terms of a measurable density $f(x,y) \geq 0$ defined by a product measure $\mu_1\times \mu_2$ on $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2))$  such that
    \[
      P(C) = \int_C  f(x,y) d(\mu_{1}\times \mu_2)(x,y),\quad C \in \mathcal{B}(\mathbb{R}^2).
    \]
Define the random vectors $X$ and $Y$ on $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2),P)$  such that $X(x,y)=x$
 and $Y(x,y)=y$ for $(x,y)\in \mathbb{R}^2$ (i.e., $X$ and $Y$ are just first and second coordinate mappings, each taking $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2))$ to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$).  Let $\sigma \langle  Y\rangle =\{ Y^{-1}(A): A \in \mathcal{B}(\mathbb{R}) \} = \{ \mathbb{R}\times A: A\in  \mathcal{B}(\mathbb{R})\} \subset \mathcal{B}(\mathbb{R}^2) $ denote the  $\sigma$-algebra generated by $Y$
 and let $f_Y(y) = \int_{\mathbb{R}} f(x,y) d \mu_1(x)$ for $y\in\mathbb{R}$.  Fix $B\in\mathcal{B}(\mathbb{R})$ and define
 \[
     h(x,y) = \left\{ \begin{array}{cl}
     \int_B f(t,y) d \mu_1(t) /f_Y(y) &\mbox{if $f_Y(y)>0$},\\
     \Phi(B) &\mbox{otherwise},
   \end{array}  \right.
 \]
 where $\Phi(\cdot)$ denotes the standard normal distribution. \\[.1cm]
 Show that $h$ is a version of the conditional probability $P(X\in B|Y)\equiv P( X\in B| \sigma \langle  Y\rangle)$.
\end{tcolorbox}

{\bf Solution:}

We need to show that 
\begin{enumerate}[label=\roman*.]
  \item $h(x,y)$ is measurable with respect to $(\mathbb{R}^{2}, \sigma\langle Y\rangle )$,\footnote{By this we mean $\langle
      (\mathbb{R}^{2}, \sigma \langle Y\rangle), (\mathbb{R}, \mathcal{B}(\mathbb{R}))\rangle$-measurable.} \\
    \item and for any $G \in \sigma\langle Y\rangle$, 
      \begin{equation}
        \int_{G}h(x,y)\ dP(x,y) = \int_{G} P(X\in B | Y)\ dP.
        \label{1.1}
      \end{equation}
\end{enumerate}

To show i., let $g_{B}(y) := \int_{B}f(t,y)d\mu_{1}(t)$. Note that by Fubini's Theorem $g_{B}(y)$ and $f_{Y}(y)$ are measurable with respect to 
$(\mathbb{B}, \mathcal{B}(\mathbb{R}))$ as functions of just $y$. Hence $g_{B}(y)$ and $f_{Y}(y)$ are measurable with respect to $\sigma\langle
Y\rangle$ as functions of $(x,y)$. It follows easily that $h(x,y)$ is measurable with respect to $(\mathbb{R}^{2}, \mathcal{B}(\mathbb{R}^{2}))$.

To show ii., let $G = \mathbb{R} \times A \in \sigma \langle Y \rangle$. Note that 
\begin{equation}
  \int_{G}P(X\in B | Y)\ dP = \int_{G}E(I_{B\times \mathbb{R}}|Y)\ dP = \int_{G}I_{B\times \mathbb{R}}\ dP = P(\mathbb{R}\times A \cap B \times
  \mathbb{R}) = P(B\times A).
  \label{1.2}
\end{equation}
Further, note that $h(x,y) \equiv \frac{1}{f_{Y}(y)}\int_{B}f(t,y)d\mu_{1}(t)$ almost surely. Hence,
\begin{align*}
  \int_{G}h(x,y)\ dP(x,y) & = \int_{G}\frac{1}{f_{Y}(y)}\int_{B}f(t,y)d\mu_{1}(t)\ dP(x,y) \\
  & = \int_{G}\frac{1}{f_{Y}(y)}\int_{B}f(t,y)d\mu_{1}(t) \cdot f(x,y)\ d(\mu_{1}\times \mu_{2})(x,y) \\
  & \qquad \text{(since $f$ is the R-N derivative)} \\
  & = \int_{A}\int_{\mathbb{R}}\frac{1}{f_{Y}(y)}\int_{B}f(t,y)d\mu_{1}(t)\cdot f(x,y)\ d\mu_{1}(x)d\mu_{2}(y) \ \ \text{(by Tonelli's)} \\
  & = \int_{A}\int_{B}f(t,y)\ d\mu_{1}(t)d\mu_{2}(y) \\
  & = \int_{B\times A}f(t,y)\ d(\mu_{1}\times \mu_{2})(t,y) \ \ \text{(by Tonelli's again)} \\
  & = P(B\times A) = \int_{G}P(X \in B| Y) \ dP \ \ \text{by \eqref{1.2}.}
\end{align*}
Thus \eqref{1.1} is satisfied.


\subsection*{2}
\begin{tcolorbox}
  Suppose that the distributions $\mathcal{P}=\{P_\theta\}_{\theta \in \Theta}$ are dominated by a $\sigma$-finite measure $\mu$ and that
  $\theta_0\in\Theta$ is such that $f_{\theta_0} = \frac{d P_{\theta_0}}{d \mu}>0$ a.e.~$\mu$.  Consider the function of $x$ and $\theta$ given by
  \[
    \Delta(\theta,x) = \frac{f_\theta(x)}{f_{\theta_0}(x)}.
  \]
  The random function of $\theta$, $\Delta(\theta,X)$, can be thought of as a ``statistic."  Argue that it is sufficient.
\end{tcolorbox}

{\bf Solution:}

Note that $\triangle(\theta, X)$ is not a statistic since it a function of $X$ AND $\theta$, but by assumption of the problem we can write
\[ 
  f_{\theta}(x) = \triangle(\theta, x)\cdot f_{\theta_{0}}(x) = g_{\theta}(x) \cdot h(x),
\]
for almost all $x$ (a.e. $\mu$), where $g_{\theta}(x) := \triangle(\theta, x)$, $h(x) := f_{\theta_0}(x)$. Since $\mathcal{P} \ll \mu$, where $\mu$ is
$\sigma$-finite, and $f_{\theta}$ is equal a.e. $\mu$ to the product of a function $h$ that depends only on $x$ and a function $g_{\theta}$ of the
``statistic'', we could say the ``statistic'' is sufficient by the Factorization Theorem.


\newpage
\subsection*{3}
\begin{tcolorbox}
  Suppose that $X=(X_1,X_2,\ldots,X_n)$ has independent components, where each $X_i$ is generated as follows.  For independent random variables $W_i\sim N(\mu,1)$  and $Z_i \sim $Poisson$(\mu)$, $X_i=W_i$ with probability $p$ and $X_i=Z_i$ with probability $1-p$.  Suppose that $\mu\in[0,\infty)$.  Use the factorization theorem and find low-dimensional sufficient statistics in the cases that
  \begin{enumerate}
    \item $p$ is known to be $1/2$;
    \item $p\in[0,1]$ is unknown.
  \end{enumerate}
  Note: In the first case, the parameter space is $\Theta =\{1/2\}\times [0,\infty)$, while in the second case it is  $\Theta =[0,1]\times [0,\infty)$.
\end{tcolorbox}

{\bf Solution:}
Let $\theta := (p, \mu)$. Let $\lambda := m + c$, where $m$ is Lebesgue measure on $\mathbb{R}$ and $c$ is the counting measure on $\mathbb{N} := \{0, 1, 2, \dots\}$.
Then $P_{X_{i}} \ll \lambda$ for each $1 \leq i \leq n$, where 
\[
  f_{\theta, i}(x) := \frac{dP_{X_{i}}}{d\lambda}(x) = \left\{ \begin{array}{cl} 
      \frac{p}{\sqrt{2\pi}}e^{-\frac{1}{2}(x-u)^{2}} & \text{ if } x \in \mathbb{R} - \mathbb{N}, \\
      (1-p)\frac{e^{-\mu}\mu^{x}}{x!} & \text{ if } x \in \mathbb{N}
  \end{array} \right. \qquad \text{a.e. }\lambda, \text{ for }x \in \mathbb{R}.
\]
Since $X$ has independent components $P_{X} = P_{X_{1}} \times P_{X_{2}} \times \dots \times P_{X_{n}}$ and 
\begin{align*}
  f_{\theta}(\bm{x}) := \frac{dP_{X}}{d\eta}(\bm{x}) & = \left( \prod_{\{i : x_{i} \in \mathbb{R}\}}\frac{p}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_{i} -
  \mu)^{2}} \right)
  \left( \prod_{\{j : x_{j} \in \mathbb{R}-\mathbb{N}\}}(1-p)\frac{e^{-\mu}\mu^{x_{j}}}{x_{j}!} \right) \\
  & = \left( \frac{p}{\sqrt{2\pi}} \right)^{n - n_{1}}(1-p)^{n_{1}}e^{\mu\sum\{x_{i} : x_{i} \in \mathbb{R}-\mathbb{N}\}}e^{ - \frac{1}{2}\mu^{2}}
  e^{-n_{1}\mu}\mu^{\sum\{x_{j}:x_{j}\in\mathbb{N}\}} \\
  & \qquad \times e^{-\frac{1}{2}\sum\{x_i : x_{i}\in\mathbb{R} - \mathbb{N}\}}\prod_{\{j : x_{j} \in \mathbb{N}\}}\frac{1}{x_{j}!}
  \qquad \text{a.e. }\eta, 
\end{align*}

where $\eta := \lambda \times \dots \times \lambda$ ($n$ times), $\bm{x} := (x_{1}, \dots, x_{n}) \in \mathbb{R}^{n}$, and $n_{1} :=
\sum_{i=1}^{n}I_{\mathbb{N}}(x_{i})$. Since $\eta$ is $\sigma$-finite, we can apply the Factorization Theorem. Thus, when $p$ is unknown, a sufficient
statistics is 
\[
  T(\bm{x}) := \left( n_{1}, \sum\{x_i : x_{i} \in \mathbb{R} - \mathbb{N}\}, \sum\{x_j : x_{j}\in\mathbb{N}\}\right).
\]
When $p$ is known to be $1/2$, a sufficient statistic is
\[
  S(\bm{x}) := \left( \sum\{x_i : x_{i} \in \mathbb{R} - \mathbb{N}\}, \sum\{x_j : x_{j}\in\mathbb{N}\}\right).
\]


\newpage
\subsection*{4}
\begin{tcolorbox}
  Let $X$ be a sample from $P\in\mathcal{P}$, where $\mathcal{P}$ is a family of distributions on $(\mathbb{R}^k,\mathcal{B}(\mathbb{R}^k))$ (where $\mathcal{P}$ may not necessarily be dominated by a $\sigma$-finite measure $\mu$).  Show that if $T(X)$ is sufficient for $\mathcal{P}$ and $T=\psi(S)$, where $\psi$ is measurable and $S(X)$ is another statistic, then $S(X)$ is sufficient for $\mathcal{P}$.
\end{tcolorbox}

{\bf Solution:}

Without loss of generality assume $\Theta$ has at least two elements. 
Let $\theta_{0} \in \Theta$ and $B \in \mathcal{B}$. It suffices to show that $P_{\theta_{0}}[B | \mathcal{B}_{S}] = P_{\theta}[B | \mathcal{B}_{S}]$
a.s. $P_{\theta}$ for all $\theta \in \Theta$. Thus, pick $\theta_1 \in \Theta$, $\theta_1 \neq \theta_0$. Let $\tilde{\mathcal{P}} := \{P_{\theta_0}, P_{\theta_1}\}$.
Then $\tilde{\mathcal{P}} \ll \mu := \frac{1}{2}P_{\theta_0} + \frac{1}{2}P_{\theta_1}$, where $\mu$ is clearly $\sigma$-finite.
As $\tilde{\mathcal{P}} \subseteq \mathcal{P}$, $T$ is sufficient for $\tilde{\mathcal{P}}$. Hence
\[
  \frac{dP_{\theta}}{d\mu}(x) = g_{\theta}(T(x))h(x) \qquad \text{a.e. } \mu
\]
for $\theta \in \{\theta_{1}, \theta_2\}$ by the Factorization Theorem. But then
\[
  \frac{dP_{\theta}}{d\mu}(x) = g_{\theta}(\psi(S(x)))h(x) \qquad \text{a.e. } \mu.
\]
Hence, by the Factorization Theorem once more, $S$ is sufficient for $\tilde{\mathcal{P}}$. So by the definition of sufficiency,
\[ P_{\theta_0}[B|\mathcal{B}_{S}] = P_{\theta_1}[B|\mathcal{B}_{S}] \qquad \text{a.s. } P_{\theta_{1}}. \]
Since $\theta_{1}$ was arbitrary,
\[ P_{\theta_{0}}[B | \mathcal{B}_{S}] = P_{\theta}[B | \mathcal{B}_{S}] \qquad \text{a.s. } P_{\theta},\]
for all $\theta \in \Theta$.



\newpage
\subsection*{5}
\begin{tcolorbox}
  Suppose that $Z$ is exponential with mean $1/\lambda>0$, but that one only observes $X = Z I(Z \geq 1)$ (where $I(\cdot)$ is the indicator function).
  \begin{enumerate}
    \item Consider the measure $\mu$ on $\mathcal{X} = \{0\}\cup [1,\infty)$ consisting of a point mass of 1 at 0 plus the Lebesgue measure on $[1,\infty)$.  Give a formula for the R-N derivative of $P_\lambda^X$ with respect to $\mu$ on $\mathcal{X}$.
    \item Suppose that $X_1,X_2,\ldots,X_n$ are iid with the distribution $P_\lambda^X$.  Find a two-dimensional sufficient statistic for this problem and argue that it is indeed sufficient.
    \item Argue carefully that your statistic from (b) is minimal sufficient.
  \end{enumerate}
\end{tcolorbox}

{\bf Solution:}
\begin{enumerate}[label=(\alph*)]
  \item A version of the Radon-Nikodym derivative of $P_{\lambda}^{X}$ with respect to $\mu$ is given by 
    \[ f_{\lambda}(x) := \left\{ \begin{array}{cl}
          \lambda e^{-\lambda x} & \text{ if } x \geq 1 \\
          1 - e^{-\lambda} & \text{ if } x = 0,
      \end{array} \right. \qquad x \in \mathcal{X}.
    \]

  \item Let $X := (X_{1}, \dots, X_{n})$. Let $P_{\lambda}$ be the distribution of $X$. Since $P_{\lambda}^{X} \ll \mu$ and the $X_{i}$'s are
    independent, $P = P_{\lambda}^{X} \times \dots \times P_{\lambda}^{X} \ll \nu := \mu \times \dots \times \mu$ (multiplication $n$ times).
    Thus, a version of $\frac{dP_{\lambda}}{d\nu}$ is given by 
    \[ \frac{dP_{\lambda}}{d\nu}(x) = \prod_{i=1}^{n}f_{\lambda}(x_{i}) = (1 - e^{-\lambda})^{k}\lambda^{n - k}e^{-\lambda\sum_{i=1}^{n}x_{i}}, 
    \qquad x = (x_{1}, \dots, x_{n}) \in \mathcal{X}^{n}, \]
    where $k \equiv k(x) := \sum_{i=1}^{n}I(x_{i} = 0)$, i.e. $k$ is a statistic that is the number of $x_{i}$'s equal to 0.
    By the Factorization Theorem, $T(x) := \left(k(x), \sum_{i=1}^{n}x_{i}\right)$ is clearly sufficient.

  \item Let $x,y \in \mathcal{X}^{n}$. Since $\frac{dP_{\lambda}}{d\nu} > 0$ on $\mathcal{X}^{n}$, 
    \begin{align*}
    \frac{ \frac{dP_{\lambda}}{d\nu}(x) }{ \frac{dP_{\lambda}}{d\nu}(y)} & = \frac{ (1-e^{-\lambda})^{k(x)}\lambda^{n - k(x)}e^{-\lambda
    \sum_{i=1}^{n}y_{i}} }{ (1-e^{-\lambda})^{k(y)}\lambda^{n - k(y)}e^{-\lambda \sum_{i=1}^{n}y_{i}} } \\
    & = (1 - e^{-\lambda})^{k(x) - k(y)} \lambda^{k(y) - k(x)}e^{\lambda\left( \sum_{i=1}^{n}y_{i} - \sum_{i=1}^{n}x_{i}\right)}
  \end{align*}
  is well-defined and, as we can see, is independent of $\lambda$ if and only if $T(x) = T(y)$. Hence $T$ is minimal sufficient by Theorem 06 in our
  notes.
\end{enumerate}



\newpage
\subsection*{6}
\begin{tcolorbox}
  Consider a family of distributions $\mathcal{P}=\{P_\theta\}_{\theta\in \Theta}$ on $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ which is absolutely continuous with respect to the Lebesgue measure $\mu$, where $f_\theta=\frac{d P_\theta}{d \mu}>0$ for any $\theta\in \Theta$ a.e.~$\mu$.  Let $F_\theta$ be the cdf of $P_\theta$.  For $d\in \mathbb{R}$, let $P_{\theta,d}$ have the density
  \[
    f_{\theta,d}(x) = I(x>d) \frac{f_\theta(x)}{1-F_\theta(d)}
  \]
  with respect to the Lebesgue measure $\mu$.  Let $X=(X_1,\ldots,X_n)$ and suppose that $T(X)$ is sufficient for $\theta$ in a model where $X_1,X_2,\ldots,X_n$ are iid $P_\theta$

  \begin{enumerate}
    \item Prove or give a counter-example to that $[T(X),\min_{1\leq i \leq n} X_i]$ is sufficient for $(\theta,d)$.
    \item If $T(X)$ is minimal sufficient for $\theta$, is $[T(X),\min_{1\leq i \leq n} X_i]$ guaranteed to be minimal sufficient for $(\theta,d)$?
  \end{enumerate}
\end{tcolorbox}

{\bf Solution:}
Assume that $X_{1}, \dots, X_{n}$ are iid from the distribution with pdf $f_{\theta, d}$
\begin{enumerate}[label=(\alph*)]
  \item We will show that $(T(X), S(X) := \min_{i} X_{i})$ is sufficient for $(\theta, d)$. Since $T$ is sufficient for $\Theta$, we can write
    \begin{equation}
      \prod_{i=1}^{n}f_{\theta}(x_{i}) = g_{\theta}(x)h(x)
      \label{6.0}
    \end{equation}
    for $x = (x_{1}, \dots, x_{n}) \in \mathbb{R}^{n}$. Further,
    since the $X_{i}$'s are independent, the density for $X$
    with respect to Lebesgue measure $\nu$ on $\mathbb{R}^{n}$ is equal a.e. $\nu$ to 
    \begin{equation}
      q_{\theta, d}(x) := \prod_{i=1}^{n}\frac{I(x_{i} > d)f_{\theta}(x_{i})}{1 - F_{\theta}(d)} = \frac{I(\min_{i}x_{i} > d)}{(1 - F_{\theta}(d))^{n}}
    \prod_{i=1}^{n}f_{\theta}(x_{i}) = \frac{I(\min_{i} x_{i} > d)g_{\theta}(T(x))h(x)}{(1 - F_{\theta}(d))^{n}}. 
    \label{6.1}
    \end{equation}
    Hence by the Factorization Theorem, $(T, S)$ is sufficient for $(\theta, d)$.

  \item Yes, it turns out that $(T, S)$ is necessarily minimal sufficient if $T$ is minimal sufficient. 
    \begin{proof} 
      $ $
      \begin{claim}
        $S$ is minimal sufficient for $d$ when $\theta$ is fixed. 
      \end{claim}
      \begin{claimproof}
        Fix $\theta_0 \in \Theta$.
        Let 
        \[
          A := \{x \in \mathbb{R}^{n} : f_{\theta_0}(x_{i}) > 0 \ \forall \ 1 \leq i \leq n \}.
        \]
        Then clearly $P_{\theta_0,d}(A) = 1$ for all $d \in \mathbb{R}$.
        Now let $x,y \in A$ and assume that
        \begin{equation}
          q_{\theta_0,d}(x) = q_{\theta_0,d}(y)\cdot \frac{h(x)}{h(y)} \ \ \forall \ d \in \mathbb{R}.
          \label{6.2}
        \end{equation}
        But \eqref{6.2} holds if and only if 
        \begin{equation}
          I(\min_{i} x_{i} > d)g_{\theta_0}(T(x)) = I(\min_{i}y_{i} > d)g_{\theta_0}(T(y)) \ \ \forall \ d \in \mathbb{R}.
          \label{6.3}
        \end{equation}
        Since $g_{\theta_0}(x), g_{\theta_0}(y) > 0$, this implies that $\min_{i} x_{i} = \min_{i}y_{i}$ (otherwise we could choose $d$ in between $\min_{i}
        x_{i}$ and $\min_{i} y_{i}$ and \eqref{6.3} would not hold). Hence, by Theorem 06, $S$ is minimal sufficient for $d$ when $\theta_0 \in \Theta$ is fixed.
      \end{claimproof}

      \vspace{5mm}

      So if $\tilde{S}$ is an other sufficient statistic for $d$ under $\theta_0$, then there exists $U$ such that $S = U\circ \tilde{S}$ a.s. $P_{d,\theta_0}$.


      \begin{claim}
        $T$ is minimal sufficient for $\theta$ for fixed $d_0 \in \mathbb{R}$.
      \end{claim}
      \begin{claimproof}
        Fix $d_0 \in \mathbb{R}$. Let 
        \[ 
          A := \{x \in \mathbb{R}^{n} : \min_{i} x_{i} > d_0\}.
        \]
        Then $P_{\theta, d_0}(A) = 1$ for all $\theta \in \Theta$. Now assume $\tilde{T}$ is another sufficient statistic for $\theta$.
        Then 
        \[ 
          q_{\theta, d_0}(x) = \frac{I(\min_i x_i > d_0)\tilde{g}_{\theta}(\tilde{T}(x)) \tilde{h}(x)}{(1 - F_{\theta}(d_0))^{n}}  \ \text{ a.e. }\nu. 
        \]
        So for $x \in A$, $g_{\theta}(T(x))h(x) = \tilde{g}_{\theta}(\tilde{T}(x))\tilde{h}(x)$ a.e. $\nu$. Thus,
        \begin{align*}
          g_{\theta}(T(x))h(x) & = [g_{\theta}(T(x))I(\min_i x_i \leq d_0) + \tilde{g}_{\theta}(\tilde{T}(x))I(\min_i x_i > d_0)] \\
          & \qquad \qquad \times [h(x)I(\min_i x_i \leq d_0) + \tilde{h}(x)I(\min_i x_i > d_0)] \ \ \text{ a.e. }\nu 
        \end{align*}
        for $x \in \mathbb{R}^{n}$. By the Factorization Theorem, this gives us that $T^{*}(x) := (\bar{T}(x), \min_i x_i)$ is sufficient for $\theta$, where 
        $\bar{T}(x) := T(x)I(\min_i x_i \leq d_0) + \tilde{T}(x)I(\min_i x_i > d)$.
        
        Thus we can write $T = U\circ T^{*}$ a.s. $\mathcal{P}$. So for $x \in A$, $T(x) = U(\tilde{T}(x), 1)$ a.s. $P_{\theta, d_0}$
        for all $\theta \in \Theta$. Thus $T$ is a function of $\tilde{T}$ a.s. on $A$. Hence $T$ is minimal sufficient for $\theta$ when $d_0$ is
        fixed.
      \end{claimproof}

      \vspace{5mm}

      Now suppose $W$ is another sufficient statistic for $(\theta, d)$. It remains to show that we can write $T$ as a function of $W$ a.s.
      $P_{\theta, d}$ for all $(\theta, d) \in \Theta\times \mathbb{R}$. Well, for fixed $\theta_0 \in \Theta$, there exists a function $U$ such that
      $S = U\circ W$ a.s. $P_{\theta_0, d}$ for all $d \in \mathbb{R}$ by Claim 1. But since $f_{\theta_0} > 0$ a.e. $\mu$, $S = U\circ W$ a.e. $\mu$.
      Thus $S = U\circ W$ a.s. $P_{\theta, d}$ for all $(\theta, d) \in \Theta \times \mathbb{R}$. 

      Now, for each $n \in \mathbb{N}$, let $d_n := -n$. By Claim 2, for each $n$ there exists $V_n$ such that $T = V_n\circ W$ a.s. $P_{\theta, d_n}$
      for all $\theta \in \Theta$. Clearly $T = V_n\circ W$ a.s. $P_{\theta, d}$ for any $d \leq d_n$. Thus, 
      \begin{align*}
        T(x) & = V_0\circ W(x) I(\min_i x_i \geq 0) + \sum_{n=1}^{\infty}V_n\circ W(x) I(-n \leq \min_i x_i < -n + 1) \text{ a.s. } P_{\theta, d} \\
        & = V_0\circ W(x) I(U\circ W(x) \geq 0) + \sum_{n=1}^{\infty}V_n\circ W(x) I(-n \leq U\circ W(x) < -n + 1) \text{ a.s. } P_{\theta, d} 
      \end{align*}
      for all $(\theta, d) \in \Theta \times \mathbb{R}$. Hence $(T, S)$ is a function of $W$ a.s. $P_{\theta, d}$ for all $(\theta, d) \in \Theta
      \times \mathbb{R}$.


    \end{proof}

\end{enumerate}



\newpage
\subsection*{7}
\begin{tcolorbox}
  Suppose that $X_1,X_2,\ldots,X_n$ are iid $P_{\bm{\theta}}$ for $\bm{\theta}=(\theta_1,\theta_2)\in(0,1)\times\{1,2\}$, 
  where $P_{(\theta_1,1)}$ is the Poisson$(\theta_1)$ distribution and $P_{(\theta_1,2)}$ is the Bernoulli$(\theta_1)$ distribution.  
  Find a two-dimensional minimal sufficient statistic for $\bm{\theta}$ (argue carefully for minimal sufficiency).
\end{tcolorbox}

{\bf Solution:}
Let $c$ denote the counting measure on $\mathbb{N} = \{0, 1, \dots\}$. Then $P_{\theta} \ll c$ and a version of the Radon-Nikodym derivative
of $P_{\theta}$ with respect to $c$ is given by 
\[
  \frac{dP_{\theta}}{dc}(x) = \frac{e^{-\theta_{1}}\theta_{1}^{x}}{x!}I(\theta_{2} = 1) + \theta_{1}^{x}(1 - \theta_1)^{1-x}I(\theta_2 =
  2)I(x\in\{0,1\}) \qquad x \in \mathbb{N}.
\]
Let $Q_{\theta}$ be the distribution of $X = (X_{1}, \dots, X_{n})$ on $\mathbb{N}^{n}$. Then $Q_{\theta} \ll \mu := c \times \dots \times c$ on
$\mathbb{N}^{n}$, and a version $f_{\theta}$ of the R-N derivative of $Q_{\theta}$ with respect to $\mu$ is 
\begin{align*}
  f_{\theta}(x) & := \prod_{i=1}^{n} \frac{dP_{\theta}}{dc}(x_{i}) \\
  & = \prod_{i=1}^{n}\left[ \frac{e^{-\theta_{1}}\theta_{1}^{x_{i}}}{x_{i}!}I(\theta_{2} = 1) + \theta_{1}^{x_{i}}(1 - \theta_1)^{1-x_{i}}I(\theta_2 = 2)I(x_{i}\in\{0,1\}) \right] \\
  & \qquad \text{(cross terms cancel since $I(\theta_2 = 1)\times I(\theta_2 = 2) \equiv 0$)} \\
  & = \theta_{1}^{\sum_{i=1}^{n}x_{i}}\left[ \frac{ e^{-\theta_1 \sum_{i=1}^{n} x_{i}} }{ \prod_{i=1}^{n}x_{i}!} I(\theta_2 = 1) + (1 - \theta_1)^{n -
    \sum_{i=1}^{n}x_{i}} I(\theta_2 = 2)\prod_{i=1}^{n} I(x_{i} \in \{0,1\}) \right] \\
  & = \theta_{1}^{\sum_{i=1}^{n}x_{i}}\left[ \frac{ e^{-\theta_1 \sum_{i=1}^{n} x_{i}} }{ \prod_{i=1}^{n}x_{i}!} I(\theta_2 = 1) + (1 - \theta_1)^{n -
    \sum_{i=1}^{n}x_{i}} I(\theta_2 = 2) I(\max_i x_{i} \leq 1) \right] \\
  & = \theta_{1}^{\sum_{i=1}^{n}x_{i}}\left[ e^{-\theta_1 \sum_{i=1}^{n} x_{i}} I(\theta_2 = 1) + (1 - \theta_1)^{n -
  \sum_{i=1}^{n}x_{i}} I(\theta_2 = 2) I(\max_i x_{i} \leq 1) \right] \left( \frac{1}{\prod_{i=1}^{n}x_{i}!} \right) \\
  & = g_{\theta}(T(x)) h(x), \qquad x \in \mathbb{N}^{n}, 
\end{align*}
where $h(x) := \left( \frac{1}{\prod_{i=1}^{n}x_{i}!} \right)$, $T(x) := \left( \sum_{i=1}^{n}x_{i}, I(\max_{i} x_{i} \leq 1 )\right)$, and
$g_{\theta}(T(x))$ is everything to the left of $h(x)$. Hence by the Factorization Theorem, $T$ is sufficient for $\theta$.
It remains to show that $T$ is minimal. To that end, let $x, y \in \mathbb{N}$ and suppose that 
\begin{equation}
  f_{\theta}(x) = f_{\theta}(y) \cdot \prod_{i=1}^{n}\frac{y_{i}!}{x_{i}!} \qquad \text{ for all $\theta$. }
  \label{7.1}
\end{equation}
We need to show that \eqref{7.1} implies $T(x) = T(y)$. 
Well, if $\theta_2 = 1$, then by re-writing the equation in \eqref{7.1} we get 
\[
  \theta_{1}^{\sum_{i=1}^{n}x_{i} - \sum_{i=1}^{n}y_{i}}\exp\left\{ \theta_1\left(\sum_{i=1}^{n}y_{i} - \sum_{i=1}^{n}x_{i}\right) \right\} = 1,
\]
which holds if and only if $\sum_{i=1}^{n}x_{i} = \sum_{i=1}^{n}y_{i}$. 
On the other hand, if $\theta_2 = 2$, then \eqref{7.1} implies
\[ 
  \theta_{1}^{\sum_{i=1}^{n}x_{i} - \sum_{i=1}^{n}y_{i}}(1 - \theta_{1})^{\sum_{i=1}^{n}y_{i} - \sum_{i=1}^{n}x_{i}}I(\max_{i} x_{i} \leq 1) =
  I(\max_{i}y_{i} \leq 1),
\]
which holds and only if $I(\max_{i}x_{i} \leq 1) = I(\max_{i}y_{i} \leq 1)$. Hence \eqref{7.1} implies $T(x) = T(y)$, so $T$ is minimal by Theorem 06 from our notes.


\newpage
\subsection*{8}
\begin{tcolorbox}
  Suppose that $X_1,X_2,\ldots,X_n$ are iid $N(\gamma,\gamma^2)$ for $\gamma\in\mathbb{R}$.  Find a minimal sufficient statistic and show that it is not complete.
\end{tcolorbox}

{\bf Solution:}
Let $X := (X_{1}, \dots, X_{n})$, $P_{\gamma}^{X}$ be the distribution of $X$. Then $P_{\gamma}^{X} \ll \mu :=$ Lebesgue measure on $\mathbb{R}^n$.
By independence,
\[ 
  \frac{dP_{\gamma}^{X}}{d\mu}(x) = \left( 2\pi\gamma^{2} \right)^{-n/2}\exp\left\{ -\frac{1}{2\gamma^{2}}\left[ \sum_{i=1}^{n}x_{i}^{2} -
  2\gamma\sum_{i=1}^{n}x_{i} + \gamma^{2} \right] \right\} \qquad \text{a.e. } \mu, \ x \in \mathbb{R}^{n}.
\]
Let $T(x) := (\sum_{i=1}^{n}x_i, \sum_{i=1}^{n}x_{i}^{2})$. Then $T$ is sufficient by the Factorization Theorem. We also claim that $T$ is minimal.
To show this, let $x, y \in \mathbb{R}^{n}$ and suppose $\frac{dP_{\gamma}^{X}}{d\mu}(x) = \frac{dP_{\gamma}^{X}}{d\mu}(y)$ for all $\gamma$.
Then for each $\gamma$
\[
  \exp\left\{ \frac{1}{2\gamma}\left( \sum_{i=1}^{n}y_{i}^{2} - \sum_{i=1}^{n}x_{i}^{2} \right) \right\}\exp\left\{ \frac{1}{\gamma}\left( 
  \sum_{i=1}^{n}x_{i} - \sum_{i=1}^{n}y_{i}\right) \right\} = 1
\]
if and only if 
\[
  \sum_{i=1}^{n}x_{i}^{2} = \sum_{i=1}^{n}y_{i}^{2} \qquad \text{and} \qquad \sum_{i=1}^{n}x_{i} = \sum_{i=1}^{n}y_{i}.
\]
Hence $T$ is minimal sufficient by Theorem 06. Lastly we claim that $T$ is not complete. To show this, first note that the typical unbiased estimate
of the variance of $X_{i}$ can be written as
\[
  s^{2} := \frac{n\sum_{i=1}^{n}X_{i}^{2} - \left( \sum_{i=1}^{n}X_{i} \right)^{2}}{n(n-1)}.
\]
As $s^{2}$ is unbiased, $Es^{2} = \gamma^{2}$. We also know that 
\[
  \gamma^{2} = E\left( \frac{1}{2n}\sum_{i=1}^{n}X_{i}^{2} \right).
\]
Thus, let 
\[
  g(T(X)) := s^{2} - \frac{1}{2n}\sum_{i=1}^{n}X_{i}^{2} = \frac{(n+1)\sum_{i=1}^{n}X_{i}^{2}}{2n(n-1)} - \frac{\left( \sum_{i=1}^{n}X_{i}
  \right)^{2}}{n(n-1)}.
\]
Then $g(T)$ has expected value of 0 yet is not identically 0. So $T$ is not complete.




\end{document}

