\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 643: HW 1}
\author{Evan Pete Walsh}
\makeatletter
\makeatother
\lhead{Evan Pete Walsh}
\chead{STAT 643: HW 1}
\rhead{\thepage}
\cfoot{}

\begin{document}
% \maketitle

\subsection*{1}
\begin{tcolorbox}
Consider a probability space $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2),P)$ where $P$ is defined in terms of a measurable density $f(x,y) \geq 0$ defined by a product measure $\mu_1\times \mu_2$ on $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2))$  such that
    \[
      P(C) = \int_C  f(x,y) d(\mu_{1}\times \mu_2)(x,y),\quad C \in \mathcal{B}(\mathbb{R}^2).
    \]
Define the random vectors $X$ and $Y$ on $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2),P)$  such that $X(x,y)=x$
 and $Y(x,y)=y$ for $(x,y)\in \mathbb{R}^2$ (i.e., $X$ and $Y$ are just first and second coordinate mappings, each taking $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2))$ to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$).  Let $\sigma \langle  Y\rangle =\{ Y^{-1}(A): A \in \mathcal{B}(\mathbb{R}) \} = \{ \mathbb{R}\times A: A\in  \mathcal{B}(\mathbb{R})\} \subset \mathcal{B}(\mathbb{R}^2) $ denote the  $\sigma$-algebra generated by $Y$
 and let $f_Y(y) = \int_{\mathbb{R}} f(x,y) d \mu_1(x)$ for $y\in\mathbb{R}$.  Fix $B\in\mathcal{B}(\mathbb{R})$ and define
 \[
     h(x,y) = \left\{ \begin{array}{cl}
     \int_B f(t,y) d \mu_1(t) /f_Y(y) &\mbox{if $f_Y(y)>0$},\\
     \Phi(B) &\mbox{otherwise},
   \end{array}  \right.
 \]
 where $\Phi(\cdot)$ denotes the standard normal distribution. \\[.1cm]
 Show that $h$ is a version of the conditional probability $P(X\in B|Y)\equiv P( X\in B| \sigma \langle  Y\rangle)$.
\end{tcolorbox}

{\bf Solution:}

We need to show that 
\begin{enumerate}[label=\roman*.]
  \item $h(x,y)$ is measurable with respect to $(\mathbb{R}^{2}, \sigma\langle Y\rangle )$,\footnote{By this we mean $\langle
      (\mathbb{R}^{2}, \sigma \langle Y\rangle), (\mathbb{R}, \mathcal{B}(\mathbb{R}))\rangle$-measurable.} \\
    \item and for any $G \in \sigma\langle Y\rangle$, 
      \begin{equation}
        \int_{G}h(x,y)\ dP(x,y) = \int_{G} P(X\in B | Y)\ dP.
        \label{1.1}
      \end{equation}
\end{enumerate}

To show i., let $g_{B}(y) := \int_{B}f(t,y)d\mu_{1}(t)$. Note that by Fubini's Theorem $g_{B}(y)$ and $f_{Y}(y)$ are measurable with respect to 
$(\mathbb{B}, \mathcal{B}(\mathbb{R}))$ as functions of just $y$. Hence $g_{B}(y)$ and $f_{Y}(y)$ are measurable with respect to $\sigma\langle
Y\rangle$ as functions of $(x,y)$. It follows easily that $h(x,y)$ is measurable with respect to $(\mathbb{R}^{2}, \mathcal{B}(\mathbb{R}^{2}))$.

To show ii., let $G = \mathbb{R} \times A \in \sigma \langle Y \rangle$. Note that 
\begin{equation}
  \int_{G}P(X\in B | Y)\ dP = \int_{G}E(I_{B\times \mathbb{R}}|Y)\ dP = \int_{G}I_{B\times \mathbb{R}}\ dP = P(\mathbb{R}\times A \cap B \times
  \mathbb{R}) = P(B\times A).
  \label{1.2}
\end{equation}
Further, note that $h(x,y) \equiv \frac{1}{f_{Y}(y)}\int_{B}f(t,y)d\mu_{1}(t)$ almost surely. Hence,
\begin{align*}
  \int_{G}h(x,y)\ dP(x,y) & = \int_{G}\frac{1}{f_{Y}(y)}\int_{B}f(t,y)d\mu_{1}(t)\ dP(x,y) \\
  & = \int_{G}\frac{1}{f_{Y}(y)}\int_{B}f(t,y)d\mu_{1}(t) \cdot f(x,y)\ d(\mu_{1}\times \mu_{2})(x,y) \\
  & \qquad \text{(since $f$ is the R-N derivative)} \\
  & = \int_{A}\int_{\mathbb{R}}\frac{1}{f_{Y}(y)}\int_{B}f(t,y)d\mu_{1}(t)\cdot f(x,y)\ d\mu_{1}(x)d\mu_{2}(y) \ \ \text{(by Tonelli's)} \\
  & = \int_{A}\int_{B}f(t,y)\ d\mu_{1}(t)d\mu_{2}(y) \\
  & = \int_{B\times A}f(t,y)\ d(\mu_{1}\times \mu_{2})(t,y) \ \ \text{(by Tonelli's again)} \\
  & = P(B\times A) = \int_{G}P(X \in B| Y) \ dP \ \ \text{by \eqref{1.2}.}
\end{align*}
Thus \eqref{1.1} is satisfied.


\newpage
\subsection*{2}
\begin{tcolorbox}
  Suppose that the distributions $\mathcal{P}=\{P_\theta\}_{\theta \in \Theta}$ are dominated by a $\sigma$-finite measure $\mu$ and that
  $\theta_0\in\Theta$ is such that $f_{\theta_0} = \frac{d P_{\theta_0}}{d \mu}>0$ a.e.~$\mu$.  Consider the function of $x$ and $\theta$ given by
  \[
    \Delta(\theta,x) = \frac{f_\theta(x)}{f_{\theta_0}(x)}.
  \]
  The random function of $\theta$, $\Delta(\theta,X)$, can be thought of as a ``statistic."  Argue that it is sufficient.
\end{tcolorbox}

{\bf Solution:}

Note that $\triangle(\theta, X)$ is not a statistic since it a function of $X$ AND $\theta$, but by assumption of the problem we can write
\[ 
  f_{\theta}(x) = \triangle(\theta, x)\cdot f_{\theta_{0}}(x) = g_{\theta}(x) \cdot h(x),
\]
for almost all $x$ (a.e. $\mu$), where $g_{\theta}(x) := \triangle(\theta, x)$, $h(x) := f_{\theta_0}(x)$. Since $\mathcal{P} \ll \mu$, where $\mu$ is
$\sigma$-finite, and $f_{\theta}$ is equal a.e. $\mu$ to the product of a function $h$ that depends only on $x$ and a function $g_{\theta}$ of the
``statistic'', we could say the ``statistic'' is sufficient by the Factorization Theorem.


\newpage
\subsection*{3}
\begin{tcolorbox}
  Suppose that $X=(X_1,X_2,\ldots,X_n)$ has independent components, where each $X_i$ is generated as follows.  For independent random variables $W_i\sim N(\mu,1)$  and $Z_i \sim $Poisson$(\mu)$, $X_i=W_i$ with probability $p$ and $X_i=Z_i$ with probability $1-p$.  Suppose that $\mu\in[0,\infty)$.  Use the factorization theorem and find low-dimensional sufficient statistics in the cases that
  \begin{enumerate}
    \item $p$ is known to be $1/2$;
    \item $p\in[0,1]$ is unknown.
  \end{enumerate}
  Note: In the first case, the parameter space is $\Theta =\{1/2\}\times [0,\infty)$, while in the second case it is  $\Theta =[0,1]\times [0,\infty)$.
\end{tcolorbox}

{\bf Solution:}
Let $\theta := (p, \mu)$. Let $\lambda := m + c$, where $m$ is Lebesgue measure on $\mathbb{R}$ and $c$ is the counting measure on $\mathbb{N} := \{0, 1, 2, \dots\}$.
Then $P_{X_{i}} \ll \lambda$ for each $1 \leq i \leq n$, where 
\[
  f_{\theta, i}(x) := \frac{dP_{X_{i}}}{d\lambda}(x) = \left\{ \begin{array}{cl} 
      \frac{p}{\sqrt{2\pi}}e^{-\frac{1}{2}(x-u)^{2}} & \text{ if } x \in \mathbb{R} - \mathbb{N}, \\
      (1-p)\frac{e^{-\mu}\mu^{x}}{x!} & \text{ if } x \in \mathbb{N}
  \end{array} \right. \qquad \text{a.e. }\lambda, \text{ for }x \in \mathbb{R}.
\]
Since $X$ has independent components $P_{X} = P_{X_{1}} \times P_{X_{2}} \times \dots \times P_{X_{n}}$ and 
\begin{align*}
  f_{\theta}(\bm{x}) := \frac{dP_{X}}{d\eta}(\bm{x}) & = \left( \prod_{\{i : x_{i} \in \mathbb{R}\}}\frac{p}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_{i} -
  \mu)^{2}} \right)
  \left( \prod_{\{j : x_{j} \in \mathbb{R}-\mathbb{N}\}}(1-p)\frac{e^{-\mu}\mu^{x_{j}}}{x_{j}!} \right) \\
  & = \left( \frac{p}{\sqrt{2\pi}} \right)^{n - n_{1}}(1-p)^{n_{1}}e^{\mu\sum\{x_{i} : x_{i} \in \mathbb{R}-\mathbb{N}\}}e^{ - \frac{1}{2}\mu^{2}}
  e^{-n_{1}\mu}\mu^{\sum\{x_{j}:x_{j}\in\mathbb{N}\}} \\
  & \qquad \times e^{-\frac{1}{2}\sum\{x_i : x_{i}\in\mathbb{R} - \mathbb{N}\}}\prod_{\{j : x_{j} \in \mathbb{N}\}}\frac{1}{x_{j}!}
  \qquad \text{a.e. }\eta, 
\end{align*}

where $\eta := \lambda \times \dots \times \lambda$ ($n$ times), $\bm{x} := (x_{1}, \dots, x_{n}) \in \mathbb{R}^{n}$, and $n_{1} :=
\sum_{i=1}^{n}I_{\mathbb{N}}(x_{i})$. Since $\eta$ is $\sigma$-finite, we can apply the Factorization Theorem. Thus, when $p$ is unknown, a sufficient
statistics is 
\[
  T(\bm{x}) := \left( n_{1}, \sum\{x_i : x_{i} \in \mathbb{R} - \mathbb{N}\}, \sum\{x_j : x_{j}\in\mathbb{N}\}\right).
\]
When $p$ is known to be $1/2$, a sufficient statistic is
\[
  S(\bm{x}) := \left( \sum\{x_i : x_{i} \in \mathbb{R} - \mathbb{N}\}, \sum\{x_j : x_{j}\in\mathbb{N}\}\right).
\]


\newpage
\subsection*{4}
\begin{tcolorbox}
  Let $X$ be a sample from $P\in\mathcal{P}$, where $\mathcal{P}$ is a family of distributions on $(\mathbb{R}^k,\mathcal{B}(\mathbb{R}^k))$ (where $\mathcal{P}$ may not necessarily be dominated by a $\sigma$-finite measure $\mu$).  Show that if $T(X)$ is sufficient for $\mathcal{P}$ and $T=\psi(S)$, where $\psi$ is measurable and $S(X)$ is another statistic, then $S(X)$ is sufficient for $\mathcal{P}$.
\end{tcolorbox}

{\bf Solution:}

Without loss of generality assume $\Theta$ has at least two elements. 
Let $\theta_{0} \in \Theta$ and $B \in \mathcal{B}$. It suffices to show that $P_{\theta_{0}}[B | \mathcal{B}_{S}] = P_{\theta}[B | \mathcal{B}_{S}]$
a.s. $P_{\theta}$ for all $\theta \in \Theta$. Thus, pick $\theta_1 \in \Theta$, $\theta_1 \neq \theta_0$. Let $\tilde{P} := \{P_{\theta_0, \theta_1}\}$.
Then $\tilde{P} \ll \mu := \frac{1}{2}P_{\theta_0} + \frac{1}{2}P_{\theta_1}$, where $\mu$ is clearly $\sigma$-finite.
As $\tilde{P} \subseteq P$, $T$ is sufficient for $\tilde{P}$. Hence
\[
  \frac{dP_{\theta}}{d\mu}(x) = g_{\theta}(T(x))h(x) \qquad \text{a.e. } \mu
\]
for $\theta \in \{\theta_{1}, \theta_2\}$ by the Factorization Theorem. But then
\[
  \frac{dP_{\theta}}{d\mu}(x) = g_{\theta}(\psi(S(x)))h(x) \qquad \text{a.e. } \mu.
\]
Hence, by the Factorization Theorem once more, $S$ is sufficient for $\tilde{P}$. So by the definition of sufficiency,
\[ P_{\theta_0}[B|\mathcal{B}_{S}] = P_{\theta_1}[B|\mathcal{B}_{S}] \qquad \text{a.s. } P_{\theta_{1}}. \]
Since $\theta_{1}$ was arbitrary,
\[ P_{\theta_{0}}[B | \mathcal{B}_{S}] = P_{\theta}[B | \mathcal{B}_{S}] \qquad \text{a.s. } P_{\theta},\]
for all $\theta \in \Theta$.



\newpage
\subsection*{5}
\begin{tcolorbox}
  Suppose that $Z$ is exponential with mean $1/\lambda>0$, but that one only observes $X = Z I(Z \geq 1)$ (where $I(\cdot)$ is the indicator function).
  \begin{enumerate}
    \item Consider the measure $\mu$ on $\mathcal{X} = \{0\}\cup [1,\infty)$ consisting of a point mass of 1 at 0 plus the Lebesgue measure on $[1,\infty)$.  Give a formula for the R-N derivative of $P_\lambda^X$ with respect to $\mu$ on $\mathcal{X}$.
    \item Suppose that $X_1,X_2,\ldots,X_n$ are iid with the distribution $P_\lambda^X$.  Find a two-dimensional sufficient statistic for this problem and argue that it is indeed sufficient.
    \item Argue carefully that your statistic from (b) is minimal sufficient.
  \end{enumerate}
\end{tcolorbox}

{\bf Solution:}


\newpage
\subsection*{6}
\begin{tcolorbox}
  Consider a family of distributions $\mathcal{P}=\{P_\theta\}_{\theta\in \Theta}$ on $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ which is absolutely continuous with respect to the Lebesgue measure $\mu$, where $f_\theta=\frac{d P_\theta}{d \mu}>0$ for any $\theta\in \Theta$ a.e.~$\mu$.  Let $F_\theta$ be the cdf of $P_\theta$.  For $d\in \mathbb{R}$, let $P_{\theta,d}$ have the density
  \[
    f_{\theta,d}(x) = I(x>d) \frac{f_\theta(x)}{1-F_\theta(d)}
  \]
  with respect to the Lebesgue measure $\mu$.  Let $X=(X_1,\ldots,X_n)$ and suppose that $T(X)$ is sufficient for $\theta$ in a model where $X_1,X_2,\ldots,X_n$ are iid $P_\theta$

  \begin{enumerate}
    \item Prove or give a counter-example to that $[T(X),\min_{1\leq i \leq n} X_i]$ is sufficient for $(\theta,d)$.
    \item If $T(X)$ is minimal sufficient for $\theta$, is $[T(X),\min_{1\leq i \leq n} X_i]$ guaranteed to be minimal sufficient for $(\theta,d)$?
  \end{enumerate}
\end{tcolorbox}

{\bf Solution:}

\newpage
\subsection*{7}
\begin{tcolorbox}
  Suppose that $X_1,X_2,\ldots,X_n$ are iid $P_{\bm{\theta}}$ for $\bm{\theta}=(\theta_1,\theta_2)\in(0,1)\times\{1,2\}$, where $P_{(\theta_1,1)}$ is the Poisson$(\theta_1)$ distribution and $P_{(\theta_2,2)}$ is the Bernoulli$(\theta_2)$ distribution.  Find a two-dimensional minimal sufficient statistic for $\bm{\theta}$ (argue carefully for minimal sufficiency).
\end{tcolorbox}

{\bf Solution:}


\newpage
\subsection*{8}
\begin{tcolorbox}
  Suppose that $X_1,X_2,\ldots,X_n$ are iid $N(\gamma,\gamma^2)$ for $\gamma\in\mathbb{R}$.  Find a minimal sufficient statistic and show that it is not complete.
\end{tcolorbox}

{\bf Solution:}



\end{document}

