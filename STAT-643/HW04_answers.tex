\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{Solution}{\stepcounter{ProofCounter}\textbf{Solution:}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 643: HW 4}
\author{Evan P. Walsh}
\makeatletter
\makeatother
\lhead{Evan P. Walsh}
\chead{STAT 643: HW 4}
\rhead{\thepage}
\cfoot{}

\begin{document}
\maketitle


\newcommand{\E}{\mathrm{E}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\p}{\phi}
\newcommand{\ta}{\theta}
\renewcommand{\baselinestretch}{1}

\subsection*{1}
\begin{tcolorbox}
  Consider the two-state decision problem with $\Theta=\{1,2\}$ and $P_1$ as the Bernoulli$(1/4)$ distribution and
  $P_2$ as the Bernoulli$(1/2)$ distribution. Let $\mathcal{A}=\Theta$ and $L(\theta,a)=I(\theta \neq a)$.
  \begin{enumerate}
    \item[(a)] Find $\mathcal{S}^0$, the set of risk vectors (risk points in $\mathbb{R}^2$) for the four possible non-randomized decision rules here.  Plot these risk points in the plane and then sketch $\mathcal{S}$, the set of all randomized risk vectors.
    \item[(b)] Identify $A(\mathcal{S})$, the set of all admissible risk vectors.  Is there a minimal complete class for this decision problem? If there is one, what is it?  (Note: It can be shown through direct, but tedious, calculation that any element of $\mathcal{D}^*$ has a corresponding element of $\mathcal{D}_*$ with the same risk vector, and vice versa.  You may use this fact without proof. In the finite geometry decision framework, we used randomized rules $\mathcal{D}_*$ and, in discussing complete classes, we used behavioral rules $\mathcal{D}^*$; but this difference generally doesn't matter, in this problem in particular.)
  \end{enumerate}
\end{tcolorbox}
\begin{Solution}
  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{./figures/hw04.png}
    \caption{Risk vectors. The bottom boundary of the polygon given by the thicker line represents the set of admissible risk vectors.}
    \label{fig:1}
  \end{figure}


  (a) The four possible non-randomized decision rules are given by 
  \[ 
    \delta_1(x) := \left\{ \begin{array}{cl} 1 & \text{ if } x = 0 \\ 1 & \text{ if } x = 1 \end{array} \right. \qquad 
    \delta_2(x) := \left\{ \begin{array}{cl} 1 & \text{ if } x = 0 \\ 2 & \text{ if } x = 1 \end{array} \right. 
  \]
  \[ 
    \delta_3(x) := \left\{ \begin{array}{cl} 2 & \text{ if } x = 0 \\ 1 & \text{ if } x = 1 \end{array} \right. \qquad 
    \delta_4(x) := \left\{ \begin{array}{cl} 2 & \text{ if } x = 0 \\ 2 & \text{ if } x = 1 \end{array} \right. 
  \]

  We can then calculate the risk associated with each one as 
  \begin{align*}
    R(1, \delta_1) & = E_{1}I(1 \neq \delta_1) = P_{1}(1 \neq \delta_1) = 0 \\
    R(2, \delta_1) & = P_{2}(2 \neq \delta_1) = 1 \\
    R(1, \delta_2) & = P_{1}(1 \neq \delta_2) = 1/4 \\
    R(2, \delta_2) & = P_{2}(1 \neq \delta_2) = 1/2 \\
    R(1, \delta_3) & = P_{1}(1 \neq \delta_3) = 3/4 \\
    R(2, \delta_3) & = P_{2}(1 \neq \delta_3) = 1/2 \\
    R(1, \delta_4) & = P_{1}(1 \neq \delta_4) = 1 \\
    R(2, \delta_4) & = P_{2}(1 \neq \delta_4) = 0 \\
  \end{align*}

  Thus, the corresponding points in $\mathbb{R}^2$ of the risk vectors associated with each rule are 
  \[
    (0,1) \text{ for } \delta_1, \qquad (1/4, 1/2) \text{ for } \delta_2, \qquad (3/4, 1/2) \text{ for } \delta_3, \qquad \text{ and } (1, 0) \text{
    for } \delta_4.
  \]
  These are the corners of the polygon in Figure \ref{fig:1}. The polygon itself makes up $S$, the set of risk vectors for all randomized decision
  rules, since $S$ is the convex hull of the non-randomized risk vectors. \\

  (b) The set of all admissible rick vectors, $A(\mathcal{S})$, is given by the thick lower sides of the polygon in Figure \ref{fig:1}. We also claim that
  $A(\mathcal{S})$ is a minimal complete class. Well, clearly $A(\mathcal{S})$ is a complete class. Thus, by Theorem 64, $A(\mathcal{D}^{*})$ is
  minimal complete. But $A(\mathcal{D}^*) = A(\mathcal{S})$.

\end{Solution}

\newpage

\subsection*{2}
\begin{tcolorbox}
Consider the two-state decision problem with $\Theta=\{1,2\}$, where the observable $X=(X_1,X_2)$ has iid Bernoulli$(1/4)$
  components when $\theta=1$ and  has iid Bernoulli$(1/2)$
  components when $\theta=2$. Let $\mathcal{A}=\Theta$ and $L(\theta,a)=I(\theta \neq a)$.  Consider the behavioral decision rule $\phi$, with $\phi_x\equiv \phi_{(x_1,x_2)}$, defined as
  \begin{eqnarray*}
    \phi_x(\{1\})=1 &\mbox{if $x_1=0$}\\
    \phi_x(\{1\})=1/2 &\mbox{if $x_1=1$}
  \end{eqnarray*}

  \begin{enumerate}
    \item Show that $\phi$ is inadmissible by finding a decision rule with a better risk function.

    \item Find a behavioral decision rule that is a function of the sufficient statistic $T(X)=X_1+X_2$ and is risk equivalent to $\phi$.

  \end{enumerate}
\end{tcolorbox}

\begin{Solution}
  Let $x = (x_1, x_2)$.  \\
  
  (a) For $\theta = 1$, the risk of $\phi$ is given by 
  \begin{align*}
    R(1, \phi) = \int_{\mathcal{X}}\int_{\mathcal{A}} L(1, a)d\phi_{x}(a)dP_1(x) & = \sum_{x \in \{0,1\}^2} \phi_x(\{a \neq 1\}) P_1(X_1=x_1)P_1(X_2 =
    x_2) \\
    & = (1/2)(1/4)(3/4) + (1/2)(3/4)(3/4) = 3/8,
  \end{align*}
  and similarly for $\theta = 2$,
  \begin{align*}
    R(2,\phi) = \int_{\mathcal{X}}\int_{\mathcal{A}} L(2, a)d\phi_{x}(a)dP_2(x) & = \sum_{x \in \{0,1\}^2} \phi_x(\{a \neq 2\}) P_2(X_1=x_1)P_2(X_2 =
    x_2) \\
    & = (1/2)^2 + (1/2)^2 + (1/2)^3 + (1/2)^3 = 3/4.
  \end{align*}
  Now, set 
  \[
    \phi_x'(\{1\}) := \left\{ \begin{array}{cl} 1 & \text{ if } x_1 = x_2 \\ 1/2 & \text{ if } x_1 \neq x_2. \end{array} \right. 
  \]
  Then, similar to the calculations above, we find that for $\theta = 1$, the risk of $\phi'$ is given by 
  \begin{align*}
    R(1, \phi') = \int_{\mathcal{X}}\int_{\mathcal{A}} L(1, a)d\phi_{x}'(a)dP_1(x) & = \sum_{x \in \{0,1\}^2} \phi_x'(\{a \neq 1\}) P_1(X_1=x_1)P_1(X_2 =
    x_2) \\
    & = (1/2)(3/16) + (1/2)(3/16) \\
    & = 3/16 < 3/8 = R(1,\phi)
  \end{align*}
  and for $\theta = 2$,
  \begin{align*}
    R(2,\phi') = \int_{\mathcal{X}}\int_{\mathcal{A}} L(2, a)d\phi_{x}'(a)dP_2(x) & = \sum_{x \in \{0,1\}^2} \phi_x'(\{a \neq 2\}) P_2(X_1=x_1)P_2(X_2 =
    x_2) \\
    & = (1/4) + (1/2)(1/4) + (1/2)(1/4) + (1/4) \\
    & = 3/4 = R(2, \phi).
  \end{align*}
  Hence $\phi'$ is better than $\phi$, so $\phi$ is inadmissible. \\

  (b) Take $\phi_x'(\{1\}) := 3/4$ for all $x \in \{0,1\}^2$. Then clearly $\phi_x' \equiv \phi_T'$ is a function of $T$, and 
  \begin{align*}
    R(1, \phi') = \int_{\mathcal{X}}\int_{\mathcal{A}} L(1, a)d\phi_{x}'(a)dP_1(x) & = \sum_{x \in \{0,1\}^2} \phi_x'(\{a \neq 1\}) P_1(X_1=x_1)P_1(X_2 =
    x_2) \\
    & = (1/4)(9/16) + (1/4)(3/16) + (1/4)(3/16) + (1/4)(9/16) \\
    & = 3/8 = R(1, \phi),
  \end{align*}
  and 
  \begin{align*}
    R(2,\phi') = \int_{\mathcal{X}}\int_{\mathcal{A}} L(2, a)d\phi_{x}'(a)dP_2(x) & = \sum_{x \in \{0,1\}^2} \phi_x'(\{a \neq 2\}) P_2(X_1=x_1)P_2(X_2 =
    x_2) \\
    & = (3/4)(1/4) + (3/4)(1/4) + (3/4)(1/4) + (3/4)(1/4) \\
    & = 3/4 = R(2,\phi).
  \end{align*}
  Thus $\phi'$ and $\phi$ are risk equivalent.
\end{Solution}


\subsection*{3}
\begin{tcolorbox}
  Let $X_1,\ldots,X_n$ be iid binary random variables with $P_\theta(X_1=1)=\theta=1-P_\theta(X_1=0)$ for $\theta \in(0,1)$.  Consider estimating $\theta$ based on squared error loss and $X=(X_1,\ldots,X_n)$.  Derive the risk functions of the following estimators:
  \begin{enumerate}
    \item the non-randomized rules $\bar{X}$ (sample mean of $X$) and
      \[
        T_0(X) = \left\{\begin{array}{lcl}
            0 && \mbox{if more than half of the $X_i$'s are 0}\\
            1 && \mbox{if more than half of the $X_i$'s are 1}\\
            0.5 && \mbox{if exactly half of the $X_i$'s are 0}
        \end{array}\right.
      \]
      It suffices to express risks for $T_0(X)$ in terms of probabilities involving $\bar{X}$.
    \item the decision rules (involving forms of randomization) given by
      \[
        T_1(X) = \left\{\begin{array}{lcl}
            \bar{X} && \mbox{w.p. $0.5$}\\
            T_0(X) &&  \mbox{w.p. $0.5$}
        \end{array}\right. \quad \& \quad  T_2(X) = \left\{\begin{array}{lcl}
            \bar{X} && \mbox{w.p. $\bar{X}$}\\
            0.5 &&  \mbox{w.p. $1-\bar{X}$}
        \end{array}\right.
      \]
  \end{enumerate}
\end{tcolorbox}

\begin{Solution}
  Let $Y := \sum_{i=1}^{n}X_{i}$ so that $Y$ is Binomial$(n, \theta)$. Let $\theta \in (0,1)$. Then 
  \begin{align*}
    R(\theta, \bar{X}) = \E_{\theta}(\theta - \bar{X})^{2} & = \mathrm{Var}_{\theta}\left[ \bar{X} \right] + [\text{Bias}_{\theta}(\bar{X})]^{2} =
    \frac{\theta(1-\theta)}{n}.
  \end{align*}
  Define $q_1(\theta) := P_{\theta}\left( Y < \frac{n}{2} \right)$, $q_2(\theta) := P_{\theta}\left( Y > \frac{n}{2} \right)$, and $q_3(\theta) := 1 -
  q_{1}(\theta) - q_2(\theta)$. Then,
  \begin{align*}
    R(\theta, T_0(X)) = \E_{\theta}\left[ \theta - T_0(X) \right]^2 & = \E_{\theta}\left[ \theta^2 - 2\theta T_0(X) + T_0(X)^2 \right] \\
    & = \theta^2 - 2\theta \E_{\theta}T_0(X) + \E_{\theta}T_{0}(X)^{2} \\
    & = \theta^2 - 2\theta\left[ q_2(\theta) + \frac{1}{2}q_3(\theta) \right] + \left[ q_2(\theta) + \frac{1}{4}q_3(\theta) \right].
  \end{align*}

  (b) Simply enough, the risk function for $T_1$ is just 
  \[
    R(\theta, T_1) = \E_{\theta}\left[ (\theta - \bar{X})^2(1/2) + (\theta - T_0)^2(1/2) \right] = \frac{1}{2}\left[R(\theta, \bar{X}) +
    R(\theta, T_0)\right].
  \]
  On the other hand,
  \begin{align*}
    R(\theta, T_2) & = \E_{\theta}\left[ (\theta - \bar{X})^2(\bar{X}) + \left( \theta - 1/2 \right)^2(1 - \bar{X}) \right] \\
    & = \E_{\theta}\left[ \frac{\theta^2}{n}Y - \frac{2\theta}{n^2}Y^2 + \frac{1}{n^3}Y^3 + \left( \theta - \frac{1}{2} \right)^2\left( 1 -
        \frac{Y}{n}
    \right) \right] \\
    & = \frac{1}{n^2}\left[ n^2\theta^2 - 2\theta\left\{ n\theta(1-\theta) + n^2\theta^2 \right\} + \theta - 3\theta^2 + 3n\theta^2 + 2\theta^2 -
    3n\theta^3 + n^2\theta^3 \right] + \left( \theta - \frac{1}{2} \right)^2(1 - \theta) \\
    & = \frac{\ta}{n^2}\left[ \ta^2\left\{ 2 - n \right\} + \ta\left\{ n-3 \right\} + 1 \right] + \left( \ta - \frac{1}{2} \right)^2(1 - \ta) \\
    & = \frac{\ta}{n^2}\left[ (1-\ta)(1 + \ta\left\{ n-2 \right\}) \right] + \left( \ta - \frac{1}{2} \right)^2(1 - \ta). \\
  \end{align*}
\end{Solution}

\subsection*{4}
\begin{tcolorbox}
  Find decision rules that are better than $T_1(X)$ and $T_2(X)$, respectively, in Problem~3 for $n\geq 3$.  (Hint: Consider Lemma 51.)
\end{tcolorbox}

\begin{Solution}
  Note that $T_1$ and $T_2$ are risk equivalent to $\phi', \phi'' \in \mathcal{D}^*$, respectively, where 
  \[ 
    \phi_x'(A) := \frac{1}{2}I(\bar{X} \in A) + \frac{1}{2}I(T_0 \in A) \qquad \text{and} \qquad 
    \phi_x''(A) := \bar{X}I(\bar{X} \in A) + (1-\bar{X})I\left( \frac{1}{2} \in A \right),
  \]
  for $A \in \mathcal{A}$. Also note that $\mathcal{A} = (0,1)$ is a convex subset of $\mathbb{R}$ and $L(\theta, \cdot)$ is a strictly convex
  function of $a$ for any $\theta \in \Theta$. Further, since $n \geq 3$ by assumption, 
  \[
    P_{\theta}\left(\left\{ x \in \mathcal{X} : \phi_x' \text{ non-degenerate} \right\} \right) > 0 \text{ and }  
    P_{\theta}\left(\left\{ x \in \mathcal{X} : \phi_x'' \text{ non-degenerate} \right\} \right) > 0.
  \]
  Then define 
  \[
    \delta_1(x) := \int_{\mathcal{A}}ad\phi_x'(a) = \frac{1}{2}\left( \bar{x} + T_0(x) \right) \text{ and } \delta_2(x) :=
    \int_{\mathcal{A}}ad\phi_x''(a) = \bar{x}^2 + \frac{1}{2}(1-\bar{x}),
  \]
  for $x \in \mathcal{X}$. Then by Lemma 51, $R(\theta, \delta_1) < R(\theta, \phi') = R(\theta, T_1)$ and $R(\theta, \delta_2) < R(\theta, \phi'') =
  R(\theta, T_2)$ for all $\theta \in \Theta$. Hence $\delta_1$ and $\delta_2$ are better than $T_1$ and $T_2$, respectively.
\end{Solution}

\newpage

\subsection*{5}
\begin{tcolorbox}
  Can any of the rules in Problem~4 be improved by using the Rao-Blackwell theorem?  Explain why or why not.
\end{tcolorbox}

\begin{Solution}
  It is easy to see by examining the canonical version of the joint distribution function of $X$ that $\bar{X}$ is a minimal sufficient statistic.
  Thus, since $\delta_1$ and $\delta_2$ are functions of $\bar{X}$, they are functions of any other sufficient statistic $T$ because $\bar{X}$ is a
  function of $T$. Hence conditioning $\delta_1$ or $\delta_2$ on any other sufficient statistic $T$ does not change anything, i.e.
  $E_{\theta}[\delta_j|T] = \delta_j$ a.e. $c$, for $j = 1,2$, where $c$ is the counting measure on $\{0,1\}^n$. Therefore we cannot improve the
  decision rules $\delta_1$ and $\delta_2$ by using the Rao-Blackwell Theorem.
\end{Solution}

\subsection*{6}
\begin{tcolorbox}
  Consider the two-state decision problem with $\Theta=\{0,1\}=\mathcal{A}$, where  $P_0$ and $P_1$ have densities $f_0(x)$ and $f_1(x)$
  with respect to a dominating $\sigma$-finite measure $\mu$.  Let the loss function be $L(\theta,a)$ for $a,\theta \in\{0,1\}$.
  \begin{enumerate}
    \item For an arbitrary prior distribution $G$, find a formal Bayes rule with respect to $G$.
    \item Specialize your result in (a) to the case where $L(\theta,a)=I(\theta\neq a)$.  What connection does the form of these Bayes rules have to the theory of simple-versus-simple hypothesis testing (i.e., $H_0:\theta=0$ vs $H_1:\theta=1$)?
  \end{enumerate}
\end{tcolorbox}

\begin{Solution}
  Let $g_j := G(\{j\})$ for $j = 0,1$. \\

  (a) Let $\p \in \D^*$. Let $\alpha_0(x) := g_0L(0,0)f_0(x) + g_1L(1,0)f_1(x)$ and $\alpha_1(x) := g_0L(0,1)f_0(x) + g_1L(1,1)f_1(x)$
  for $x \in \X$.
  Then 
  \begin{align*}
    \text{BR}(G,\p) & = \int_{\Theta}R(\ta, \p)dG(\ta) \\
    & = R(0,\p) g_0 + R(1,\p)g_1 \\
    & = g_0 \int_{\X}\int_{\A}L(0,a)d\p_x(a)dP_0(x) + \int_{\X}\int_{\A}L(1,a)d\p_x(a)dP_1(x) \\
    & = g_0 \int_{\X}\left[ L(0,0)\p_x(\left\{ 0 \right\}) + L(0,1)\p_x\left( \left\{ 1 \right\} \right) \right]f_0(x)d\mu(x) \\
    & \qquad + 
    g_1\int_{\X}\left[ L(1,0)\p_x\left( \left\{ 0 \right\} \right) + L(1,1)\p_x\left( \left\{ 1 \right\} \right) \right]f_1(x)d\mu(x) \\
    & = \int_{\X}\phi_x\left( \left\{ 0 \right\} \right)\overbrace{\left[ g_0L(0,0)f_0(x) + g_1L(1,0)f_1(x) \right]}^{= \alpha_0(x)}d\mu(x) \\
    & \qquad + \int_{\X}\p_x\left( \left\{ 1 \right\} \right)\underbrace{\left[ g_0L(0,1)f_0(x) + g_1L(1,1)f_1(x) \right]}_{=\alpha_1(x)}d\mu(x) \\
    & = \int_{\X}\p_x\left( \left\{ 0 \right\} \right)\alpha_0(x)d\mu(x) + \int_{\X}\left[ 1 - \phi_x\left( \left\{ 0 \right\} \right)
    \right]\alpha_1(x)d\mu(x) \\
    & = \int_{\X}\alpha_1(x)d\mu(x) + \int_{\X}\left[ \alpha_0(x) - \alpha_1(x) \right]\phi_x\left( \left\{ 0 \right\} \right)d\mu(x).
  \end{align*}
  Thus, $\phi$ is a Bayes rule if and only if $BR(G,\phi) \leq BR(G,\phi')$ for all $\phi \in \D^*$, if and only if 
  \begin{equation}
    \int_{\X}\left[ \alpha_0(x) - \alpha_1(x) \right]\phi_x\left( \left\{ 0 \right\} \right)d\mu(x) \leq \int_{\X}\left[ \alpha_0(x) - \alpha_1(x)
    \right]\phi_x'\left( \left\{ 0 \right\} \right)d\mu(x) \ \ \forall \phi' \in \D^*.
    \label{eq:1}
  \end{equation}
  So in order \eqref{eq:1} to hold, $\phi$ must be defined by 
  \[
    \phi_x\left( \left\{ 0 \right\} \right) := \left\{ \begin{array}{cl}
        1 & \text{ if } \alpha_0(x) \leq \alpha_1(x) \\
        0 & \text{ if } \alpha_0(x) > \alpha_1(x) 
    \end{array} \right. 
  \]
  in order to minimize the left-hand side of the inequality in \eqref{eq:1}.

  (b) If we take $L(\theta, a)$ to be the indicator $I(\theta \neq a$, then the Bayes risk of $\phi$ simplifies to 
  \begin{align*}
    BR(G,\phi) & = g_0\int_{\X}\p_x\left( \left\{ 1 \right\} \right)f_0(x)d\mu(x) + g_1\int_{\X}\phi_x\left( \left\{ 0 \right\} \right)f_1(x)d\mu(x)
    \\
    & = g_0 \int_{\X}\left[ 1 - \phi_x\left( \left\{ 0 \right\} \right) \right]f_0(x)d\mu(x) + g_1\int_{\X}\p_x\left( \left\{ 0 \right\}
    \right)f_1(x)d\mu(x) \\
    & = g_0 + \int_{\X}\left[ g_1 f_1(x) - g_0f_0(x) \right]\phi_x\left( \left\{ 0 \right\} \right) d\mu(x),
  \end{align*}
  and so $\phi$ is a Bayes rule if we define 
  \[
    \phi_x\left( \left\{ 0 \right\} \right) := \left\{ \begin{array}{cl}
        1 & \text{ if } g_1 f_1(x) \leq g_0f_0(x) \\
        0 & \text{ if } g_1f_1(x) > g_0f_0(x) 
    \end{array} \right. 
  \]
  in order to minimize $\int_{\X}\left[ g_1 f_1(x) - g_0f_0(x) \right]\phi_x\left( \left\{ 0 \right\} \right) d\mu(x)$.
  Note that since the posterior 
  \[
    \pi(\theta) \propto g_{\theta}f_{\theta}(x) \ \ \text{for } \theta \in \{0,1\},
  \]
  the rule $\phi$ correspondes to the decision rule of a simple Bayes hypothesis test.
\end{Solution}



\end{document}
