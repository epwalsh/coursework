\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 643: Assignment 3}
\author{Evan P. Walsh}
\makeatletter
\makeatother
\lhead{Evan P. Walsh}
\chead{STAT 643: Assignment 3}
\rhead{\thepage}
\cfoot{}

\begin{document}
\maketitle

\newcommand{\E}{\mathrm{E}}
\renewcommand{\baselinestretch}{1}


\subsection*{1}
\begin{tcolorbox}
  Suppose that $X=(T,S)$ and the family $\mathcal{P}$ of distributions of $X$ on $\mathcal{X}=\mathcal{T}\times \mathcal{S}$ is dominated by the measure $\mu=\nu\times \gamma$ (a product of $\sigma$-finite measures on $\mathcal{T}$ and $\mathcal{S}$, respectively).  With $\frac{d P_\theta}{d \mu}(t,s) = f_\theta(t,s)$, let
  \[
    g_\theta(t) = \int_{\mathcal{S}}f_\theta(t,s) d\gamma(s)  \qquad \mbox{and}\qquad f_\theta(s|t) =\frac{f_\theta(t,s)}{g_\theta(t)}.
  \]
  Suppose that $\mathcal{P}$ is FI regular at $\theta_0 \in \mathbb{R}$ and $g^\prime_{\theta_0}(t)=\int_{\mathcal{S}} f^\prime_{\theta_0}(t,s) d \gamma(s)$ holds for any $t$, where the derivatives $g_\theta^\prime$ and $f_\theta^\prime$ are with respect to $\theta$.  Then show that
  \[
    I_X(\theta_0) \geq I_{T}(\theta_0).
  \]
\end{tcolorbox}
\textbf{Solution:}

\begin{Proof}
  Let $Y:= \frac{ d \log f_\theta(T,s) } { d \theta} \big|_{\theta=\theta_0}$,
  First note that 
  \begin{align}
    \E_{\theta_0}[Y|T](t) = \frac{1}{g_{\theta_0}(t)}\int_{\mathcal{S}}\frac{d\log f_{\theta}(t,s)}{d\theta}\bigg|_{\theta=\theta_0}
    f_{\theta_0}(t,s)d\gamma(s)
    & = \frac{1}{g_{\theta_0}(t)}\int_{\mathcal{S}}\frac{df_{\theta}(t,s)}{d\theta}\bigg|_{\theta=\theta_0}d\gamma(s) \nonumber \\
    & = \frac{g_{\theta_0}'(t)}{g_{\theta_0}(t)} \nonumber \\
    & = \frac{d\log g_{\theta}(t)}{d\theta}\bigg|_{\theta = \theta_0} \ \ \text{a.e.}\ \  \mu.
    \label{1.1}
  \end{align}
  Now, without loss of generality assume $\E_{\theta_0}Y^{2} < \infty$. Then by \eqref{1.1} and conditional Jensen's inequality,
  \begin{align*}
    I_{X}(\theta_0) = \E_{\theta_0}[Y^{2}] = \E_{\theta_0}\left( \E_{\theta_0}\left[ Y^2|T \right] \right)
    \geq \E_{\theta_0}\left( E_{\theta_0}[Y|T] \right)^2 & = E_{\theta_0}\left( \frac{d\log g_{\theta}(T)}{d\theta}\bigg|_{\theta=\theta_0} \right)^2  \\
    & = I_{T}(\theta_0),
  \end{align*}
  since $\frac{dP_{\theta}^{T}}{d\nu}(t) = g_{\theta}(t)$ a.e. $\nu$.
\end{Proof}


\newpage
\subsection*{2}
\begin{tcolorbox}
  In the context of the above problem, now prove that
  \[
    I_X(P,Q) \geq I_{T}(P^T,Q^T)
  \]
  assuming that $P$ and $Q$ are elements of $\mathcal{P}$.\\
\end{tcolorbox}
\textbf{Solution:}

\begin{Proof}
  Let $p(t,s):=\frac{d P}{d \mu}(t,s)$ and $q(t,s) := \frac{d Q}{d \mu}(t,s)$. Then
  $p_{T}(t) := \int_{\mathcal{S}}p(t,s)d\gamma(s)$ and $q_{T}(t) := \int_{\mathcal{S}}q(t,s)d\gamma(s)$ are the densities for $T$ with respect to
  $\nu$. Now, define $Z := q(X) / p(X)$ and $C(z) := -\log z$, $z > 0$. Note that 
  \begin{equation}
    E_P[Z|T=t] = \frac{1}{p_{T}(t)}\int_{\mathcal{S}}\frac{q(t,s)}{p(t,s)}p(t,s)d\gamma(s) = \frac{q_T(t)}{p_T(t)}.
      \label{2.1}
  \end{equation}
  Now, without loss of generality assume $\E_{P}|C(Z)| < \infty$.
  Then by conditional Jensen's inequality and \eqref{2.1},
  \begin{align*}
    I_X(P,Q) = E_PC(Z) = E_P\left( E_P[C(Z)|T] \right) & \geq E_P\left\{ C\left( E_P[Z|T] \right) \right\} \\
    & = E_P\left\{ -\log\left[ \frac{q_T(T)}{p_T(T)} \right] \right\} \\
    & = E_P\left\{ \log\left[\frac{p_T(T)}{q_T(T)}\right] \right\} = I_T(P^{T},Q^{T}).
  \end{align*}
\end{Proof}

 
 
\newpage
\subsection*{3}
\begin{tcolorbox}
  Let $X$ be a discrete measurable random observable and let $T(X)$ be a statistic.  (Here, ``discrete" means that probabilities for $X$ can be found by summation:  e.g., $P(X \in B) = \sum_{x \in B} p(x)  $ or $Q(T(X)=b) = \sum_{x: T(x)=b} q(x)$ under probability distributions $P$ or $Q$ for $X$ with densities $p(x)=P(X=x)$ and $q(x)=Q(X=x)$.)
  \begin{enumerate}
    \item Show that \[
        I_X(P,Q) \geq I_{T(X)}(P^T,Q^T)
      \]

    \item Supposing that $P$ and $Q$ have common support for discrete $X$ and that $I_X(P,Q)<\infty$,  show that  $I_X(P,Q) = I_{T(X)}(P^T,Q^T)$ holds if and only if $T(X)$ is sufficient for $\{P,Q\}$.
  \end{enumerate}
\end{tcolorbox}

\textbf{Solution:} Let $Z := q(X) / p(X)$ and $C(z) := -\log z$ for $z > 0$. Further, note that $p_T(t) := P(T(X)=t) = \sum_{x : T(x)=t} p(x)$ and 
$q_T(t) := Q(T(X)=t) = \sum_{x : T(x)=t} q(x)$.
Without loss of generality assume $E_{P}|C(Z)| < \infty$. Then, by
conditional Jensen's inequality,
\begin{equation}
  \E_P[C(Z)|T] \geq C\left( E_P[Z|T] \right).
  \label{3.1}
\end{equation}
We also have that for each $t$ such that $P(T(X) = t) > 0$,
\begin{equation}
  \E_P[Z|T=t] = \frac{\sum_{x:T(x) = t}\frac{q(x)}{p(x)}p(x) }{\sum_{x:T(x) = t}p(x)} = \frac{q_{T}(x)}{p_T(x)}.
  \label{3.2}
\end{equation}
\begin{enumerate}
  \item By \eqref{3.1} and \eqref{3.2},
    \begin{align}
      I_{X}(P,Q) = \E_P[C(Z)] = \E_{P}\left( E[C(Z)|T] \right) \geq \E_P\left\{ C\left( E[Z|T] \right) \right\} & = \E_P\left[
      \log\left(\frac{p_T(T)}{q_T(T)}\right) \right] \nonumber \\
      & = I_{T(X)}(P^{T},Q^{T}).
      \label{3.3}
    \end{align}

  \item Without loss of generality assume $p(x), q(x) > 0$ for all $x \in \mathcal{X}$.
    By \eqref{3.3}, 
    \[
      I_X(P,Q) = I_{T(X)}(P^T, Q^T)
    \]
    if and only if 
    \begin{equation}
      E[C(Z)|T=t] = C\left( E[Z|T=t] \right)\  \text{ for all }\ t \in \mathrm{Ran} T.
      \label{3.4}
    \end{equation}
    But since $C$ is strictly convex, \eqref{3.4} holds if and only if for each $t \in \mathrm{Ran} T$, 
    \[
      Z = \frac{q(X)}{p(X)} \equiv \text{ some constant} 
    \]
    whenever $T(X) = t$, i.e. the conditional distribution of $Z | T = t$ is
    degenerate. But this equivalent to the condition that we can factor $p$ and $q$ as $p(x) = g_P(T(x))h(x)$ and $q(x) = g_Q(T(x))h(x)$, respectively.
    Hence by the Factorization Theorem we see that $I_X(P,Q) = I_{T(X)}(P^{T}, Q^{T})$ if and only if $T$ is sufficient.

\end{enumerate}



\newpage
\subsection*{4}
\begin{tcolorbox}
  If $P$ is an exponential distribution with mean $\beta>0$ and $Q$ is an exponential distribution with mean $\theta>0$, show that
  \[
    I(P,Q)+I(Q,P) = \frac{1}{\beta \theta} (\beta-\theta)^2.
  \]
\end{tcolorbox}

\textbf{Solution:}

\begin{Proof}
  Let $p(x) := \beta^{-1}\exp\left[ -x\beta^{-1} \right]$ and $q(x) := \theta^{-1}\exp\left[ -x\theta^{-1} \right]$ be the R-N derivatives of $P$ and
  $Q$ with respect to the Lebesgue measure $\mu$ on $(0,\infty)$, respectively. By definition,
  \begin{align}
    I(P,Q) = \int_{0}^{\infty}\log\left[ \frac{p(x)}{q(x)} \right]p(x)d\mu(x) & = \int_{0}^{\infty}\left[ \log\left( \frac{\theta}{\beta}\right) + 
    x(\theta^{-1} - \beta^{-1}) \right]\beta^{-1}\exp^{-x\beta^{-1}}d\mu(x) \nonumber \\
    & = \log\left( \frac{\theta}{\beta} \right) + (\theta^{-1} - \beta^{-1})\E_{P}X \nonumber \\
    & = \log\left( \frac{\theta}{\beta} \right) + (\theta^{-1} - \beta^{-1})\beta. \label{4.1}
  \end{align}
  By symmetry,
  \begin{equation}
    I(Q,P) = \log\left( \frac{\beta}{\theta} \right) + (\beta^{-1} - \theta^{-1})\theta.
    \label{4.2}
  \end{equation}
  Thus, combining \eqref{4.1} and \eqref{4.2}, we have 
  \[ 
    I(P,Q) + I(Q,P) = (\theta^{-1} - \beta^{-1})\beta + (\beta^{-1} - \theta^{-1})\theta = \frac{1}{\theta\beta}(\beta^{2} - 2\theta\beta +
    \theta^{2}) = \frac{1}{\beta\theta}(\beta - \theta)^{2}.
  \]
\end{Proof}


\newpage
\subsection*{5}
\begin{tcolorbox}
  Suppose that $\Theta = \Theta_1 \times \Theta_2$ and that a decision rule $\phi \in D^*$ is such that for each $\theta_2\in\Theta_2$, $\phi$
  is admissible in $D^*$ when the parameter space is $\Theta_1\times \{\theta_2\}$.  Show that $\phi$ is then admissible in $D^*$ when the parameter space is $\Theta$.
\end{tcolorbox}
\textbf{Solution:} 

\begin{Proof}
  By way of contradiction suppose $\phi$ is inadmissable in $\mathcal{D}^{*}$ under the parameter space $\Theta$. Then there exists some $\phi' \in
  \mathcal{D}^{*}$ and $\theta' = (\theta_1', \theta_2') \in \Theta$ such that 
  \begin{equation}
    \mathrm{R}(\theta, \phi') \leq \mathrm{R}(\theta, \phi) \ \ \forall \ \theta \in \Theta
    \label{5.1}
  \end{equation}
  and 
  \begin{equation}
    \mathrm{R}(\theta', \phi') < \mathrm{R}(\theta', \phi).
    \label{5.2}
  \end{equation}
  But since $\Theta_1 \times \{\theta_2'\} \subseteq \Theta$, \eqref{5.1} implies that $\mathrm{R}(\theta, \phi') \leq \mathrm{R}(\theta, \phi)$ for
  all $\theta \in \Theta_1 \times \{\theta_2'\}$, and \eqref{5.2} implies that at $\theta' \in \Theta_1 \times \{\theta_2'\}$, $\mathrm{R}(\theta',
  \phi') < \mathrm{R}(\theta', \phi)$. So $\phi$ is inadmissable in $\mathcal{D}^{*}$ under the parameter space $\Theta_1 \times \{\theta_2\}$.
  This is a condradiction.
\end{Proof}

\newpage
\subsection*{6}
\begin{tcolorbox}
  Suppose that $w(\theta)>0$ for any $\theta \in \Theta$ and let $\phi \in D^*$.  Show $\phi$ is admissible in $D^*$ with the loss function $L(\theta,a)$
  if and only if $\phi$ is admissible in $D^*$ with the loss function $w(\theta)L(\theta,a)$.
\end{tcolorbox}

\textbf{Solution:}

\begin{Proof}
  Let $R_1(\theta, \phi)$ be the risk function for $\phi$ under the loss function $L(\theta, a)$, and $R_2(\theta, \phi)$ be the risk function for
  $\phi$ under the loss function $w(\theta)L(\theta, a)$. Then note that 
  \begin{equation}
    R_2(\theta, \phi) = \int_{\mathcal{X}}\int_{\mathcal{A}}w(\theta)L(\theta, a)d\phi_x(a)dP_{\theta}(x) = w(\theta)R_1(\theta, \phi).
    \label{6.1}
  \end{equation}

  $(\Rightarrow)$ So suppose $\phi$ is admissible in $\mathcal{D}^{*}$ with respect to $R_1$. By way of contradiction suppose $\phi$ is inadmissible 
  with respect to $R_2$. Then there exists $\phi' \in \mathcal{D}^{*}$ and $\theta' \in \Theta$ such that 
  \begin{equation}
    R_2(\theta, \phi') \leq R_2(\theta, \phi) \ \ \forall \ \theta \in \Theta
    \label{6.2}
  \end{equation}
  and 
  \begin{equation}
    R_2(\theta', \phi') < R_2(\theta', \phi).
    \label{6.3}
  \end{equation}
  But by \eqref{6.1} and since $w(\theta) > 0$, \eqref{6.2} implies $R_1(\theta, \phi') \leq R_1(\theta, \phi)$ for all $\theta \in \Theta$ and \eqref{6.3} implies $R_1(\theta',
  \phi') < R_1(\theta', \phi)$. This is a contradiction.

  $(\Leftarrow)$ Now suppose $\phi$ is admissible in $\mathcal{D}^{*}$ with respect to $R_2$. Then by \eqref{6.1} we can apply the same argument above with 
  $w'(\theta) := [w(\theta)]^{-1}$.
\end{Proof}


\newpage
\subsection*{7}
\begin{tcolorbox}
  Consider estimation of $p \in [0,1]$ with the squared error loss, based on $X\sim$Binomial$(n,p)$, and the two non-randomized decision rules
  $\delta_1(x)= x/n$ and $\delta_2(x)= 2^{-1}(  2^{-1} + n^{-1}x)$. Let $\psi$ be a randomzied decision rule that chooses $\delta_1$ with probability $1/2$ and $\delta_2$ with probability $1/2$.

  \begin{enumerate}
    \item Write out expressions for the risk functions of $\delta_1,\delta_2$, and $\psi$.
    \item Find a behavioral rule $\phi$ that is risk equivalent to $\phi$ (that is, $\psi$ and $\phi$ have the same risk functions).
    \item Identify a non-randomized estimator (decision rule) that is strictly better than $\psi$ (or $\phi$).
  \end{enumerate}
\end{tcolorbox}

\textbf{Solution:}


\end{document}
