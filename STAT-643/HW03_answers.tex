\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 643: Assignment 3}
\author{Evan P. Walsh}
\makeatletter
\makeatother
\lhead{Evan P. Walsh}
\chead{STAT 643: Assignment 3}
\rhead{\thepage}
\cfoot{}

\begin{document}
% \maketitle

\newcommand{\E}{\mathrm{E}}
\renewcommand{\baselinestretch}{1}


\subsection*{1}
\begin{tcolorbox}
  Suppose that $X=(T,S)$ and the family $\mathcal{P}$ of distributions of $X$ on $\mathcal{X}=\mathcal{T}\times \mathcal{S}$ is dominated by the measure $\mu=\nu\times \gamma$ (a product of $\sigma$-finite measures on $\mathcal{T}$ and $\mathcal{S}$, respectively).  With $\frac{d P_\theta}{d \mu}(t,s) = f_\theta(t,s)$, let
  \[
    g_\theta(t) = \int_{\mathcal{S}}f_\theta(s,t) d\gamma(s)  \qquad \mbox{and}\qquad f_\theta(s|t) =\frac{f_\theta(t,s)}{g_\theta(t)}.
  \]
  Suppose that $\mathcal{P}$ is FI regular at $\theta_0 \in \mathbb{R}$ and $g^\prime_{\theta_0}(t)=\int_{\mathcal{S}} f^\prime_{\theta_0}(t,s) d \gamma(s)$ holds for any $t$, where the derivatives $g_\theta^\prime$ and $f_\theta^\prime$ are with respect to $\theta$.  Then show that
  \[
    I_X(\theta_0) \geq I_{T}(\theta_0).
  \]

  \noindent Note:  Don't use Proposition 43 here; you should prove this result directly.    For this, it is helpful to use that, for $Y\equiv \frac{ d \log f_\theta(t,s) } { d \theta} \big|_{\theta=\theta_0}$,  $\E_{\theta_0}[ Y^2 ] = \E_{\theta_0}( \E_{\theta_0}[ Y^2 |T])$ holds and
  $\E_{\theta_0}[ Y^2 |T] \geq (\E_{\theta_0}[ Y |T])^2$ by conditional Jensen's inequality.
\end{tcolorbox}


\newpage
\subsection*{2}
\begin{tcolorbox}
  In the context of the above problem, now prove that
  \[
    I_X(P,Q) \geq I_{T}(P^T,Q^T)
  \]
  assuming that $P$ and $Q$ are elements of $\mathcal{P}$.\\

  \noindent Hint:  For $Z\equiv  q(X)/p(X)$, use that $\E_{\theta_0}[ C(Z) |T] \geq C(\E_{\theta_0}[ Z |T])$ by conditional Jensen's inequality for $C(z)= - \log z $.  One also needs  densities for $T$ from $p_T(t) = \int_{\mathcal{S}}p(s,t) d\gamma(s)$ and
  $q_T(t) = \int_{\mathcal{S}}q(s,t) d\gamma(s)$ where $\frac{d P}{d \mu}(t,s) = p(t,s)$ and $\frac{d Q}{d \mu}(t,s) = q(t,s)$.
\end{tcolorbox}

 
 
\newpage
\subsection*{3}
\begin{tcolorbox}
  Let $X$ be a discrete measurable random observable and let $T(X)$ be a statistic.  (Here, ``discrete" means that probabilities for $X$ can be found by summation:  e.g., $P(X \in B) = \sum_{x \in B} p(x)  $ or $Q(T(X)=b) = \sum_{x: T(x)=b} q(x)$ under probability distributions $P$ or $Q$ for $X$ with densities $p(x)=P(X=x)$ and $q(x)=Q(X=x)$.)
  \begin{enumerate}
    \item Show that \[
        I_X(P,Q) \geq I_{T(X)}(P^T,Q^T)
      \]

    \item Supposing that $P$ and $Q$ have common support for discrete $X$ and that $I_X(P,Q)<\infty$,  show that  $I_X(P,Q) = I_{T(X)}(P^T,Q^T)$ holds if and only if $T(X)$ is sufficient for $\{P,Q\}$.
  \end{enumerate}

  Hint:  For $Z\equiv  q(X)/p(X)$, again use that $\E_{P}[ C(Z) |T(X)=t] \geq C(\E_{P}[ Z |T(X)=t])$ by conditional Jensen's inequality for $C(z)= - \log z $ when $P(T(X)=t)>0$; equality can only hold if and only if the conditional distribution $Z|T(X)=t$ is degenerately equal to $\E_{P}[ Z |T(X)=t]$.     Note $p_T(t) = P(T(X)=t) = \sum_{x : T(x)=t} p(x)$ and $q_T(t) = Q(T(X)=t) = \sum_{x : T(x)=t} q(x)$.
\end{tcolorbox}


\newpage
\subsection*{4}
\begin{tcolorbox}
  If $P$ is an exponential distribution with mean $\beta>0$ and $Q$ is an exponential distribution with mean $\theta>0$, show that
  \[
    I(P,Q)+I(Q,P) = \frac{1}{\beta \theta} (\beta-\theta)^2.
  \]
\end{tcolorbox}

\textbf{Solution:}

\begin{Proof}
  Let $p(x) := \beta^{-1}\exp\left[ -x\beta^{-1} \right]$ and $q(x) := \theta^{-1}\exp\left[ -x\theta^{-1} \right]$ be the R-N derivatives of $P$ and
  $Q$ with respect to the Lebesgue measure $\mu$ on $(0,\infty)$, respectively. By definition,
  \begin{align}
    I(P,Q) = \int_{0}^{\infty}\log\left[ \frac{p(x)}{q(x)} \right]p(x)d\mu(x) & = \int_{0}^{\infty}\left[ \log\left( \frac{\theta}{\beta}\right) + 
    x(\theta^{-1} - \beta^{-1}) \right]\beta^{-1}\exp^{-x\beta^{-1}}d\mu(x) \nonumber \\
    & = \log\left( \frac{\theta}{\beta} \right) + (\theta^{-1} - \beta^{-1})\E_{P}X \nonumber \\
    & = \log\left( \frac{\theta}{\beta} \right) + (\theta^{-1} - \beta^{-1})\beta. \label{4.1}
  \end{align}
  By symmetry,
  \begin{equation}
    I(Q,P) = \log\left( \frac{\beta}{\theta} \right) + (\beta^{-1} - \theta^{-1})\theta.
    \label{4.2}
  \end{equation}
  Thus, combining \eqref{4.1} and \eqref{4.2}, we have 
  \[ 
    I(P,Q) + I(Q,P) = (\theta^{-1} - \beta^{-1})\beta + (\beta^{-1} - \theta^{-1})\theta = \frac{1}{\theta\beta}(\beta^{2} - 2\theta\beta +
    \theta^{2}) = \frac{1}{\beta\theta}(\beta - \theta)^{2}.
  \]
\end{Proof}



\subsection*{5}
\begin{tcolorbox}
  Suppose that $\Theta = \Theta_1 \times \Theta_2$ and that a decision rule $\phi \in D^*$ is such that for each $\theta_2\in\Theta_2$, $\phi$
  is admissible in $D^*$ when the parameter space is $\Theta_1\times \{\theta_2\}$.  Show that $\phi$ is then admissible in $D^*$ when the parameter space is $\Theta$.
\end{tcolorbox}



\subsection*{6}
\begin{tcolorbox}
  Suppose that $w(\theta)>0$ for any $\theta \in \Theta$ and let $\phi \in D^*$.  Show $\phi$ is admissible in $D^*$ with the loss function $L(\theta,a)$
  if and only if $\phi$ is admissible in $D^*$ with the loss function $w(\theta)L(\theta,a)$.
\end{tcolorbox}



\subsection*{7}
\begin{tcolorbox}
  Consider estimation of $p \in [0,1]$ with the squared error loss, based on $X\sim$Binomial$(n,p)$, and the two non-randomized decision rules
  $\delta_1(x)= x/n$ and $\delta_2(x)= 2^{-1}(  2^{-1} + n^{-1}x)$. Let $\psi$ be a randomzied decision rule that chooses $\delta_1$ with probability $1/2$ and $\delta_2$ with probability $1/2$.

  \begin{enumerate}
    \item Write out expressions for the risk functions of $\delta_1,\delta_2$, and $\psi$.
    \item Find a behavioral rule $\phi$ that is risk equivalent to $\phi$ (that is, $\psi$ and $\phi$ have the same risk functions).
    \item Identify a non-randomized estimator (decision rule) that is strictly better than $\psi$ (or $\phi$).
  \end{enumerate}
\end{tcolorbox}


\end{document}
