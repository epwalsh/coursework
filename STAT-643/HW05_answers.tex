\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{Solution}{\stepcounter{ProofCounter}\textbf{Solution:}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\newcommand{\E}{\mathrm{E}}
\renewcommand{\baselinestretch}{1}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 643: HW 5}
\author{Evan P. Walsh}
\makeatletter
\makeatother
\lhead{Evan P. Walsh}
\chead{STAT 643: HW 5}
\rhead{\thepage}
\cfoot{}

\begin{document}
\maketitle


\subsection*{1}
\begin{tcolorbox}
  Suppose that $X \sim$Binomial$(n,p)$ and that one wishes to estimate $p\in(0,1)=\Theta$. Suppose first that $L(p,a)=p^{-1}(1-p)^{-1}(p-a)^2$ where $a\in[0,1]=\mathcal{A}$

  \begin{enumerate}[label=(\alph*)]
    \item Show that $X/n$ is Bayes with respect to the uniform prior on $(0,1)$.\\
      Note: Technically, be careful in considering posterior risk (expected loss) when $x=0$ or $n$ because the posterior risk diverges to $\infty$ unless the action $a$ is chosen appropriately.
    \item Show that $X/n$ is admissible.
    \item Show that $X/n$ is minimax.
    \item Now consider the ordinary squared error loss, $L^*(\theta,a)=(p-a)^2$.  By applying Problem~6 of Homework~3,
      prove that
      $X/n$ is admissible under this loss function as well.
  \end{enumerate}
\end{tcolorbox}

\begin{Solution}
  Let $\mu$ denote the Lebesgue measure on $(0,1)$.
  \begin{enumerate}[label=(\alph*), leftmargin=6mm]
    \item To show that $X / n$ is Bayes with respect to the uniform prior $G = \mu$ on $(0,1)$, we will show that $x / n$ minimizes
      \begin{equation}
        \int_{\Theta}L(p,a) \pi^{p|X=x}(p|X=x) = \int_{(0,1)} \frac{(p-a)^2}{p(1-p)}\pi^{p|X=x}(p|X=x)
        \label{1.1}
      \end{equation}
      with respect to $a \in \mathcal{A} = [0,1]$ for each $x \in \mathcal{X} = \{0, \dots, n\}$, where $\pi^{p|X=x}$ is the posterior distribution of
      $p$ given the data $X = x$. Note that $\frac{d\pi^{p|X=x}}{d\mu} \propto p^{x}(1-p)^{n-x}$. Thus
      \begin{equation}
        \int_{(0,1)} \frac{(p-a)^2}{p(1-p)}\pi^{p|X=x}(p|X=x) \propto \int_{(0,1)}(p-a)^2 p^{x-1}(1-p)^{n-x-1}d\mu(p).
        \label{1.2}
      \end{equation}
      So minimizing \eqref{1.1} with respect to $a$ is equivalent to minimizing the right-hand side of \eqref{1.2}.
      First consider the case when $x \neq 0$ and $x\neq n$, i.e. $x \in \{1, \dots, n-1\}$. Since $p^{x-1}(1-p)^{n-x-1}$ is the kernel of a beta
      distribution in $p$ with parameters $\alpha = x$ and $\beta = n-x$, the integrand on the right side of \eqref{1.2} can be expressed as 
      $E_{\alpha,\beta}(Y-a)^2$
      where $Y \equiv p \sim$ Beta($\alpha, \beta$). This expectation is clearly minimized by taking 
      \[
        a := E_{\alpha,\beta}Y = \frac{\alpha}{\alpha + \beta} = \frac{x}{n}.
      \]
      On the other hand, if $x = 0$ or $x = n$, the integral on the right side of \eqref{1.2} is finite if and only if $a = 0$ or $a = 1$,
      respectively. So in either case, we must have $a := x / n$. Hence the non-randomized decision rule $X / n$ is Bayes with respect to the uniform
      prior on $(0,1)$.

    \item We will show that $X / n$ is admissible by way of Theorem 73. Note that $\Theta = (0,1)$ is open, $R(p, \phi)$ continuous in $p$ for all $\phi
      \in \mathcal{D}^*$, and the support of the prior $G = \mu$ is $\Theta$. Thus, since by part (a) $X / n$ is Bayes with respect to 
      $G$, it only remains to verify that $BR(G) < \infty$. But
      \begin{align*} 
        BR(G) = BR(G, X/n) & = \int_{\mathcal{X}} \int_{\Theta}L(p,x/n)\pi^{p|X=x}(p|X=x)d\pi^X(x) \\
        & = \sum_{x=0}^n\int_{\Theta}L(p,x/n)\pi^{p|X=x}(p|X=x)d\pi^X(x),
      \end{align*}
      which is the finite sum of finite terms, and hence is finite. Therefore the conditions for Theorem 73 are satisfied, so $X / n$ is admissible.

    \item We will show that $X / n$ is minimax by way of Theorem 84. Well, let $p \in (0,1)$. Then 
      \[ 
        R(p, X/n) = E_{p}\left\{ L(p,X/n) \right\} = E_{p}\left\{ \frac{(p-X/n)^2}{p(1-p)} \right\} = \frac{p(1-p)/n}{p(1-p)} = \frac{1}{n}.
      \]
      Hence $X / n$ is an equalizer rule since $R(p, X/n) \equiv n^{-1}$ for all $p \in (0,1)$. So by Theorem 84, $X / n$ is minimax.

    \item Let $w(p) := p^{-1}(1-p)^{-1} > 0$ for all $p \in (0,1)$. So, since $L(p,a) \equiv w(p)L^*(p,a)$ and $X / n$ is admissible under $L(p,a)$ by part (b),
      $X / n$ is admissible under $L^*(p,a)$ by the result on Problem 6 of Homework 3.

  \end{enumerate}
\end{Solution}



\newpage
\subsection*{2}
\begin{tcolorbox}
  Suppose that $X\sim$Bernoulli$(p)$ and that one wishes to estimate $p\in[0,1]=\Theta=\mathcal{A}$ with the loss function $L(p,a)=|p-a|$.  
  Consider the estimator $\delta$ with $\delta(0)=\frac{1}{4}$ and $\delta(1)=\frac{3}{4}$.
  \begin{enumerate}[label=(\alph*)]
    \item Write out the risk function of $\delta$ and show that $R(p,\delta)\leq \frac{1}{4}$ for all $p\in[0,1]$.
    \item Show that there exists a prior distribution placing all its mass on $\{0,\frac{1}{2},1\}$ with respect to which $\delta$ is Bayes.\\
      Hint: Consider finding   prior probabilities on $\{0,\frac{1}{2},1\}$ so that $\delta$ is a median (in the STAT 642 sense) of the posterior distribution on $\{0,\frac{1}{2},1\}$.
    \item Prove that $\delta$ is minimax in this problem and identify a least favorable prior.
  \end{enumerate}
\end{tcolorbox}
\begin{Solution}
  \begin{enumerate}[label=(\alph*), leftmargin=6mm]
    \item For $p \in [0,1]$, the risk function of $\delta$ is given by
      \begin{equation}
        R(p, \delta) = E_p|p - \delta(X)| = p\left|p - \frac{3}{4}\right| + (1-p)\left|p - \frac{1}{4}\right|.
        \label{2.1}
      \end{equation}
      Now, for $0 \leq p \leq 1/4$, $R(p, \delta) = p(3/4 - p) + (1-p)(1/4 - p)$ and $\frac{dR(p,\delta)}{dp} \equiv -1/2$. Thus, since $R(0,\delta) =
      1/4$, it must be that $R(p, \delta) \leq 1/4$ for all $p \in [0,1/4]$. Similarly $R(p, \delta) \leq 1/4$ for all $p \in [3/4, 1]$. Now, for
      $1/4 \leq p \leq 3/4$, $R(p, \delta) = p(3/4 - p) + (1-p)(p - 1/4)$, $\frac{dR(p,\delta)}{dp} = 2 - 4p$, and $\frac{d^2R(p,\delta)}{dp^2} \equiv
      -4$. Thus $R$ is maximized at $p = 1/2$ on this subinterval. But $R(1/2, \delta) = 1/4$. Hence $R(p, \delta) \leq 1/4$ for all $p \in [0,1]$.

    \item Suppose $G$ is a prior that places all of its mass on $\{0, \frac{1}{2}, 1\}$. Then the prior distribution is given by
      \[
        \pi^{p|X=x}(C) = \frac{ G(\{0\})I(x = 0)I(0 \in C) + G(\{1/2\})(1/2)I(1/2 \in C) + G(\{1\})I(x=1)I(1\in C) }{  G(\{0\})I(x = 0) +
        G(\{1/2\})(1/2) + G(\{1\})I(x=1) }.
      \]
      Now take $G(\{0\}) = G(\{1\}) := 1/4$ and $G(\{1/2\}) = 1/2$.

      \begin{claim}
        $\delta$ is a median with respect to $\pi^{p|X=x}$ for all $x \in \{0,1\}$.
      \end{claim}
      \begin{claimproof}
        In order to show that $\delta$ is a median with respect to $\pi^{p|X}$ we need to show that 
        \begin{equation}
          \pi^{p|X=x}\left( [0,\delta(x)) \right) \leq 1/2 \leq \pi^{p|X=x}\left( [0, \delta(x)] \right) \ \ \forall \ x \in \{0,1\}.
          \label{2.2}
        \end{equation}
        Now, 
        \[
          \pi^{p|X=0}\left( [0,\delta(x)) \right) = \pi^{p|X=0}\left( [0,1/4) \right) = \pi^{p|X=0}\left( [0,1/4] \right) = \frac{1/4}{(1/4) +
          (1/2)(1/2)} = \frac{1}{2},
        \]
        and
        \[
          \pi^{p|X=1}\left( [0,\delta(x)) \right) = \pi^{p|X=1}\left( [0,3/4) \right) = \pi^{p|X=1}\left( [0,3/4] \right) = \frac{(1/2)(1/2)}{(1/4) +
          (1/2)(1/2)} = \frac{1}{2}.
        \]
        Thus \eqref{2.2} is verified.
      \end{claimproof}

      Since $\delta$ is a median with respect to the posterior distribution, $\delta$ is Bayes with respect to $G$ by the remarks on 33 of section
      4.5.


    \item Note that by part (b)
      \begin{align*}
        BR(G) = BR(G, \delta)& = \int_{\Omega}R(p, \delta)dG(p) \\
        & = G(\left\{ 0 \right\}) R(0,\delta) + G(\left\{ 1/2 \right\})R(1/2, \delta) + G(\left\{ 1 \right\})R(1,\delta) \\
        & = (1/4)(1/4) + (1/2)(1/4) + (1/4)(1/4) = 1/4.
      \end{align*}
      Further, by part (a) $R(p, \delta) \leq 1/4 = BR(G)$ for all $p \in [0,1]$. Thus $\delta$ is minimax by Corollary 87. It also follows by Theorem
      89 that $G$ is least favorable.
  \end{enumerate}
\end{Solution}

\newpage
\subsection*{3}
\begin{tcolorbox}
  Consider a decision problem where $P_\theta$ is the Normal$(\theta,1)$ distribution on $\mathcal{X}=\mathbb{R}$, $\mathcal{A}=\{0,1\}$, and $L(\theta,a)=I[a=0]I[\theta>5] + I[a=1]I[\theta \leq 5]$.  If $\Theta= (-\infty,5] \cup [6,\infty)$, guess what prior is least favorable, find the corresponding Bayes decision rule, and prove that it is minimax.
\end{tcolorbox}

\begin{Solution}
  Let $f_{\theta}(x)$ be the density of a $N(\theta, 1)$ distribution with respect the Lebesgue measure $\mu$ on $\mathbb{R}$.
  Following the example on page 38 of section 4.6 in our notes, it makes sense that the prior $G$ defined by $G(\left\{ 5 \right\}) = G(\left\{ 6
  \right\}) = 1/2$ is least favorable. Under $G$, the posterior distribution is given by 
  \[
    \pi^{\theta | X}(C) := \frac{f_{5}(x)I(5\in C) + f_{6}(x)I(6\in C)}{f_{5}(x) + f_{6}(x)}, \ C \subseteq \Theta.
  \]
  Thus, by the result from Question 6 (b) on homework 5 or the remarks on page 33 of section 4.5, the Bayes rule with respect to $G$ is given by 
  \[
    \delta(x) := I\left\{ \pi^{\theta|X}(\{6\}) > \pi^{\theta |X}(\{5\}) \right\} = I\{f_{6}(x) > f_{5}(x)\} = I\{x > 5.5\}.
  \]
  \begin{claim}
    $\delta$ is minimax.
  \end{claim}
  \begin{claimproof}
    First note that for all $\theta \in \Theta$,
    \begin{equation}
      R(\theta, \delta) = E_{\theta}L(\theta, \delta) = I(\theta \leq 5)P_{\theta}(X > 5.5) + I(\theta > 5)P_{\theta}(X \leq 5.5).
      \label{3.0}
    \end{equation}
    Further, the Bayes risk of $G$ is given by 
    \begin{align}
      BR(G) = BR(G, \delta) = \int_{\Theta}R(\theta, \delta)dG(\theta) & = \frac{1}{2}\left[ R(5, \delta) + R(6,\delta)  \right] \nonumber \\
      & = \frac{1}{2}\left[ E_{\theta=5}L(5, \delta(X)) + E_{\theta=6}L(6,\delta(X)) \right] \nonumber\\
      & = \frac{1}{2}\left[ P_{5}(X > 5.5) + P_{6}(X \leq 5.5) \right]\nonumber \\
      & = P_{5}(X > 5.5) \label{3.1} \\
      & = P_{6}(X \leq 5.5) \label{3.2}
    \end{align}
    Thus, if $\theta \leq 5$, then by \eqref{3.0} and \eqref{3.1}, 
    \[
      R(\theta, \delta) = P_{\theta}(X > 5.5) \leq P_{5}(X > 5.5) = BR(G),
    \]
    and if $\theta \geq 6$, then by \eqref{3.0} and \eqref{3.2},
    \[
      R(\theta, \delta) = P_{\theta}(X \leq 5.5) \leq P_{6}(X \leq 5.5) = BR(G).
    \]
    Hence $R(\theta, \delta) \leq BR(G)$ for all $\theta \in \Theta$. Thus $\delta$ is minimax by Corollary 87.
  \end{claimproof}

\end{Solution}


\newpage
\subsection*{4}
\begin{tcolorbox}
  Consdier estimation of $\lambda$, the mean of a Poisson distribution, with the (inverse-mean) weighted squared error loss.
  That is, let $\Lambda=(0,\infty)$ be the parameter space, $\mathcal{A}=[0,\infty)$ be the action space, $P_\lambda$ be the Poisson distribution on $\mathcal{X}=\{0,1,2,3,\ldots\}$ and $L(\lambda,a)=
  \lambda^{-1}(\lambda-a)^2$.  Let $\delta(X)=X$.
  \begin{enumerate}[label=(\alph*)]
    \item Show that $\delta$ is an equalizer rule.
    \item Show that $\delta$ is generalized Bayes with respect to the Lebesgue measure on $\Lambda$.
    \item Find the Bayes estimator with respect to a gamma$(\alpha,\beta)$ prior on $\Lambda$ which has a density given by
      \[
        g(\lambda|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha-1} e^{-\beta \lambda},\quad \lambda>0,
      \]
      for $\alpha,\beta>0$ and $\Gamma(\alpha) = \int_0^\infty t^{\alpha-1} e^{-t}dt$.\\[.2cm]
      Note: Different parameterizations of the gamma distribution exist and the above one has mean $\alpha/\beta$.
    \item Prove that $\delta$ is minimax for this problem.
  \end{enumerate}
\end{tcolorbox}
\begin{Solution}
  \begin{enumerate}[label=(\alph*),leftmargin=6mm]
  \item Let $\lambda \in \Lambda$. Then 
    \begin{equation}
      R(\lambda, \delta) = \int_{\mathcal{X}}L(\lambda, \delta)dP_{\lambda} = \sum_{x=0}^{\infty}\frac{(\lambda - x)^2}{\lambda} \frac{\lambda^x
      e^{-\lambda}}{x!} = \lambda^{-1}\text{Var}_{\lambda}(X) = 1.
      \label{4.1}
    \end{equation}
    Hence $R(\lambda, \delta) \equiv 1$ for all $\lambda \in \Lambda$.

  \item Let $\mu$ denote Lebesgue measure on $\Lambda$. Then for $a \in \mathcal{A}$,
    \[
      \int_{\Lambda}L(\lambda, a)f_{\lambda}(x)d\mu(x) = \int_{\Lambda}\frac{(\lambda-a)^2}{\lambda}\frac{\lambda^x e^{-\lambda}}{x!}d\mu(\lambda) =
      \frac{1}{x!}\int_{0}^{\infty}(\lambda - a)^2 \lambda^{x-1}e^{-\lambda}d\mu(\lambda).
    \]
    Now, if $x \geq 1$, then $\lambda^{x-1}e^{-\lambda}$ is the kernel of a gamma distribution with expectation $x$. Hence choosing $a = x$ minimizes
    the integral above. If $x = 0$, then the integral above is finite if and only if $a = 0 = x$. So in either case, $a = x$ minimizes the integral.

  \item Let $G$ denote the gamma$(\alpha, \beta)$ prior on $\Lambda$. The posterior distribution can be found by noting that 
    \[ \pi^{\lambda|X=x} \propto \lambda^{\alpha + x - 1}e^{-(\beta + 1)\lambda}, \]
    which is the kernel of a gamma$(\alpha + x, \beta + 1)$ distribution. Now,
    by the remarks on page 33 in section 4.5 of our notes, the Bayes estimator with
    respect to $G$ is given by 
    \begin{align}
      \delta_{G}(x) := \frac{1}{E_{\lambda|X}[\lambda^{-1}|X=x]} & = \left\{ \int_{0}^{\infty}\lambda^{-1} \frac{(\beta + 1)^{\alpha +
        x}}{\Gamma(\alpha + x)} \lambda^{\alpha + x - 1}\exp\left[ -(\beta + 1)\lambda \right]d\mu(\lambda) \right\}^{-1} \nonumber \\
        & = \left\{ \frac{(\beta+1)^{\alpha + x}}{\Gamma(\alpha + x)} \int_{0}^{\infty} \lambda^{\alpha + x - 2}\exp\left[ -(\beta+1)\lambda
        \right]d\mu(\lambda) \right\}^{-1}. \label{4.2}
    \end{align}
    If $x \geq 1$ or $\alpha > 1$, then $\lambda^{\alpha + x -2}\exp\left[ -(\beta + 1)\lambda \right]$ is the kernel of a gamma$(\alpha + x - 1,
    \beta + 1)$ distribution, and therefore the integral in \eqref{4.2} is given by $\Gamma(\alpha + x - 1) * (\beta + 1)^{-\alpha - x + 1}$,
    in which case 
    \[
      \delta_{G}(x) = \left\{ \frac{(\beta+1)^{\alpha + 1}}{\Gamma(\alpha + x)} \frac{\Gamma(\alpha + x - 1)}{(\beta + 1)^{\alpha + x - 1}}
        \right\}^{-1} = \left\{ \frac{\beta + 1}{\alpha + x - 1} \right\}^{-1} = \frac{\alpha + x - 1}{\beta + 1}.
    \]
    On the
    other hand, if $x = 0$ or $\alpha \leq 1$, then the integral in \eqref{4.2} is equal to $+\infty$. Hence 
    \begin{equation}
      \delta_{G}(x) := \left\{ \begin{array}{cl}
          \frac{\alpha + x - 1}{\beta + 1} & \text{ if } x \geq 1 \text{ or } \alpha > 1 \\
          0 & \text{ if } x = 0 \text{ and } \alpha \leq 1. 
      \end{array}\right.
      \label{4.3}
    \end{equation}


  \item For $n \geq 1$, let $G_n$ be the gamma$(1, \beta_n)$ prior, where $\beta_n := n^{-1}$. By part (c), the Bayes rule with respect to $G_n$ is given by $\delta_{G_n}$ from
    \eqref{4.3}. Thus,
    \begin{align*}
      BR(G_n) = BR(G_n, \delta_{G_n}) & = \int_{\Lambda}R(\lambda, \delta_{G_n})dG_n(\lambda) \\
      & = \int_{0}^{\infty}\sum_{x=0}^{\infty} \frac{(\lambda - \delta_{G_n}(x))^2 \lambda^x e^{-\lambda}}{\lambda x!} \beta_n e^{-\beta_n \lambda}
      d\mu(\lambda) \\
      \text{(Tonelli's Thm)} \ & = \sum_{x=0}^{\infty} \frac{\beta_n}{x!} \int_{0}^{\infty}(\lambda - \delta_{G_n})^2 \lambda^{x - 1}e^{-(\beta_n +
      1)\lambda}d\mu(\lambda) \\
      & = \beta_n\int_{0}^{\infty}\lambda e^{-(\beta_n + 1)\lambda}d\mu(\lambda) +
      \sum_{x=1}^{\infty}\frac{\beta_n}{x!}\int_{0}^{\infty}\left( \lambda - \frac{x}{\beta_n + 1}
      \right)^2\lambda^{x-1}e^{-(\beta_n+1)\lambda}d\mu(\lambda) \\
      & = \frac{\beta_n}{(\beta_n + 1)^2} + \beta_n\sum_{x=1}^{\infty}\frac{\Gamma(x) x}{(\beta_n + 1)^{2 + x}x!} \\
      & = \frac{\beta_n}{(\beta_n + 1)^2} \sum_{x=0}^{\infty} \frac{1}{(\beta_n + 1)^2} \\
      & = \frac{1}{\beta_n + 1} \longrightarrow 1 \ \text{ as } \ n \rightarrow \infty.
    \end{align*}
    Therefore, by \eqref{4.1} and Theorem 85, $\delta$ is minimax.


\end{enumerate}
\end{Solution}






\subsection*{5}
\begin{tcolorbox}
  Let $X_1,\ldots,X_n$ be iid random variables, each having a distribution $P_\lambda$, $\lambda>0$, with a density \[
    f_\lambda(x) = \frac{d P_\lambda}{d \mu}(x) =  \left(\lambda\exp(-\lambda x)\right)^{I[x\neq0]} \left((1-e^{-\lambda})\right)^{I[x=0]}
  \]
  for $\mu=m+\nu$ with $m$ being the Lebesgue measure on $[1,\infty)$ and $\nu$ being the point mass measure at $0$. Let $\delta_i= I[X_i=0]$, $i=1,\ldots,n$ and $M_n = n- \sum_{i=1}^n \delta_i$.
  \begin{enumerate}[label=(\alph*)]
    \item Show that there is no maximum likelihood estimate (MLE) of $\lambda$ when $M_n=0$, but there is a MLE of $\lambda$ when $M_n>0$.
    \item  Show that, for any $\lambda>0$, with $\lambda$-probability tending to 1 as $n\rightarrow \infty$, the MLE of $\lambda$ exists.
    \item Give  as a simple estimator of $\lambda$ based on $M_n$ alone and prove that this estimator is consistent.
    \item Write down an explicit one-step Newton improvement of the estimator from (c) using the likelihood function from (a). (No need to simplify the expression).
  \end{enumerate}
\end{tcolorbox}
\begin{Solution}
  Let $\mathcal{X} := \{0\} \cup [1,\infty)$. Then for $\bm{x} \in \mathcal{X}^n$, the joint density function with respect to $\mu^n$ is 
  \begin{equation}
    f_{\lambda}^n(\bm{x}) = \lambda^{M_n}\{1 - \exp(-\lambda)\}^{n-M_n}\exp\{-\lambda n\bar{x}\} \ \ \text{a.e.} \ \mu^n.
    \label{5.1}
  \end{equation}
  \begin{enumerate}[label=(\alph*),leftmargin=6mm]
    \item Let $\bm{x} = (x_1, \dots, x_n) \in \mathcal{X}^n$. First suppose $M_n = 0$. Then the density in \eqref{5.1} becomes
      \[
        f_{\lambda}^n(\bm{x}) = \{1 - \exp(-\lambda)\}^n,
      \]
      which is strictly less than 1 for all $\lambda > 0$.
      But as $\lambda \rightarrow \infty$, $f_{\lambda}^n(\bm{x}) \rightarrow 1$. Hence there is no MLE.
      On the other hand, if $M_n > 0$, then from \eqref{5.1} we see that $f_{\lambda}^n(\bm{x}) \rightarrow 0$ and $\lambda \rightarrow 0$ or $\lambda
      \rightarrow \infty$. So, since $f_{\lambda}^n(\bm{x})$ is continuous as a function of $\lambda$, there must be a point $\lambda_0 > 0$ that
      $f_{\lambda_0}^n(\bm{x}) = \sup_{\lambda > 0}f_{\lambda}^n(\bm{x})$, i.e. an MLE exists.

    \item Let $\lambda_0 > 0$. By part (a) and iid-ness,
      \begin{align*}
        P_{\lambda_0}^n\left( \text{MLE exists} \right) = P_{\lambda_0}^n(M_n > 0) = 1 - P_{\lambda_0}^n(M_n = 0) & = 1 - P_{\lambda_0}^n(X_i = 0 \
        \forall \ i = 1,\dots, n) \\
        & = 1 - \left[ P_{\lambda_0}(X_1 = 0) \right]^{n} \\
        & = 1 - \left[ 1 - e^{-\lambda_0} \right]^{n} \longrightarrow 1
      \end{align*}
      as $n \rightarrow \infty$.

    \item Note that $\delta_1, \dots, \delta_n$ are iid Ber$(1 - e^{-\lambda})$. Thus, by (Kolmogorov's) SLLN,
      \[
        \frac{1}{n}\sum_{i=1}^{n}\delta_i = \frac{n - M_n}{n} \longrightarrow 1 - e^{-\lambda} \ \ \text{a.s.} \ P_{\lambda}^{\infty} \ \text{ as } n \rightarrow \infty.
      \]
      Now let $\epsilon > 0$. Then we also have that 
      \[
        \frac{n - M_n}{n + \epsilon} \longrightarrow 1 - e^{-\lambda} \ \ \text{a.s.} \ P_{\lambda}^{\infty} \ \text{ as } n \rightarrow \infty,
      \]
      so by the continuous mapping theorem,
      \[
        T_n := -\log\left( 1 - \frac{n - M_n}{n + \epsilon}\right) = \log\left( \frac{n + \epsilon}{M_n + \epsilon} \right) \longrightarrow \lambda  \
        \ \text{a.s.} \ P_{\lambda}^{\infty} \ \text{ as } n \rightarrow \infty.
      \]

    \item From \eqref{5.1}, the log-likelihood is given by 
      \[
        L_n(\lambda) = M_n \log(\lambda) + (n - M_n)\log\left\{ 1 - \exp(-\lambda) \right\} - \lambda n \bar{x}\ \ \text{ for } \ \lambda > 0.
      \]
      Thus,
      \begin{align*}
        L_n'(\lambda) & = \frac{M_n}{\lambda} + \frac{(n-M_n)\exp(-\lambda)}{1 - \exp(-\lambda)} - n\bar{x} \\
        L_n''(\lambda) & = -\frac{M_n}{\lambda^2} + (n - M_n)\left\{ \frac{[1 - \exp(-\lambda)][-\exp(-\lambda)] - \exp(-\lambda)\exp(-\lambda)}{[1 -
        \exp(-\lambda)]^2}\right\} \\
        & = -\frac{M_n}{\lambda^2} - \frac{(n - M_n)\exp(-\lambda)}{[1 - \exp(-\lambda)]^{2}}.
      \end{align*}
      Hence, a one-step Newton improvement to the estimator $T_n$ from part (c) is given by 
      \[
        \hat{\lambda}_n := T_n - \frac{L_n'(T_n)}{L_n''(T_n)} = T_n - \frac{\left( \frac{M_n}{T_n} + \frac{(n-M_n)\exp(-T_n)}{1 - \exp(-T_n)} -
        n\bar{x} \right)}{\left( -\frac{M_n}{T_n^2} - \frac{(n - M_n)\exp(-T_n)}{[1 - \exp(-T_n)]^2} \right)}.
      \]
  \end{enumerate}
\end{Solution}


\subsection*{6}
\begin{tcolorbox}
  Suppose that $X_1,\ldots,X_n$ are iid with the distribution $P_\theta$ for $\theta \in \mathbb{R}$, where $P_\theta$ has the RN derivative with respect to the counting measure $\nu$ on $\mathcal{X}=\{0,1,2\}$ given by
  \[
    f_\theta(x)= \frac{\exp(x\theta)}{1+\exp(\theta)+\exp(2\theta)}.
  \]
  Find an estimator $T_n$ of $\theta$ based on $n_0\equiv \sum_{i=1}^n I[X_i=0]$ such that $\sqrt{n}(T_n-\theta)$ has a limiting normal distribution (implying that $\sqrt{n}(T_n-\theta)$  is bounded in probability or tight and so that $T_n$ is $\sqrt{n}$-consistent).\\
\end{tcolorbox}
\begin{Solution}
  For $i = 1, \dots, n$, let $Y_i := I[X_i = 0]$. Then $Y_1, \dots, Y_n$ are iid Ber$(p_\theta)$, where $p_\theta \equiv p(\theta) := [1 + \exp(\theta) +
  \exp(2\theta)]^{-1}$. Then by the Central Limit Theorem,
  \[
    \sqrt{n}(\bar{Y} - p_{\theta}) = \sqrt{n}\left( \frac{n_0}{n} - p_{\theta} \right) \stackrel{d}{\longrightarrow} N\bigg(0, p_{\theta}(1 - p_{\theta})\bigg).
  \]
  Note that for $\theta > 0$, the inverse of $p(\theta)$ is $g(p) := \log\left\{ \sqrt{4p^{-1} - 3} - \frac{1}{2} \right\}$, $p \in (0,1)$, for which
  the derivative $g'$ exists and is non-zero on $(0,1)$. Then let $T_n := g(\bar{Y})$. So by the delta method,
  \[
    \sqrt{n}\left( T_n - g(p_\theta) \right) \stackrel{d}{\longrightarrow} N\bigg(0, [g'(p_{\theta})]^2p_{\theta}(1-p_{\theta})\bigg).
  \]
\end{Solution}

\end{document}



% \begin{enumerate} \itemsep .3cm
% \item Suppose that $X \sim$Binomial$(n,p)$ and that one wishes to estimate $p\in(0,1)=\Theta$. Suppose first that $L(p,a)=p^{-1}(1-p)^{-1}(p-a)^2$ where $a\in[0,1]=\mathcal{A}$

% \begin{enumerate}
% \item Show that $X/n$ is Bayes with respect to the uniform prior on $(0,1)$.\\
% Note: Technically, be careful in considering posterior risk (expected loss) when $x=0$ or $n$ because the posterior risk diverges to $\infty$ unless the action $a$ is chosen appropriately.
 % \item Show that $X/n$ is admissible.
 % \item Show that $X/n$ is minimax.
% \item Now consider the ordinary squared error loss, $L^*(\theta,a)=(p-a)^2$.  By applying Problem~6 of Homework~3,
% prove that
 % $X/n$ is admissible under this loss function as well.
         % \end{enumerate}
         % \item Suppose that $X\sim$Bernoulli$(p)$ and that one wishes to estimate $p\in[0,1]=\Theta=\mathcal{A}$ with the loss function $L(p,a)=|p-a|$.  Consider the estimator $\delta$ with $\delta(0)=\frac{1}{4}$ and $\delta(0)=\frac{3}{4}$.


% \begin{enumerate}
% \item Write out the risk function of $\delta$ and show that $R(p,\delta)\leq \frac{1}{4}$ for all $p\in[0,1]$.
% \item Show that there exists a prior distribution placing all its mass on $\{0,\frac{1}{2},1\}$ with respect to which $\delta$ is Bayes.\\
% Hint: Consider finding   prior probabilities on $\{0,\frac{1}{2},1\}$ so that $\delta$ is a median (in the STAT 642 sense) of the posterior distribution on $\{0,\frac{1}{2},1\}$.
% \item Prove that $\delta$ is minimax in this problem and identify a least favorable prior.
         % \end{enumerate}

             % \item Consider a decision problem where $P_\theta$ is the Normal$(\theta,1)$ distribution on $\mathcal{X}=\mathbb{R}$, $\mathcal{A}=\{0,1\}$, and $L(\theta,a)=I[a=0]I[\theta>5] + I[a=1]I[\theta \leq 5]$.  If $\Theta= (-\infty,5] \cup [6,\infty)$, guess what prior is least favorable, find the corresponding Bayes decision rule, and prove that it is minimax.

             % \item Consdier estimation of $\lambda$, the mean of a Poisson distribution, with the (inverse-mean) weighted squared error loss.
% That is, let $\Lambda=(0,\infty)$ be the parameter space, $\mathcal{A}=[0,\infty)$ be the action space, $P_\lambda$ be the Poisson distribution on $\mathcal{X}=\{0,1,2,3,\ldots\}$ and $L(\lambda,a)=
% \lambda^{-1}(\lambda-a)^2$.  Let $\delta(X)=X$.
% \begin{enumerate}
% \item Show that $\delta$ is an equalizer rule.
% \item Show that $\delta$ is generalized Bayes with respect to the Lebesgue measure on $\Lambda$.
 % \item Find the Bayes estimator with respect to a gamma$(\alpha,\beta)$ prior on $\Lambda$ which has a density given by
 % \[
 % g(\lambda|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha-1} e^{-\beta \lambda},\quad \lambda>0,
 % \]
 % for $\alpha,\beta>0$ and $\Gamma(\alpha) = \int_0^\infty t^{\alpha-1} e^{-t}dt$.\\[.2cm]
 % Note: Different parameterizations of the gamma distribution exist and the above one has mean $\alpha/\beta$.
 % \item Prove that $\delta$ is minimax for this problem.
         % \end{enumerate}

         % \item Let $X_1,\ldots,X_n$ be iid random variables, each having a distribution y7u$P_\lambda$, $\lambda>0$, with a density \[
         % f_\lambda(x) = \frac{d P_\lambda}{d \mu}(x) =  \lambda\exp(-\lambda x)I[x\neq0] +(1-e^{-\lambda})I[x=0]  =  \left(\lambda\exp(-\lambda x)\right)^{I[x\neq0]} \left((1-e^{-\lambda})\right)^{I[x=0]}
         % \]
         % for $\mu=m+\nu$ with $m$ being the Lebesgue measure on $[1,\infty)$ and $\nu$ being the point mass measure at $0$. Let $\delta_i= I[X_i=0]$, $i=1,\ldots,n$ and $M_n = n- \sum_{i=1}^n \delta_i$.
         % \begin{enumerate}
% \item Show that there is no maximum likelihood estimate (MLE) of $\lambda$ when $M_n=n$, but there is a MLE of $\lambda$ when $M_n<n$.
% \item  Show that, for any $\lambda>0$, with $\lambda$-probability tending to 1 as $n\rightarrow \infty$, the MLE of $\lambda$ exists.
% \item Give  as a simple estimator of $\lambda$ based on $M_n$ alone and prove that this estimator is consistent.
% \item Write down an explicit one-step Newton improvement of the estimator from (c) using the likelihood function from (a). (No need to simplify the expression).
         % \end{enumerate}
         % \item Suppose that $X_1,\ldots,X_n$ are iid with the distribution $P_\theta$ for $\theta \in \mathbb{R}$, where $P_\theta$ has the RN derivative with respect to the counting measure $\nu$ on $\mathcal{X}=\{0,1,2\}$ given by
             % \[
             % f_\theta(x)= \frac{\exp(x\theta)}{1+\exp(\theta)+\exp(2\theta)}.
             % \]
          % Find an estimator $T_n$ of $\theta$ based on $n_0\equiv \sum_{i=1}^n I[X_i=0]$ such that $\sqrt{n}(T_n-\theta)$ has a limiting normal distribution (implying that $\sqrt{n}(T_n-\theta)$  is bounded in probability or tight and so that $T_n$ is $\sqrt{n}$-consistent).\\

          % Note:  It may be helpful to recall the delta method.  Let  $T_n$ be a statistic such that $\sqrt{n}(T_n-c)\stackrel{d}{\rightarrow} N(0,\sigma^2)$ as $n\rightarrow \infty$ for some $c\in \mathbb{R}$ and variance $\sigma^2>0$.  Take a function any function $g$ where $g(t)$ is differentiable at $t=c$ and $g^\prime(c)\neq 0$ (the derivative of $g(t)$ at $t=c$ is not zero).  Then, as $n\rightarrow \infty$,
% $\sqrt{n}(g(T_n)-g(c))\stackrel{d}{\rightarrow} N(0,[g^\prime(c)]^2\sigma^2)$. 
% \end{enumerate}
