\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 642: HW 10}
\author{Evan P. Walsh}
\makeatletter
\makeatother
\lhead{Evan P. Walsh}
\chead{STAT 642: HW 10}
\rhead{\thepage}
\cfoot{}

\begin{document}
\maketitle

\subsection*{1 [AL 11.3]}
\begin{tcolorbox}
  Let $\left\{ X_n \right\}_{n\geq 1}$ be a sequence of independent random variables with 
  \[ 
    P(X_n = \pm 1) = \frac{1}{2} - \frac{1}{2\sqrt{n}}, P(X_n = \pm n^{2}) = \frac{1}{2\sqrt{n}}, n \geq 1. 
  \]
  Find constants $a_1, a_2, \hdots \in (0,\infty)$ and $b_1, b_2, \hdots \in \mathbb{R}$ such that 
  \[
    \frac{\sum_{k=1}^{n}X_k - b_n}{a_n} \rightarrow^{d} N(0,1).
  \]
\end{tcolorbox}
Let $X_{n,k} := X_k$ whenever $1 \leq k \leq n$. Then $\left\{ X_{n,k} : 1\leq k \leq n \right\}_{n\geq 1}$ is a triangular array with $EX_{n,k} = 0$
and 
\[
  EX_{n,k}^{2} = 2\left( \frac{1}{2} - \frac{1}{2\sqrt{k}} \right) + 2k^{4}\cdot \frac{1}{2\sqrt{k}} = 1 - \frac{1}{\sqrt{k}} + \frac{k^{4}}{\sqrt{k}} = k^{7/2} + 1 - k^{-1/2} < \infty.
\]
Thus,
\begin{equation}
  v_{n}^{2} := \sum_{k=1}^{n}(k^{7/2} + 1 - k^{-1/2}) \geq \int_{0}^{n}(x^{7/2} + 1 - x^{-1/2})dx = \frac{2}{9}x^{9/2} + n + 2n^{1/2} \geq
  \frac{1}{3}n^{9/2}.
  \label{1.1}
\end{equation}
Using~\eqref{1.1}, we will verify that the Lindeberg condition is satisfied. Let $\epsilon > 0$. By~\eqref{1.1}, $v_n > \frac{1}{3}n^{9/4}$.
Thus, when $n^{1/4} \geq 3 / \epsilon$, we have $k^{2} \leq
n^{2} \leq \frac{\epsilon}{3}n^{9/4}$ for all $1 \leq k \leq n$, which implies 
\[
  \mathbb{I}\left(|X_{n,k}| > \epsilon v_{n}\right) \leq \mathbb{I}\left(|X_{n,k} > \frac{\epsilon}{3}n^{9/4} \right) = 0.
\]
Hence,
\[ 
  \lim_{n\rightarrow\infty}\frac{1}{v_{n}^{2}}\sum_{k=1}^{n}EX_{n,k}^{2}\mathbb{I}\left( |X_{n,k}| > \epsilon v_{n} \right) \leq
  \lim_{n\rightarrow\infty}\frac{1}{v_{n}^{2}}\sum_{k=1}^{n}EX_{n,k}^{2}\mathbb{I}\left( |X_{n,k}| > \frac{\epsilon}{3}n^{9/4} \right) = 0.
\]
Since the Lindeberg condition is satisfied, we can choose $a_n := v_n$ and $b_n := 0$ for all $n \geq 1$.


\subsection*{2 [AL 11.4]}
\begin{tcolorbox}
  Let $\left\{ X_{n} \right\}_{n\geq 1}$ be a sequence of independent random variables such that for some $\alpha \geq 1/2$,
  \[
    P(X_{n} = \pm n^{\alpha}) = \frac{n^{1-2\alpha}}{2} \ \ \text{and} \ \ P(X_{n} = 0) = 1 - n^{1-2\alpha}, \ n\geq 1.
  \]
  Let $S_{n} = \sum_{j=k}^{n}X_{n,k}$ and $s_{n}^{2} = \text{Var}(S_{n})$.
  \begin{enumerate}[label = (\alph*)]
    \item Show that for all $1/2 \leq \alpha < 1$, $\frac{S_{n}}{s_{n}} \rightarrow^{d} N(0,1)$.
    \item Show that (a) fails for $\alpha \geq 1$.
    \item Show that for $\alpha \geq 1$, $\left\{ S_n \right\}_{n\geq 1}$ converges to a random variable $S$ w.p. 1 and that $s_{n} \rightarrow
      \infty$.
  \end{enumerate}
\end{tcolorbox}
\begin{Proof}
  Let $X_{n,k} := X_{k}$ whenever $1 \leq k \leq n$. Let $\sigma_{n,k}^{2} := E|X_{n,k}|^{2} = k^{2\alpha + 1 - 2\alpha} = k$, and $v_{n}^{2} :=
  \sum_{k=1}^{n}\sigma_{n,k}^{2} = \frac{n(n+1)}{2}$. Then $\left\{ X_{n,k} : 1 \leq k \leq n \right\}_{n\geq 1}$ is a triangular array with $EX_{n,k}
  = 0$ for all $1 \leq k \leq n$.
  \begin{enumerate}[label = (\alph*)]
    \item Suppose $1/2 \leq \alpha < 1$. We will show that the Lindeberg condition is satisfied. Let $\epsilon > 0$. When $n^{1 - \alpha} \geq
      \frac{\sqrt{2}}{\epsilon}$, we have 
      \[ 
        \mathbb{I}(|X_{n,k}| > \epsilon v_{n}) \leq \mathbb{I}\left(k^{\alpha} > \frac{\epsilon n}{\sqrt{2}}\right) \leq \mathbb{I}\left(n^{\alpha} >
        \frac{\epsilon n}{\sqrt{2}}\right) = 0.
      \]
      Thus,
      \[ \frac{1}{v_{n}^{2}}\sum_{k=1}^{n}E|X_{n,k}|^{2}\mathbb{I}(|X_{n,k}| > \epsilon v_{n}) \rightarrow 0. \]
      Hence the Lindeberg condition is satisfied, so $\frac{S_{n}}{s_{n}} = \frac{S_{n}}{v_{n}} \rightarrow^{d} N(0,1)$.\addtocounter{enumi}{1} 
    \item Suppose $\alpha > 1$. Let $X_{n}^{(1)} := X_{n}\mathbb{I}(|X_{n}| \leq 1)$. Then,
      \begin{align*}
        & \sum_{n=1}^{\infty}P(|X_{n}| > 1) = \sum_{n=2}^{\infty}n^{1-2\alpha} < \infty, \\
        & \sum_{n=1}^{\infty}|EX_{n}^{(1)}| = 0, \\
        & \sum_{n=1}^{\infty}\text{Var}(X_{n}^{(1)}) = \sum_{n=1}^{\infty}E|X_{n}^{(1)}|^{2} = 1.
      \end{align*}
      Thus, by the 3-series theorem, $S_{n}$ converges w.p. 1. Further, $s_{n} = \sqrt{v_{n}^{2}} = \frac{\sqrt{n(n+1)}}{\sqrt{2}} \rightarrow
      \infty$. \addtocounter{enumi}{-2}
      \newpage
    \item If $\alpha > 1$, then by part (c), $\frac{S_{n}}{s_{n}} \rightarrow 0$ w.p. 1. Now suppose $\alpha = 1$. Let $X_{n,j} :=
      \frac{\sqrt{2}}{\sqrt{n(n+1)}}X_{i}$, for $1 \leq i \leq n$, $n \geq 1$.
        \begin{claim}
          $\left\{ X_{n,j} : 1 \leq i\leq n \right\}$ is a null array.
        \end{claim}
        \begin{claimproof}
          Let $\epsilon > 0$. For each $n \geq 1$, let $i_{n} := \min\left\{i : i > \epsilon \frac{\sqrt{n(n+1)}}{\sqrt{2}}\right\}$. Then
          \[ \lim_{n\rightarrow\infty}\max_{1\leq i \leq n}P(|X_{n,j} > \epsilon) = \lim_{n\rightarrow\infty}\frac{1}{2i_{n}} = 0. \]
          Hence $\left\{ X_{n,j} : 1 \leq i\leq n \right\}$ is a null array.
        \end{claimproof}

        By Claim 1 and since $N(0,1)$ distributions are infinitely divisible, we can apply the result of Theorem 6.2. Let $M(A) = \mathbb{I} (0\in A)$
        for all $A \in \mathcal{B}(\mathbb{R})$. So
        \begin{equation}
          \int_{x}^{\infty}y^{-2}dM(y) = 0 \ \ \forall \ x > 0.
          \label{2.1}
        \end{equation}
        However, since $i > \frac{n+1}{2}$ implies $\frac{i\sqrt{2}}{\sqrt{n(n+1)}} > \frac{\sqrt{2}}{2}$,
        \begin{equation}
          \sum_{i=1}^{n}P\left( X_{n,i} > \frac{\sqrt{2}}{2} \right) \geq \int_{\frac{n+3}{2}}^{n}\frac{1}{2x}dx = \frac{1}{2}\left( \log\left(
          \frac{n}{n+3}\right) + \log(2) \right) \longrightarrow  \log(3) / 2 > 0.
            \label{2.2}
        \end{equation}
        So by~\eqref{2.1} and~\eqref{2.2} and Theorem 6.2,
        \[ \sum_{i=1}^{n}X_{n,j} = \frac{S_n}{s_n} \not\rightarrow^{d} N(0,1). \]
  \end{enumerate}
\end{Proof}


\newpage
\subsection*{3 [AL 11.5]}
\begin{tcolorbox}
  Let $\left\{ X_{n,j} : 1 \leq j \leq r_{n} \right\}_{n\geq 1}$ be a triangular array of independent zero mean random variables satisfying the
  Lindeberg condition. Show that 
  \[ 
    E\left[ \max_{1\leq j \leq r_{n}}\frac{X_{n,j}^{2}}{s_{n}^{2}} \right] \rightarrow 0 \ \ \text{as} \ \ n \rightarrow \infty,
  \]
  where $s_{n}^{2} = \sum_{j=1}^{r_{n}}\text{Var} (X_{n,j})$, for $n \geq 1$.
\end{tcolorbox}
\begin{Proof}
  Let $0 < \epsilon < 1$. Then,
  \begin{align*}
    \limsup_{n\rightarrow\infty} E\left[ \max_{1\leq j \leq r_{n}}\frac{X_{n,j}^{2}}{s_{n}^{2}} \right] 
    & = \limsup_{n\rightarrow\infty} E\left[ \max_{1\leq j \leq r_{n}}\frac{X_{n,j}^{2}}{s_{n}^{2}} \right]\mathbb{I} \left(\max_{1\leq j \leq
    r_{n}}|X_{n,j}| \leq \epsilon s_{n}\right) \\
    & \qquad + E\left[ \max_{1\leq j \leq r_{n}}\frac{X_{n,j}^{2}}{s_{n}^{2}} \right]\mathbb{I} \left(\max_{1\leq j\leq r_{n}}|X_{n,j}| > \epsilon s_{n}\right) \\
    & \leq \limsup_{n\rightarrow\infty} \epsilon^{2} + \frac{1}{s_{n}^{2}}\sum_{j=1}^{n}EX_{n,j}^{2}\mathbb{I} (|X_{n,j}| > \epsilon s_{n}) \\
    & < \limsup_{n\rightarrow\infty} \epsilon + \frac{1}{s_{n}^{2}}\sum_{j=1}^{n}EX_{n,j}^{2}\mathbb{I} (|X_{n,j}| > \epsilon s_{n}) =
    \epsilon.
  \end{align*}
  Since $\epsilon$ was arbitrary, we are done.
\end{Proof}


\newpage
\subsection*{4 [AL 11.7]}
\begin{tcolorbox}
  For a sequence of random variables $\left\{ X_{n} \right\}_{n\geq 1}$ and for a real number $r > 2$, show that 
  \[ \lim_{n\rightarrow\infty}s_{n}^{-r}\sum_{j=1}^{n}E|X_{j}|^{r}\mathbb{I}(|X_{j}| > \epsilon s_{n}) = 0 \]
  for all $\epsilon > 0$ if and only if $\lim_{n\rightarrow\infty}s_{n}^{-r}\sum_{j=1}^{n}E|X_{j}|^{r} = 0$, wehre $s_{n}^{2} =
  \sum_{j=1}^{n}EX_{j}^{2}$.
\end{tcolorbox}
\begin{Proof}
  $(\Leftarrow)$ Trivial.

  $(\Rightarrow)$ Let $\epsilon > 0$ and let $\delta := \epsilon^{1/(r-2)}$. Then
  \begin{align*}
    \limsup_{n\rightarrow\infty}\frac{1}{s_{n}^{r}}\sum_{j=1}^{n}E|X_{j}|^{r} & =
    \limsup_{n\rightarrow\infty}\frac{1}{s_{n}^{r}}\sum_{j=1}^{n}E|X_{j}|^{r}\mathbb{I} (|X_{j}| \leq \delta s_{n}) 
    + \underbrace{\frac{1}{s_{n}^{r}}\sum_{j=1}^{n}E|X_{j}|^{r}\mathbb{I} (|X_{j}| > \delta s_{n})}_{\longrightarrow 0} \\
    & = \limsup_{n\rightarrow\infty}\frac{1}{s_{n}^{r}}\sum_{j=1}^{n}E|X_{j}|^{r-2}|X_{j}|^{2}\mathbb{I} (|X_{j}| \leq \delta s_{n}) \\
    & \leq\limsup_{n\rightarrow\infty}\frac{1}{s_{n}^{r}}\sum_{j=1}^{n}(\delta s_{n})^{r-2}E|X_{j}|^{2} \\
    & = \delta^{r-2}\cdot \frac{1}{s_{n}^{2}}\sum_{j=1}^{n}E|X_{j}|^{2} = \delta^{r-2} = \epsilon.
  \end{align*}
  Since $\epsilon > 0$ was arbitrary, we are done.
\end{Proof}


\newpage
\subsection*{5 [AL 11.9]}
\begin{tcolorbox}
  Let $\left\{ X_n \right\}_{n\geq 1}$ be a sequence of independent random variables such that 
  \[ P(X_{n} = \pm 1) = \frac{1}{4}, P(X_{n} = \pm n) = \frac{1}{4n^{2}} \]
  and 
  \[ P(X_{n} = 0) = \frac{1}{2}\left( 1 - \frac{1}{n^{2}} \right), \ n \geq 1. \]
  \begin{enumerate}[label = (\alph*)]
    \item Show that the triangular array $\left\{ X_{n,j} : 1 \leq j \leq n \right\}_{n\geq 1}$ with $X_{n,j} := X_{j}/\sqrt{n}$, for $1 \leq j \leq
      n$, does not satisfy the Lindeberg condition.
    \item Show that there exists $\sigma \in (0,\infty)$ such that 
      \[ \frac{S_n}{\sqrt{n}} \rightarrow^{d}N(0,\sigma^{2}). \]
      Find $\sigma$.
  \end{enumerate}
\end{tcolorbox}
\begin{Proof}
  \begin{enumerate}[label = (\alph*)]
    \item We have 
      \[ \sigma_{n,j}^{2} := E|X_{n,j}|^{2} = \frac{1}{n}E|X_{j}|^{2} = \frac{1}{n}\left( \frac{1}{2} + \frac{1}{2} \right) = \frac{1}{n}, \]
      and 
      \[ v_{n}^{2} := \sum_{j=1}^{n}\sigma_{n,j}^{2} = \sum_{j=1}^{n}\frac{1}{n} = 1. \]
      Thus, for $0 < \epsilon < 1$,
      \[ 
        \frac{1}{v_{n}^{2}}\sum_{j=1}^{n}E|X_{n,j}|^{2}\mathbb{I} (|X_{n,j}| > \epsilon v_{n}) = \sum_{j=1}^{n}E|X_{n,j}|^{2}\mathbb{I} (|X_{n,j}| >
        \epsilon) = \sum_{j=1}^{n}E|X_{n,j}|^{2} = 1 \not\rightarrow 0. 
      \]
      Hence the Lindeberg condition is not satisfied.
    \item Let $Y_{n} = X_{n}\mathbb{I} (|X| \leq 1)$ and $Z_{n} = X_{n}\mathbb{I} (|X| > 1)$ for each $n \geq 1$. Further, define $Y_{n,i} = Y_{i}$
      when $1 \leq i\leq n$ and $n \geq 1$.
      \begin{claim}
        $\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Y_{i} \rightarrow^{d} N(0,1/2)$.
      \end{claim}
      \begin{claimproof}
        Note that for all $1 \leq i \leq n$ and $n\geq 1$, 
        $EY_{n,i} = 0$ and $EY_{n,i}^{2} = \frac{1}{2}$, and $v_{n}^{2} := \sum_{i=1}^{n}EY_{n,i}^{2} = \frac{n}{2} < \infty$. Thus,
        \[ \lim_{n\rightarrow\infty}\frac{1}{v_{n}^{2}}\sum_{i=i}^{n}EY_{n,i}^{2}\mathbb{I} (|Y_{n,i}| > \epsilon v_{n}) \rightarrow 0. \]
        Thus, $\frac{\sqrt{n}}{\sqrt{2}}\sum_{i=1}^{n}Y_{n,i} = \frac{\sqrt{n}{2}}\sum_{i=1}^{n}Y_{i} \rightarrow^{d} N(0,1)$ by the Lindeberg CLT.
        But by Slutsky's theorem, $\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Y_{i} \rightarrow^{d} N(0,1/2)$.
      \end{claimproof}

      \begin{claim}
        $\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Z_{i} \rightarrow^{d} 0$.
      \end{claim}
      \begin{claimproof}
        Note that 
        \[
          E\left|\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Z_{i}\right| \leq \frac{1}{\sqrt{n}}\sum_{i=1}^{n}E|Z_{i}| =
          \frac{1}{\sqrt{n}}\sum_{i=1}^{n}\frac{1}{2i} \leq \frac{1}{2\sqrt{n}}\left( 1 + \int_{1}^{n}\frac{1}{x}dx \right) = \frac{1}{\sqrt{n}}(1 +
          \log(n)) \rightarrow 0.
        \]
        Hence $\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Z_{i} \rightarrow^{d} 0$ since $L^{1}$ convergence implies convergence in distribution.

      \end{claimproof}

      Thus, by claims 1 and 2,
      \[
        \frac{S_n}{\sqrt{n}} = \frac{1}{\sqrt{n}}\sum_{i=1}^{n}Y_{i} + \frac{1}{\sqrt{n}}\sum_{i=1}^{n}Z_{i} \rightarrow^{d} N(0,1/2)
      \]
      by the continuous mapping theorem.
  \end{enumerate}
\end{Proof}



\newpage
\subsection*{6 [AL 11.19]}
\begin{tcolorbox}
  For any random variable $X$, show that $EX^{2} < \infty$ implies 
  \[
    \frac{y^{2}P(|X| > y)}{E[X^{2}\mathbb{I} (|X| \leq y)]} \rightarrow 0
  \]
  as $y \rightarrow\infty$. Give an example to show that the converse is false.
\end{tcolorbox}
\begin{Proof}
  Of course we should assume $X \not\equiv 0$, otherwise the assertion is false. Let $\mu$ denote the probability distribution of $X$ on
  $(\mathbb{R},\mathcal{B}(\mathbb{R}))$. Now, note that
  \begin{equation}
    y^{2}P(|X| > y) = \int_{|x| > y}y^{2}d\mu(x) \leq \int_{|x| > y}|x|^{2}d\mu(x) \rightarrow 0\ \ \text{as}\ y \rightarrow \infty
    \label{6.1}
  \end{equation}
  since $EX^{2} < \infty$, and 
  \begin{equation}
    E[X^{2}\mathbb{I} (|X| \leq y)] = \int_{|x| \leq y}x^{2}d\mu(x) \rightarrow EX^{2} < \infty \ \ \text{as} \ y \rightarrow \infty.
    \label{6.2}
  \end{equation}
  So by~\eqref{6.1} and~\eqref{6.2}, $\frac{y^{2}P(|X| > y)}{E[X^{2}\mathbb{I} (|X| \leq y)]} \rightarrow 0$ as $y \rightarrow \infty$.
\end{Proof}

\vspace{5mm}
Consider the random variable with pdf $f(x) = c_{1}|x|^{-3}$ for $|x| > 2$. Fix $y > 2$. Then
\[ y^{2}P(|X| > y) = 2\int_{y}^{\infty}y^{2}c_{1}|x|^{-3}dx = 2y^{2}c_{1}\int_{y}^{\infty}\frac{1}{x^{3}}dx = c_{1}, \]
and 
\[ E[X^{2}\mathbb{I} (|X| \leq y)] = 2\int_{2}^{y}x^{2}\frac{1}{x^{3}}dx = 2\int_{2}^{y}\frac{1}{x}dx = 2\log(y) - 2\log(2) \rightarrow \infty \]
as $y \rightarrow \infty$. Thus $\frac{y^{2}P(|X| > y)}{E[X^{2}\mathbb{I} (|X| \leq y)]} \rightarrow 0$ as $y \rightarrow \infty$, but 
$E[X^{2}] = \lim_{y\rightarrow\infty}E[X^{2}\mathbb{I} (|X| \leq y)] = \infty$.




\newpage
\subsection*{7 [AL 11.20]}
\begin{tcolorbox}
  Let $\left\{ X_n \right\}_{n\geq 1}$ be a sequence of iid random variables with common distribution
  \[
    P(X_1 \in A) = \int_{A}|x|^{-3}\mathbb{I} (|x| > 1)dx,
  \]
  for $A \in \mathcal{B}(\mathbb{R})$. Find sequences $a_1,a_2,\hdots \in (0,\infty)$ and $b_1,b_2,\hdots \in \mathbb{R}$ such that 
  \[
    \frac{S_n - b_n}{a_n} \rightarrow^{d} N(0,1),
  \]
  where $S_n := \sum_{j=1}^{n}X_j$ for $n \geq 1$.
\end{tcolorbox}

Let $Y_{n,i} := X_{i}\mathbb{I} (|X_{i}| \leq \sqrt{n})$ and $Z_{n,i} = X\mathbb{I} (|X_{i}| > \sqrt{n})$ for each $1\leq i \leq n$ and $n \geq 1$.
\begin{claim}
  The triangular array $\left\{ Y_{n,i} : 1 \leq i \leq n \right\}_{n\geq 1}$ satisfies the Lindeberg condition, and so
  $\frac{1}{\sqrt{n\log(n)}}\sum_{i=1}^{n}Y_{n,i} \rightarrow^{d} N(0,1)$.
\end{claim}
\begin{claimproof}
  We have $EY_{n,i} = 0$, 
  \[
    EY_{n,i}^{2} = 2\int_{1}^{\sqrt{n}}\frac{1}{x}dx = 2\log(\sqrt{n}) = \log(n),
  \]
  and $v_{n}^{2} := \sum_{i=1}^{n}EY_{n,i}^{2} = n\log(n)$. Therefore
  \[ 
    \lim_{n\rightarrow\infty}\frac{1}{v_{n}^{2}}\sum_{i=1}^{n}EY_{n,i}^{2}\mathbb{I} (|Y_{n,i}| > \epsilon v_{n}) =
    \frac{1}{v_{n}^{2}}\sum_{i=1}^{n}EX_{i}\mathbb{I} (\epsilon \sqrt{n\log(n)} < |X_{i}| \leq \sqrt{n}) \rightarrow 0.
  \]
  Hence the Lindeberg condition is satisfied, and so $\frac{1}{v_{n}}\sum_{i=1}^{n}Y_{n,i}$.
\end{claimproof}

\begin{claim}
  $\frac{1}{\sqrt{n\log(n)}}\sum_{i=1}^{n}Z_{n,i} \rightarrow^{d} 0$.
\end{claim}
\begin{claimproof}
  \begin{align*}
    E\left|\frac{1}{\sqrt{n\log(n)}}\sum_{i=1}^{n}Z_{n,i}\right| \leq \frac{1}{\sqrt{n\log(n)}}\sum_{i=1}^{n}E|Z_{n,i}| & =
    \frac{2}{\sqrt{n\log(n)}}\sum_{i=1}^{n}\int_{\sqrt{n}}^{\infty}x^{-2}dx \\
    & = \frac{2}{\sqrt{n\log(n)}}\cdot \frac{n}{\sqrt{n}} = \frac{2}{\sqrt{\log(n)}} \rightarrow 0.
  \end{align*}
\end{claimproof}

By claims 1 and 2 and the continuous mapping theorem,
\[
  \frac{S_n - b_n}{a_n} = \frac{1}{\sqrt{n\log(n)}}\sum_{i=1}^{n}Y_{n,i} + \frac{1}{\sqrt{n\log(n)}}\sum_{i=1}^{n}Z_{n,i} \rightarrow^{d} N(0,1) 
\]
where $a_n = \sqrt{n\log(n)}, b_n = 0$ for each $n\geq 1$.


\newpage
\subsection*{8 [AL 11.21]}
\begin{tcolorbox}
  Show using characteristic functions that if $X_1, \hdots, X_k$ are iid Cauchy $(\mu,\sigma)$ random variables, then $S_{k} :=
  \sum_{i=1}^{k}X_{i}$ has a Cauchy $(k\mu, k\sigma)$ distribution.
\end{tcolorbox}
\begin{Proof}
  Let $t \in \mathbb{R}$. Then by independence,
  \[ 
    \phi_{S_{k}}(t) = Ee^{itS_{k}} = Ee^{it\sum_{i=1}^{k}X_{k}} = \prod_{i=1}^{k}Ee^{itX_i} = \left( Ee^{itX_1} \right)^{k} = 
    \left( e^{i\mu t - \sigma |t|} \right)^{k} = e^{i(k\mu)t - (k\sigma)|t|},
  \]
  which is the characteristic function of a Cauchy $(k\mu, k\sigma)$ distribution.
\end{Proof}


\end{document}

