\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks

\title{STAT 642: HW 7}
\author{Evan P. Walsh}
\makeatletter
\let\runauthor\@author
\let\runtitle\@title
\makeatother
\lhead{\runauthor}
\chead{\runtitle}
\rhead{\thepage}
\cfoot{}

\begin{document}
\maketitle


\subsection*{1 [AL 9.33]}
\begin{tcolorbox}
Let $X_{n} \rightarrow^{d} X$ and $x_{n} \rightarrow x$ as $n\rightarrow\infty$. If $P(X = x) = 0$, then show that $P(X_{n}\leq x_{n}) \rightarrow P(X
\leq x)$.
\end{tcolorbox}
\begin{Proof}
Let $Y_{n} \equiv x_n - x$ for each $n \geq 1$. Since $x_{n} \rightarrow x$, $Y_{n} \rightarrow 0$ a.s. ($P$). Hence $Y_n \rightarrow^{p} 0$. 
So by Slutsky's Theorem, $X_{n} - Y_{n} \rightarrow^{d} X$.
Now, since $P(X = x) = 0$, the distribution function, $F$, of $X$ is continuous at $x$. Hence 
\[ P(X_{n} - (x_n - x) \leq x) \rightarrow P(X \leq x) \text{ as } n \rightarrow \infty, \]
which implies $P(X_{n} \leq x) \rightarrow P(X \leq x)$ as $n\rightarrow \infty$.
\end{Proof}

\subsection*{2}
\begin{tcolorbox}
Consider random variables $\left\{ X_{n} \right\}_{n\geq 1}$, which may not be defined on a common probability space. Suppose that $X_{n}
\rightarrow^{d} X$ as $n \rightarrow \infty$ and that $\left\{ X_{n} \right\}_{n\geq 1}$ is uniformly integrable, i.e.
\[ \lim_{t\rightarrow\infty}\sup_{n\geq 1}E\left[ |X_{n}|\mathbb{I}(|X_{n}| > t) \right] = \lim_{t\rightarrow\infty}\sup_{n\geq 1}\int_{|x| >
t}|x|d\mu_{n}(x) = 0, \]
where $\mu_{n}$ denotes the probability distribution of $X_n$ on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$, $n\geq 1$. Show that $E|X| < \infty$ and
that $EX_n \rightarrow EX$ as $n\rightarrow\infty$.
\end{tcolorbox}
\begin{Proof}
For simplicity, let $\mu_0$ denote the distribution of $X$. Since $X_n \rightarrow^{d} X$, there exists random variables $\left\{ Y_{n}
\right\}_{n=0}^{\infty}$ on a common probability space $(\Omega, \mathcal{F}, P)$ such that each $Y_n$ has distribution $\mu_n$ and $Y_n \rightarrow
Y_0$ a.s. ($P$). Thus, by Proposition 6.2.1 (i) [AL] (the change of variables formula),
\begin{equation}
\int_{\mathbb{R}}|x|\mathbb{I}(|x| > t)d\mu_n(x) = \int_{\mathbb{R}}|x|\mathbb{I}(|x| > t)dPY_{n}^{-1}(x) = \int_{\Omega}|Y_n|\mathbb{I}(|Y_n| > t)dP.
\label{2.1}
\end{equation}
Hence $\left\{ Y_n \right\}_{n\geq 1}$ is UI, and by Proposition 2.5.7 (v) [AL], $\sup_{n\geq 1}\int_{\Omega}|Y_n|dP < \infty$, so $Y_n \in
\mathcal{L}^{1}(\Omega, \mathcal{F}, P)$ for all $n \geq 1$. Thus, by Theorem 2.5.10 [AL], $Y_0$ is integrable, i.e. $E|Y_0| < \infty$, and 
\begin{equation}
\lim_{n\rightarrow\infty}\int_{\Omega}|Y_n - Y_0|dP = 0.
\label{2.2}
\end{equation}
But $E|Y_0| < \infty$ implies $E|X| < \infty$ by \eqref{2.1} (take $t \leq 0$). Further, since $Y_n$ integrable for all $n \in \mathbb{N}$,
\begin{equation}
\int_{\mathbb{R}}x\ d\mu_n(x) = \int_{\Omega}Y_n\ dP \ \ \text{ for all } n \in \mathbb{N}, 
\label{2.3}
\end{equation}
by Proposition 2.5.10 (ii). Therefore by \eqref{2.2} and \eqref{2.3}
\begin{align*}
\limsup_{n\rightarrow\infty}|EX_{n} - EX| = \limsup_{n\rightarrow\infty}\left| \int_{\mathbb{R}}x\ d\mu_n - \int_{\mathbb{R}} x\ d\mu_0 \right| & = 
\limsup_{n\rightarrow\infty}\left| \int_{\Omega}Y_n \ dP - \int_{\Omega}Y_0\ dP\right| \\
& = \limsup_{n\rightarrow\infty}\left| \int_{\Omega}(Y_n - Y_0)\ dP \right| \\
& \leq \limsup_{n\rightarrow\infty}\int_{\Omega}|Y_n - Y_0|\ dP = 0.
\end{align*}
\end{Proof}


\subsection*{3 [AL 9.7 / Proposition 5.9 (ii)]}
\begin{tcolorbox}
If $\left\{ X_{n} \right\}_{n=0}^{\infty}$ is tight and $Y_{n} \stackrel{p}{\rightarrow} 0$ ($X_{n}, Y_{n}$ defined on $(\Omega_{n}, \mathcal{F}_{n},
P_{n})$), then $X_{n}Y_{n} \stackrel{p}{\rightarrow} 0$.
\end{tcolorbox}
\begin{Proof}
Let $\epsilon, \delta > 0$. Since $\left\{ X_{n} \right\}_{n=0}^{\infty}$ is tight, there exists some $M > 0$ such that $sup_{\mathbb{N}}P_{n}(|X_{n}|
> M) < \delta / 2$. Further, since $Y_{n} \stackrel{p}{\rightarrow} 0$, there exists some $N \in \mathbb{N}$ such that $P_{n}(|Y_{n}| > \epsilon / M)
< \delta / 2$ for all $n \geq N$. Thus, for $n \geq N$,
\begin{align*}
P_{n}\left( |X_{n}Y_{n}| > \epsilon \right) & = P_{n}\left( |X_{n}Y_{n}| > \epsilon, |X_{n}| > M \right) + P_{n}\left( |X_{n}Y_{n}| > \epsilon, |X_{n}|
\leq M \right) \\
& \leq P_{n}\left( |X_{n}| > M \right) + P_{n}\left( M|Y_{n}| > \epsilon, |X_{n}| \leq M \right) \\
& \leq P_{n}\left( |X_{n}| > M \right) + P_{n}\left( |Y_n| > \epsilon / M \right) \\
& < \delta / 2 + \delta / 2 = \delta.
\end{align*}
So $X_{n}Y_{n} \stackrel{p}{\rightarrow} 0$.
\end{Proof}


\newpage
\subsection*{4 [AL 9.9]}
\begin{tcolorbox}
Let $\left\{ X_{j,n} \right\}_{n\geq 1}$, $j = 1, \hdots, k \geq 1$ be sequence of random variables and let $X_{n} := \left( X_{1,n}, \hdots, X_{k,n}
\right)$ for all $n \geq 1$. Show that $\left\{ X_{n} \right\}_{n\geq 1}$ is tight in $\mathbb{R}^{k}$ if and only if $\left\{ X_{j,n} \right\}_{n\geq
1}$ is tight in $\mathbb{R}$ for each $1 \leq j \leq k$.
\end{tcolorbox}
\begin{Proof}
$(\Rightarrow)$ Suppose $\left\{ X_{n} \right\}_{n\geq 1}$ is tight in $\mathbb{R}^{k}$. Let $\epsilon > 0$. Then there exists $M > 0$ such that 
$\sup_{n\geq 1}P(\|X_{n}\| > M) < \epsilon$. But if $|X_{j_0,n}| > M$ for any $1 \leq j_0 \leq k$, then $\sum_{j=1}^{k}|X_{j,n}| = \|X_n\| > M$. Thus,
$P\left( |X_{j_0,n}| > M \right) \leq P\left( \|X_{n}\| > M \right)$ for all $1 \leq j_0 \leq k, n \geq 1$. 
Hence, $\sup_{n\geq 1}P\left( |X_{j,n}| > M \right) \leq \sup_{n\geq 1}P(\|X_n\| > M) < \epsilon$ for all $1 \leq j \leq k$.

$(\Leftarrow)$ Now suppose $\left\{ X_{j,n} \right\}_{n\geq 1}$ is tight in $\mathbb{R}$ for all $1 \leq j \leq k$. Let $\epsilon > 0$. Then for each
$j \in \left\{ 1,\hdots, k \right\}$, there exists some $M_j > 0$ such that $\sup_{n\geq 1}P(|X_{j,n}| > M_j / k) < \epsilon / (2k)$. Let $M :=
\max\left\{ M_{1}, \hdots, M_{k} \right\}$. Then for any $n \geq 1$,
\begin{align*}
P(\|X_n\| > M) = P\left( \sum_{j=1}^{k}|X_{j,n}| > M \right) & \leq P\left( |X_{j,n}| > \frac{M}{k}\text{ for some } 1 \leq j \leq k \right) \\
& = P\left( \bigcup_{j=1}^{k}\left\{ |X_{j,n}| > \frac{M}{k} \right\} \right) \\
& \leq \sum_{j=1}^{k}P\left(|X_{j,n}| > \frac{M}{k}\right) < \frac{\epsilon}{2}.
\end{align*}
Therefore $\sup_{n\geq 1}(\|X_{n}\| > M) \leq \epsilon / 2 < \epsilon$.
\end{Proof}


\subsection*{5 [AL 9.14]}
\begin{tcolorbox}
Let $\left\{ X_{n} \right\}_{n\geq 1}$ be a sequence of random variables and $\left\{ a_n \right\}_{n\geq 1}$ be a sequence of positive reals such
that $a_n \rightarrow \infty$ as $n \rightarrow \infty$ and $a_n(X_n - \theta) \rightarrow^{d} Z$.
for some random variable $Z$ and for some $\theta \in \mathbb{R}$. Let $H : \mathbb{R} \rightarrow \mathbb{R}$ be a function that is differentiable at
$\theta$ with derivative $c$. Show that 
$a_n(H(X_n) - H(\theta)) \rightarrow^{d} cZ$. 
\end{tcolorbox}
\begin{Proof}
By Taylor's expansion, for any $x \in \mathbb{R}$,
\begin{equation}
H(x) = H(\theta) + c(x - \theta) + R(x)(x - \theta), 
\label{5.1}
\end{equation}
where $R(x) \rightarrow 0$ as $x \rightarrow \theta$.
Let $Y_n \equiv 1 / a_n$. Since $a_n \rightarrow \infty$, $Y_n \rightarrow 0$ with probability 1. Hence, by Slutsky's Theorem, 
\[ X_n - \theta = a_n(X_n - \theta)Y_n \rightarrow^{d} 0. \]
Therefore $X_n - \theta \rightarrow^{p} 0$ by Proposition 5.2, and since $R(x) \rightarrow 0$ as $x \rightarrow \theta$, $R(X_n) \rightarrow^{p} 0$.
Hence $R(X_n)a_n(X_n - \theta) \rightarrow^{d} 0\cdot Z \equiv 0$ by Corollary 5.5 (ii). Thus by \eqref{5.1} and Corollary 5.5 (i),
\[ a_n(H(X_n) - H(\theta)) = ca_n(X_n - \theta) + R(x)a_n(X_n - \theta) \rightarrow^{d} cZ. \]
\end{Proof}







\end{document}

