\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[headheight=15pt]{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\usepackage{graphicx}
\usepackage{bm} % for bold font in math mode - command is \bm{text}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb} % for stacked arrows and other shit
\pagestyle{fancy}
\usepackage{changepage}
\usepackage{mathcomp}
\usepackage{tcolorbox}

\declaretheoremstyle[headfont=\normalfont]{normal}
\declaretheorem[style=normal]{Theorem}
\declaretheorem[style=normal]{Proposition}
\declaretheorem[style=normal]{Lemma}
\newcounter{ProofCounter}
\newcounter{ClaimCounter}[ProofCounter]
\newcounter{SubClaimCounter}[ClaimCounter]
\newenvironment{Proof}{\stepcounter{ProofCounter}\textsc{Proof.}}{\hfill$\square$}
\newenvironment{claim}[1]{\vspace{1mm}\stepcounter{ClaimCounter}\par\noindent\underline{\bf Claim \theClaimCounter:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof of claim \theClaimCounter:}\space#1}{\hfill $\blacksquare$ Claim \theClaimCounter}
\newenvironment{subclaim}[1]{\stepcounter{SubClaimCounter}\par\noindent\emph{Subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{}
% \newenvironment{subclaimproof}[1]{\begin{adjustwidth}{2em}{0pt}\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
% $\blacksquare$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}\vspace{5mm}\end{adjustwidth}}
\newenvironment{subclaimproof}[1]{\par\noindent\emph{Proof of subclaim \theClaimCounter.\theSubClaimCounter:}\space#1}{\hfill
$\Diamond$ \emph{Subclaim \theClaimCounter.\theSubClaimCounter}}

\allowdisplaybreaks{}

% chktex-file 3

\title{STAT 642: HW 11}
\author{Evan P. Walsh}
\makeatletter
\makeatother
\lhead{Evan P. Walsh}
\chead{STAT 642: HW 11}
\rhead{\thepage}
\cfoot{}

\begin{document}

\subsection*{1 [AL 12.1]}
\begin{tcolorbox}
  Let $(X,Y)$ be the bivariate random vector with $EY^{2} < \infty$. Let $\mathbb{H} = L^{2}(\mathbb{R}^{2}, \mathcal{B}(\mathbb{R}^{2}), P_{X,Y})$,
  and $\mathbb{H}_{0} = \left\{ h(X) | h : \mathbb{R} \rightarrow \mathbb{R}\text{ measurable and }Eh(X)^{2} < \infty \right\}$. Suppoe that for some
  $h(X) \in \mathbb{H}_{0}$, 
  \begin{equation}
    EYI_{A} = Eh(X)I_{A}\ \ \text{for all}\ A \in \sigma\langle X\rangle. 
    \label{1.1}
  \end{equation}
  Show that $E(Y - h(X))Z_1 = 0$ for all $Z_1 \in
  \mathbb{H}_{0}$. 
\end{tcolorbox}
\begin{Proof}
  Let $Z_1 \in \mathbb{H}_{0}$. Since $Z_1$ is integrable, there exists a sequence of simple functions $\left\{S_n\right\}_{n=0}^{\infty}$ 
  on $\mathbb{R}^{2}$ such that $|S_n| \leq |Z_1|$ for all $n \in \mathbb{N}$ and $S_n \rightarrow Z_{1}$ almost surely. For each $n$, write $S_n = \sum_{j=0}^{k_n}a_{n,j}I_{A_{n,j}}$.
  Thus, by~\eqref{1.1},
  \begin{equation}
    E(Y - h(X))S_n = \sum_{j=0}^{k_n}a_{n,j}E(Y - h(X))I_{A_{n,j}} = \sum_{j=0}^{k_n}a_{n,j}[EYI_{A_{n,j}} - Eh(X)I_{A_{n,j}}] = 0.
    \label{1.2}
  \end{equation}
  Now, since $|S_n| \leq |Z_1|$, $|(Y - h(X))S_n| \leq |(Y - h(X))Z_1|$ for each $n \in \mathbb{N}$, and since $S_n \rightarrow Z_1$ almost surely,
  $(Y - h(X))S_n \rightarrow (Y - h(X))Z_1$ almost surely. Further, 
  by Cauchy-Schwarz, $E|(Y - h(X))Z_1| < \infty$. Hence by~\eqref{1.2} and DCT,
  \[ E(Y - h(X))Z_1 = \lim_{n\rightarrow\infty}E(Y - h(X))S_n = 0. \]
\end{Proof}


\subsection*{2 [AL 12.3 (a), (b)]}
\begin{tcolorbox}
  Let $Z_1$ and $Z_2$ be two random variables on a probability space $(\Omega, \mathcal{G}, P)$.
  \begin{enumerate}[label = (\alph*)]
    \item Suppose that $E|Z_1| < \infty$, $E|Z_2| < \infty$, and 
      \begin{equation}
        EZ_1 I_A = EZ_2 I_A 
        \label{2.1}
      \end{equation}
      for all $A \in \mathcal{G}$. Show that $P(Z_1 = Z_2) = 1$.
    \item Suppose that $Z_1$ and $Z_2$ are nonnegative and~\eqref{2.1} holds for all $A \in \mathcal{G}$. Show that $P(Z_1 = Z_2) = 1$.
  \end{enumerate}
\end{tcolorbox}
\begin{Proof}
  \begin{enumerate}[label = (\alph*)]
    \item Let $A_1 := \left\{ \omega \in \Omega : Z_1(\omega) - Z_2(\omega) > 0 \right\}$ and $A_2 := \left\{ \omega \in \Omega : Z_2(\omega) - Z_1(\omega) > 0 \right\}$. Since
      $E|Z_1|, E|Z_2| < \infty$, $E(Z_1 - Z_2)I_{A}$ is well-defined for all $A \in \mathcal{G}$. Thus, by~\eqref{2.1},
      \[ \int_{A_1}Z_1 dP = \int_{A_1}Z_2 dP \Rightarrow \int_{A_1}Z_1 - Z_2\ dP = 0, \]
      so $P(A_1) = 0$ since $Z_1 - Z_2 > 0$ on $A_1$. Similarly, 
      \[ \int_{A_2}Z_2 - Z_1\ dP = 0, \]
      and so $P(A_2) = 0$ since $Z_2 - Z_1 > 0$ on $A_2$. Hence 
      \[
        P(\left\{ \omega \in \Omega : Z_1(\omega) \neq Z_2(\omega) \right\}) = P(A_1 \cup A_2) = 0. 
      \]
    \item For each $n\geq 1$, let $A_{1,n} := \{\omega \in \Omega : Z_1(\omega) - Z_2(\omega) > 0 \wedge Z_1(\omega) \leq n\}$ and $A_{2,n} := \left\{
      \omega \in \Omega : Z_2(\omega) - Z_1(\omega) > 0 \wedge Z_2(\omega) \leq n \right\}$. Then $Z_1 - Z_2$ and $Z_2 - Z_1$ are well-defined and
      non-negative on $A_{1,n}$ and $A_{2,n}$, respectively, for each $n\geq 1$. So by~\eqref{2.1},
      \[ \int_{A_{1,n}}Z_1 - Z_2 \ dP = 0 \ \ \text{and} \ \ \int_{A_{2,n}}Z_{2} - Z_{1}\ dP = 0, \]
      and thus $P(A_{1,n}) = P(A_{2,n}) = 0$ for each $n\geq 1$. By the continuity of measure,
      \[ P(\{\omega \in \Omega : Z_1(\omega) \neq Z_2(\omega)\}) = \lim_{n\rightarrow\infty}P(A_{1,n}\cup A_{2,n}) = 0. \]
  \end{enumerate}
\end{Proof}


\subsection*{3 [AL 12.4]}
\begin{tcolorbox}
  Prove Theorem 12.2.3.
\end{tcolorbox}
\begin{Proof}
  Since $|Y_{n}| \leq Z$ almost surely, $Z - Y_{n}, Z + Y_{n} \geq 0$ almost surely. Hence, by the conditional Fatou's Lemma,
  \begin{align*}
    \liminf E(Z - Y_{n}) & \geq E(\liminf Z - Y_{n}) = E(Z - Y), \ \text{i.e.}\\
    EZ + \liminf E(-Y_{n}) & \geq EZ - EY, \ \text{i.e.} \\
    EZ - \limsup EY_{n} & \geq EZ - EY.
  \end{align*}
  Thus,
  \begin{equation}
    \limsup EY_{n} \leq EY.
    \label{3.1}
  \end{equation}
  Similarly,
  \[ \liminf E(Z + Y_{n}) \geq E(Z + Y_{n}), \ \text{i.e.} \ EZ + \liminf EY_{n} \geq EZ + EY. \]
  Hence
  \begin{equation}
    \liminf EY_{n} \geq EY.
    \label{3.2}
  \end{equation}
  So by~\eqref{3.1} and~\eqref{3.2},
  \[ \limsup EY_{n} \leq EY \leq \liminf EY_{n}. \]
  Therefore $\lim_{n\rightarrow\infty}EY_{n} = EY$.
\end{Proof}


\newpage
\subsection*{4 [AL 12.6 (b)]}
\begin{tcolorbox}
  Let $X$ be a random variable on a probability space $(\Omega, \mathcal{F}, P)$ with $EX^{2} < \infty$, and let $\mathcal{G} \subset \mathcal{F}$ be
  a sub-$\sigma$-field. Show that 
  \[ \left| \int_{A} E(X|\mathcal{G}) dP\right| \leq \left( \int_{A}E(X^{2}|\mathcal{G})dP \right)^{1/2} \]
  for all $A \in \mathcal{F}$.
\end{tcolorbox}
\begin{Proof}
  Note that since $Y := E(X|\mathcal{G})$ is $\mathcal{G}$-measurable by definition, $Y$ is also $\mathcal{F}$-measurable since $\mathcal{G}
  \subset \mathcal{F}$. Let $A \in \mathcal{F}$ and let $\varphi(x) := x^{2}$. Since $\varphi$ is convex on $[0,\infty)$ and % chktex 9
  $E\varphi(XI_{A}) = EX^{2}I_{A} < \infty$,
  \[
    (YI_{A})^{2} = \varphi(YI_{A}) = \varphi(Y)I_{A} \leq E(\varphi(X) | \mathcal{G})I_{A} = E(X^{2}|\mathcal{G})I_{A} 
  \]
  by Jensen's conditional inequality. Thus 
  \begin{equation}
    \int_{A}Y^{2}\ dP \leq \int_{A} E(X^{2} | \mathcal{G}).
    \label{4.1}
  \end{equation}
  Now, without loss of generality assume the left side of~\eqref{4.1} is finite. Then by Jensen's unconditional inequality,
  \begin{equation}
    \left( \int_{A}Y\ dP \right)^{2} = \varphi(EYI_{A}) \leq E\varphi(YI_{A}) = \int_{A}Y^{2}\ dP. 
    \label{4.2}
  \end{equation}
  So by~\eqref{4.1} and~\eqref{4.2},
  \[ \left| \int_{A} Y dP\right| \leq \left( \int_{A}E(X^{2}|\mathcal{G})dP \right)^{1/2}. \]
\end{Proof}

\subsection*{5 [AL 12.9 (a)]}
\begin{tcolorbox}
  Let $\left\{ X_n : n \in \mathbb{N} \right\}$ be a collection of independent random variables with $E|X_0| < \infty$. Show that $E(X_0|X_1, \hdots,
  X_n) = EX_0$ for any $n \geq 1$.
\end{tcolorbox}
\begin{Proof}
  Clearly $EX_0$ is $\sigma\langle X_1,\hdots, X_n\rangle$-measurable since it is constant. Therefore it remains to show that $\int_{A}EX_0\ dP =
  \int_{A}X_0\ dP$ for any $A \in \sigma\langle X_1, \hdots, X_n\rangle$. So let $A \in \sigma\langle X_1,\hdots, X_n\rangle$ and note that $I_A$ is
  independent of $X_0$ since $X_0$ is independent of $X_1, \hdots, X_n$. Thus,
  \[ \int_{A}X_0\ dP = \int_{\Omega}X_0I_{A}\ dP = \int_{\Omega}X_0\ dP \int_{\Omega}I_{A}\ dP = E(X_0)\cdot P(A) = \int_{A}EX_0\ dP. \]
\end{Proof}



\end{document}
% chktex 17
